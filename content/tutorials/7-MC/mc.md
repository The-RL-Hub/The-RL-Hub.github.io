#  متدهای Monte Carlo

متدهای Monte Carlo یه سری الگوریتم محاسباتی هستن که با **random sampling** جواب عددی مسائلی رو که در اصل می‌شه به‌صورت قطعی حل کرد، در میارن. این متدها تو کلی حوزه ـ از فیزیک و فایننس گرفته تا آمار و هوش مصنوعی ـ حسابی کاربرد دارن، مخصوصاً وقتی حل مستقیم سخته یا توزیع‌های پشت پرده معلوم نیست. تو RL هم متدهای Monte Carlo به ایجنت کمک می‌کنن value function‌ها و policy‌هاش رو **فقط با experience** (اپیزودهای sample-شده) یاد بگیره، بدون این‌که احتیاج به یه مدل از محیط داشته باشه. قبل از این‌که بریم سراغ Monte Carlo تو RL، اول باید فرق **planning (مدل-based)** و **learning (مدل-free)** رو روشن کنیم.

## Planning در برابر Learning تو RL

توی RL دو راه اصلی واسه تصمیم‌گیری داریم: **planning** (معمولاً مدل-based) و **learning** (مدل-free). فرق کلّی‌شون اینه که ایجنت یه **مدل از dynamics محیط** داره یا نه:

* **Planning (Model-Based RL):** ایجنت یه **مدل** از state-ترنزیشنها و ریواردها داره و باهاش جلو جلو plan می‌ریزه. وقتی $M(s,a)$ که next stateها و ریواردها رو پیش‌بینی می‌کنه دم دست باشه، ایجنت می‌تونه با «رول-اوت» ذهنی مسیرهای آینده، اکشن‌هاشو بدون تماس واقعی با environment ارزیابی کنه. روش‌های کلاسیک planning مثلاً **DP** هستن (که یه مدل کامل از MDP می‌خوان) یا تکنیک‌هایی مثل **Monte Carlo Tree Search (MCTS)**. خلاصه اینجا محاسبه رو می‌سپریم به مدل نه تجربه واقعی. مثلاً اگه $T(s,a,s')$ (ترنزیشن function) و $R(s,a)$ معلوم باشن، ایجنت می‌تونه معادلات Bellman رو مستقیم با DP حل کنه. MCTS هم با یه simulator (generative مدل) *imaginatively* دنباله‌های اکشن آینده رو ـ مثلاً تو Go یا chess ـ شبیه‌سازی می‌کنه و از توی یه search tree اکشنِ امیدبخش رو در میاره.

* **Learning (Model-Free RL):** ایجنت **از experience خام یاد می‌گیره** و هیچ مدل از dynamics محیط نداره. با trial-and-error policyشو بهتر می‌کنه و value-estimateها یا پارامترهای policy رو طبق **ریواردها و ترنزیشنهای** دیده-شده آپدیت می‌کنه. مدل-free methods فرض نمی‌کنن $T(s,a,s')$ یا $R(s,a)$ معلومه؛ محیط یه black-boxه و policy خوب از تعامل مستقیم بیرون میاد. نمونه‌ها: **Monte Carlo methods** (میانگین returns برای یادگیری value function)، **Temporal-Difference (TD)** methods مثل SARSA یا Q-learning، و policy-gradient methods. این روش‌ها رفتار رو با داده هر چی *واقعاً* موقع اجرای اکشن‌ها اتفاق افتاده تنظیم می‌کنن، نه با planning فرضی.

Planning و learning به هم نزدیکن ـ هدف هر دو آخر سر بهبود policyه ـ ولی منبع اطلاعاتشون فرق می‌کنه. Planning وقتی یه مدل دقیق داری معمولاً **sample-efficient**ه: ایجنت می‌تونه ارزون هر چقدر تجربه فرضی خواست بسازه. Learning وقتی dynamics محیط ناشناخته یا خیلی پیچیده‌ست لازمه (و گاهی تنها گزینه‌ست). تو واقعیت، مدل دقیق کم گیر میاد، واسه همین مدل-free learning کلی کاربرد داره. البته می‌شه مدل-based و مدل-free رو قاطی کرد (مثلاً اول مدل رو یاد گرفت بعد باهاش planning کرد)، ولی متدهای Monte Carlo تو دستهٔ **مدل-free RL** می‌افتن ـ یعنی کامل از اپیزودهای دیده-شده یاد می‌گیرن و هیچ مدل از قبل داده نمی‌شه.

*Illustration of مدل-based vs. مدل-free RL:* تو روش‌های مدل-based (planning) ایجنت با یه *مدل* داخلی از dynamics محیط outcomeها رو شبیه‌سازی می‌کنه و برای اکشن‌های بهینه plan می‌چینه (پیکان‌های نقطه‌چین یعنی look-ahead با مدل). تو روش‌های مدل-free (learning) اصلاً مدل استفاده نمی‌شه ـ ایجنت فقط از تعامل واقعی با محیط یاد می‌گیره. Planning یه مدل شناخته‌شده یا یادگرفته‌شده لازم داره و به ایجنت اجازه می‌ده «فکر» کنه، در حالی که مدل-free learning روی trial-and-error سوار می‌شه.

خلاصه این که **planning** (مدل-based RL) با داشتن مدل آینده‌نگری می‌کنه؛ اگه مدل مطمئن باشه، محاسبه‌ها سریع و ارزون درمیاد چون ایجنت می‌تونه کلی سناریوی فرضی رو تو یه چشم بر هم زدن بررسی کنه. **Learning** (مدل-free RL) بی‌خیال مدل می‌شه و از تجربه واقعی یاد می‌گیره؛ وقتی مدل در دسترس یا قابل‌حل نباشه همین راه رو داریم. متدهای Monte Carlo تو RL هم *مدل-free* هستن: ایجنت value-estimateها و policyشو فقط با میانگین گرفتن از experience episodic واقعی (یا شبیه‌سازی‌شده) بهتر می‌کنه، نه با حل معادلات تو یه مدل MDP معلوم.








## یه مقدمهٔ درباره‌ی Monte Carlo

روش‌های **Monte Carlo** در واقع یه دستهٔ بزرگ از الگوریتم‌ها هستن که حساب‌کتابشونو می‌سپرن به شانس. ایدهٔ اصلی اینه که با **random sampling** بری سراغ تخمین کمیت‌های ریاضی—اکثراً expectationها، انتگرال‌ها یا جواب مسائلی که نمی‌شه مستقیم و دقیق حلشون کرد. وقتی یه عالمه experiment تصادفی انجام بدی و آخرش از نتایج میانگین بگیری، Monte Carlo یه عدد تخمینی تحویلت می‌ده. پشتِ این کار، **Law of Large Numbers** خوابیده و خیالتو راحت می‌کنه که هرچی سمپل‌ها بیشتر بشن، این میانگین‌ها تهش می‌چسبن به عدد واقعی.

اگه بخوایم تعریفیشو بگیم، Monte Carlo معمولاً برای تخمین یه expectation یا یه انتگرال استفاده می‌شه. فرض کن می‌خوایم expectation
$I = \mathbb{E}[f(X)]$
رو نسبت به یه توزیع $p(x)$ دربیاریم. می‌شه اینو به شکل یه انتگرال برای متغیر تصادفی پیوسته $X$ نوشت:

$$
I \;=\; \int_{\Omega} f(x)\,p(x)\,dx,
$$

که $\Omega$ هم محدودهٔ $X$ه. خیلی وقت‌ها همچین expectation یا انتگرالی رو نمی‌شه با قلم و کاغذ درآورد. اینجاست که Monte Carlo میاد سراغ یه روش *simulation-based*. ما $N$ تا سمپل مستقل $x_1, x_2, \ldots, x_N$ از $p(x)$ می‌گیریم و بعد میانگین $f(x)$ روی این سمپل‌ها رو حساب می‌کنیم:

$$
\hat{I}_N \;=\; \frac{1}{N} \sum_{i=1}^{N} f(x_i).
$$

این $\hat{I}_N$ می‌شه **estimator** Monte Carlo برای $I$. چون سمپل‌ها رو درست از $p$ گرفته‌ایم، $\hat{I}_N$ بدون بایاسه، یعنی $\mathbb{E}[\hat{I}_N] = I$. دلیلش هم ساده‌س:
$\mathbb{E}[\hat{I}_N] = \tfrac{1}{N}\sum_{i=1}^N \mathbb{E}[f(x_i)] = \mathbb{E}[f(X)] = I$.
علاوه بر این، **LLN** می‌گه وقتی $N$ بره به بی‌نهایت، میانگین $\hat{I}_N$ (تقریباً حتماً) می‌چسبه به $I$. یعنی با یه عالمه random سمپل، خروجیِ میانگین هر قدر بخوای نزدیک می‌شه به مقدار واقعی. خوبیش اینه که شکل $f(x)$ یا $p(x)$ هر چی باشه (تا وقتی واریانس محدود باشه) این قانون کار می‌کنه؛ همین باعث می‌شه Monte Carlo هم کاربردی باشه، هم قوی.

**Convergence و واریانس:** دقت تخمین Monte Carlo با زیاد کردن $N$ بهتر می‌شه، ولی به‌صورت statistical جلو می‌ره، نه deterministic. طبق **CLT**، برای $N$ بزرگ، $\hat{I}_N$ دوروبر $I$ تقریباً نرمال پخش می‌شه با واریانس $\sigma^2/N$، جایی که $\sigma^2 = \mathrm{Var}(f(X))$ه:

$$
\hat{I}_N \;\approx\; \mathcal{N}\!\Big(I,\; \tfrac{\sigma^2}{N}\Big).
$$

پس خطا با مرتبهٔ $1/\sqrt{N}$ کم می‌شه. این نرخ $O(1/\sqrt{N})$ کند محسوب می‌شه: اگه بخوای خطا رو نصف کنی، باید $N$ رو چهار برابر کنی. بخوای یه رقم اعشار دقیق‌تر بشی، معمولاً باید صد برابر سمپل بیشتر جمع کنی—یه trade-off بین دقت و هزینهٔ محاسباتی. با این حال برگ برندهٔ Monte Carlo اینه که **dimension-independence** داره: همین $1/\sqrt{N}$ توی بعدهای بالا هم همینه و بدتر نمی‌شه. واسه انتگرال‌های پُربُعد یا سیستم‌های probabilistic پیچیده که روش‌های grid-base شده قربانی **curse of dimensionality** می‌شن، Monte Carlo حسابی می‌درخشه.

البته Monte Carlo فرض می‌کنه **سمپل‌ها independent** هستن. وقتی سمپل مستقل و i.i.d. از $p(x)$ داشته باشی، هم unbiasedness رو داری هم می‌تونی LLN و CLT رو بی‌دردسر اعمال کنی. اگه سمپل‌ها correlated باشن (مثل بعضی تکنیک‌های پیشرفته Monte Carlo)، تحلیل خطا سخت‌تر می‌شه—عملاً تعداد داده‌های مستقلت کمتر از $N$ می‌شه. این‌جا ولی برای راحتی، استقلال رو فرض می‌کنیم.

کاربردهای Monte Carlo کلی فراتر از RLه. از **Manhattan Project** برای تخمین neutron diffusion بگیر تا فیزیک، فایننس (مثل pricing مشتقات با شبیه‌سازی random price pathها)، Bayesian statistics (تخمین expectationهای posterior)، گرافیک کامپیووتری (رندر صحنه با random ray sampling) و خیلی حوزه‌های دیگه. برای جا افتادن قضیه، دو تا طModel ساده می‌زنیم: تخمین $\pi$ و حساب یه انتگرال مشخص با Monte Carlo. این مثالا نشون می‌دن چطور random sampling می‌تونه مسئله‌های هندسی و حسابیِ ظاهراً قطعی رو حل کنه.









#### مثال: تخمین $\pi$ با Random Sampling

یه آزمایش کلاسیک Monte Carlo برای تخمین $\pi$ اینه که یه مشت نقطهٔ Random توی یه plane پرت کنیم. فرض کن یه دایره با شعاع $1$ توی یه مربع واحد جا شده. برای شفافیت، یه unit square توی ربع اول (گوشه‌هاش $(0,0)$ و $(1,1)$) و یه quarter از یه unit circle با شعاع $1$ داخلش در نظر بگیر؛ همون قوسی که از $(1,0)$ تا $(0,1)$ می‌ره. مساحت quarter-circle می‌شه

$$
\frac{\pi r^{2}}{4} = \frac{\pi}{4}
$$

(چون $r=1$) و مساحت unit square هم که $1$ه. پس اگه یه نقطه رو کاملاً تصادفی توی مربع $[0,1]\times[0,1]$ بندازیم، احتمال این‌که اون نقطه بیفته توی quarter-circle دقیقاً $\pi/4$ـه. یعنی

$$
P\{(x,y)\ \text{inside circle}\} = 
\frac{\text{Area of quarter-circle}}{\text{Area of square}}
= \frac{\pi/4}{1} = \frac{\pi}{4}.
$$

برای exploit کردن این قضیه، کافیه کلی نقطهٔ Random توی unit square بسازیم و ببینیم چندتاشون داخل quarter-circle قرار می‌گیرن (همون شرط $x^{2}+y^{2}\le 1$ رو چک می‌کنیم). بعد اون کسر رو در $4$ ضرب کنیم تا یه تخمین از $\pi$ بگیریم. وقتی $N$ خیلی بزرگ می‌شه، طبق LLN این تخمین می‌چسبه به مقدار واقعی $\pi$.

1. **سمپل‌گیری:** $N$ تا نقطه $(x_{i},y_{i})$ با $x_{i},y_{i}\sim\mathrm{Uniform}(0,1)$ بساز. هر نقطه به یه اندازه شانس داره هر جای unit square فرود بیاد.
2. **شمارش نقاط داخل دایره:** برای هر نقطه چک کن آیا داخل quarter-circle می‌افته، یعنی $x_{i}^{2}+y_{i}^{2}\le 1$؛ بعد یه indicator تعریف کن

   $$
   I_i = \begin{cases}
   1 & \text{if } x_i^{2}+y_i^{2}\le 1,\\[4pt]
   0 & \text{otherwise.}
   \end{cases}
   $$

   هر $I_i$ یه Bernoulli با احتمال $\pi/4$ می‌شه $1$ و با احتمال $1-\pi/4$ می‌شه $0$.
3. **تخمین $\pi$:** میانگین سمپل این indicatorها، $\tfrac{1}{N}\sum_{i=1}^{N} I_i$، احتمال افتادن داخل دایره رو می‌سنجه. بعد در $4$ ضربش کن تا $\pi$ دربیاد:

   $$
   \pi \approx 4 \times \frac{\#\{\text{points inside circle}\}}{N}.
   $$

چون $\mathbb{E}[I_i]=\pi/4$, این Estimator بی‌طرفه. هرچی سمپل بیش‌تر بندازی، نسبت $\tfrac{\text{inside}}{N}$ به $\pi/4$ نزدیک‌تر می‌شه و طبیعتاً $4\times(\text{inside}/N)$ هم به $\pi$ نزدیک می‌شه. انحراف معیار این تخمین حدود

$$
4\sqrt{\frac{(\pi/4)(1-\pi/4)}{N}}
$$

ه که تقریباً با $\tfrac{\text{const}}{\sqrt{N}}$ پایین می‌آد. یعنی افزایش $N$ دقت رو زیاد می‌کنه، ولی با بازده کاهنده.

*Monte Carlo simulation for estimating $\pi$.* توی این روش، نقاطی رو یکنواخت توی unit square می‌ریزیم و اونایی رو که داخل quarter از unit circle (نقطه‌های آبی) افتادن می‌شماریم. این‌جا $N=300$ سمپل نشون داده شده. نسبت نقاط داخل quarter-circle (آبی) به کل نقاط برآوردی از $\pi/4$ می‌ده. توی این اجرای نمونه، حدود سه‌چهارمِ نقاط داخل افتادن و $\pi \approx 4 \times 0.75 = 3.0$ دراومد؛ هرچی $N$ رو ببری بالا، این عدد به $\pi \approx 3.1416$ نزدیک‌تر می‌شه. ترفند اینه که احتمال افتادن داخل quarter-circle برابر $\pi/4$ـه، پس $4\times(\text{inside fraction})$ تقریب $\pi$ محسوب می‌شه.

این مثال $\pi$ نشون می‌ده چه‌طور Monte Carlo با تکیه بر تصادف یه پدیدهٔ پیچیده رو سمپل می‌کنه (اینجا رابطهٔ هندسی بین مساحت دایره و مربع) و با میانگین‌گیری عددی کمیت دلخواه رو تقریب می‌زنه. بدون نیاز به محاسبهٔ انتگرال دقیق یا فرمول‌بازی، فقط سمپل و میانگین. دقت این روش با $1/\sqrt{N}$ بالا می‌ره، ولی برای دقت خیلی زیاد گاهی باید میلیون‌ها نقطه سمپل کنی؛ این همون معاملهٔ زمان در برابر دقته که Monte Carlo بهش تکیه می‌کنه وقتی راه بهتر دیگه‌ای در دسترس نیست.


![1graph](Pictures/1.png)






#### مثال: Monte Carlo Integration

روش‌های Monte Carlo می‌تونن **definite انتگرال‌ها** رو هم با رندوم سمپل‌گیری تقریب بزنن و یه جایگزین باحال برای روش‌های عددی انتگرال‌گیری deterministic بدن. فرض کن یه تابع حقیقی $f(x)$ داریم و می‌خوایم روی بازه $[a,b]$ انتگرالش رو حساب کنیم:

$$
I \;=\; \int_a^b f(x)\,dx.
$$

وقتی $f(x)$ پیچیده باشه یا حدود انتگرال high-dimensional باشه، روش‌های کلاسیک مثل Simpson’s rule یا Gaussian quadrature کلی دردسر درست می‌کنن. Monte Carlo integration یه رویکرد خیلی سرراسته:

* **روش Uniform سمپل:** اول $N$ تا نقطه $x_1,\dots,x_N$ رو از بازه $[a,b]$ به صورت یکنواخت سمپل می‌کنیم. بعد $f(x)$ رو روی هر کدوم حساب می‌کنیم و میانگین می‌گیریم، بعدش در طول بازه یعنی $(b-a)$ ضرب می‌کنیم:

$$
\hat{I}_N \;=\; \frac{b-a}{N}\sum_{i=1}^N f(x_i).
$$

این $\hat{I}_N$ می‌شه estimator Monte Carlo برای انتگرال. ایده از این رابطه میاد که

$$
\int_a^b f(x)\,dx \;=\; (b-a)\,\mathbb{E}_{x\sim U(a,b)}[f(x)].
$$

عملاً انتگرال رو به عنوان مساحت زیر $f(x)$ می‌بینیم و با سمپل‌گیری رندوم از $[a,b]$ و گرفتن میانگین ارتفاع‌ها تقریبش می‌کنیم.

طبق Law of Large Numbers، وقتی $N$ خیلی بزرگ بشه، $\hat{I}_N$ می‌ره سمت مقدار واقعی $I$. این estimator unbiasedـه، چون

$$
\mathbb{E}[\hat{I}_N] = I.
$$

به کمک CLT هم خطای تخمین برای $N$ بزرگ تقریباً نرمال با واریانس

$$
\mathrm{Var}(f(X))\,\frac{(b-a)^2}{N}
$$

می‌شه. خلاصه، دقّت با نرخ $1/\sqrt{N}$ بهتر می‌شه. نکته باحال اینه که Monte Carlo integration **dimension-agnostic**ـه: توی هر بعدی فقط سمپل می‌گیریم، پس دردسر انفجار ابعاد براش نیست.

یه جور فهم شهودی دیگه، مدل **hit-or-miss**ه. فرض کن $f(x)$ تو $[a,b]$ غیرمنفی باشه. یه مستطیل بکش که رو محور $x$ از $a$ تا $b$ و رو $y$ از ۰ تا یه $H$ (یه کران بالا برای $f$) کشیده شده. مساحت مستطیل $(b-a)H$ـه. حالا $N$ تا نقطه رو تو این مستطیل رندوم سمپل کن. هر نقطه‌ای که مختصه $y$-ش زیر منحنی باشه یه “hit” حساب می‌شه. نسبت hitها تقریباً برابر می‌شه با

$$
\frac{ \int_a^b f(x)\,dx }{(b-a)H }.
$$

پس:

$$
\int_a^b f(x)\,dx \;\approx\; (b-a)H \times \frac{\#\text{hit}}{N}.
$$

اگه $H$ رو دقیقاً بذاریم ماکس $f$، سمپل‌های “بالای” منحنی عملاً هدر می‌رن. برای همین فرمول میانگین مستقیم معمولاً به‌صرفه‌تره، ولی دید hit-or-miss واسه تجسم خیلی کمک می‌کنه.

*Monte Carlo integration با رندوم سمپل («hit-or-miss»).* تو این شکل، مساحت زیر منحنی $y=\sin x$ رو تو $[0,\pi]$ حدس می‌زنیم. نقاط رندوم تو باکس محدود‌کننده ($0\le x\le\pi$ و $0\le y\le1$) سمپل می‌شن؛ سبزها hit و قرمزها miss. نسبت سبزها × مساحت باکس یه تخمین از $\int_0^\pi \sin x \,dx = 2$ می‌ده. هرچی $N$ بره بالا، تخمین دقیق‌تر می‌شه.

**چرا Monte Carlo رو برای انتگرال‌گیری انتخاب کنیم؟** تو یه بعد و برای توابع خوش‌اخلاق، روش‌های قطعی معمولاً سریع‌ترن. ولی Monte Carlo وقتی جذابه که:

1. **High-dimensional انتگرال‌ها** – هزینه شبکه‌ای‌ها با بعد منفجر می‌شه، ولی تعداد سمپل‌های Monte Carlo همون $1/\sqrt{N}$ می‌مونه.
2. **دامنه‌های عجیب یا integrand‌های ناجور** – با سمپل تصادفی دیگه نگران tractability تحلیلی نیستیم.
3. **کنترل خطای سرانگشتی** – با واریانس نمونه می‌تونیم حدود خطا رو تخمین بزنیم و با سمپل بیشتر محکم‌ترش کنیم.

![1graph](Pictures/2.png)

در عمل، روش‌هایی مثل importance sampling یا stratified sampling برای کاهش واریانس استفاده می‌شن، ولی اصل داستان همون سمپل و میانگین گرفتن ساده‌س.

حالا که Monte Carlo estimation رو دیدیم، برمی‌گردیم سراغ RL. تو RL، روش‌های Monte Carlo همین ایده‌ها رو برای تخمین **value function**ها و **optimize کردن policy**ها از روی داده اپیزودیک به کار می‌گیرن. بخش‌های بعدی می‌گن چطوری سمپل‌گیری Monte Carlo برای **prediction** (تخمین ارزش یه policy مشخص) و **control** (بهبود تدریجی همون policy) استفاده می‌شه، به‌علاوه نکاتی مثل first-visit vs. every-visit averaging و آپدیت‌های incremental.
