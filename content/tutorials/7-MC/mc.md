#  متدهای Monte Carlo

متدهای Monte Carlo یه سری الگوریتم محاسباتی هستن که با **random sampling** جواب عددی مسائلی رو که در اصل می‌شه به‌صورت قطعی حل کرد، در میارن. این متدها تو کلی حوزه ـ از فیزیک و فایننس گرفته تا آمار و هوش مصنوعی ـ حسابی کاربرد دارن، مخصوصاً وقتی حل مستقیم سخته یا توزیع‌های پشت پرده معلوم نیست. تو RL هم متدهای Monte Carlo به ایجنت کمک می‌کنن value function‌ها و policy‌هاش رو **فقط با experience** (اپیزودهای sample-شده) یاد بگیره، بدون این‌که احتیاج به یه مدل از محیط داشته باشه. قبل از این‌که بریم سراغ Monte Carlo تو RL، اول باید فرق **planning (مدل-based)** و **learning (مدل-free)** رو روشن کنیم.

## Planning در برابر Learning تو RL

توی RL دو راه اصلی واسه تصمیم‌گیری داریم: **planning** (معمولاً مدل-based) و **learning** (مدل-free). فرق کلّی‌شون اینه که ایجنت یه **مدل از dynamics محیط** داره یا نه:

* **Planning (Model-Based RL):** ایجنت یه **مدل** از state-ترنزیشنها و ریواردها داره و باهاش جلو جلو plan می‌ریزه. وقتی $M(s,a)$ که next stateها و ریواردها رو پیش‌بینی می‌کنه دم دست باشه، ایجنت می‌تونه با «رول-اوت» ذهنی مسیرهای آینده، اکشن‌هاشو بدون تماس واقعی با environment ارزیابی کنه. روش‌های کلاسیک planning مثلاً **DP** هستن (که یه مدل کامل از MDP می‌خوان) یا تکنیک‌هایی مثل **Monte Carlo Tree Search (MCTS)**. خلاصه اینجا محاسبه رو می‌سپریم به مدل نه تجربه واقعی. مثلاً اگه $T(s,a,s')$ (ترنزیشن function) و $R(s,a)$ معلوم باشن، ایجنت می‌تونه معادلات Bellman رو مستقیم با DP حل کنه. MCTS هم با یه simulator (generative مدل) *imaginatively* دنباله‌های اکشن آینده رو ـ مثلاً تو Go یا chess ـ شبیه‌سازی می‌کنه و از توی یه search tree اکشنِ امیدبخش رو در میاره.

* **Learning (Model-Free RL):** ایجنت **از experience خام یاد می‌گیره** و هیچ مدل از dynamics محیط نداره. با trial-and-error policyشو بهتر می‌کنه و value-estimateها یا پارامترهای policy رو طبق **ریواردها و ترنزیشنهای** دیده-شده آپدیت می‌کنه. مدل-free methods فرض نمی‌کنن $T(s,a,s')$ یا $R(s,a)$ معلومه؛ محیط یه black-boxه و policy خوب از تعامل مستقیم بیرون میاد. نمونه‌ها: **Monte Carlo methods** (میانگین returns برای یادگیری value function)، **Temporal-Difference (TD)** methods مثل SARSA یا Q-learning، و policy-gradient methods. این روش‌ها رفتار رو با داده هر چی *واقعاً* موقع اجرای اکشن‌ها اتفاق افتاده تنظیم می‌کنن، نه با planning فرضی.

Planning و learning به هم نزدیکن ـ هدف هر دو آخر سر بهبود policyه ـ ولی منبع اطلاعاتشون فرق می‌کنه. Planning وقتی یه مدل دقیق داری معمولاً **sample-efficient**ه: ایجنت می‌تونه ارزون هر چقدر تجربه فرضی خواست بسازه. Learning وقتی dynamics محیط ناشناخته یا خیلی پیچیده‌ست لازمه (و گاهی تنها گزینه‌ست). تو واقعیت، مدل دقیق کم گیر میاد، واسه همین مدل-free learning کلی کاربرد داره. البته می‌شه مدل-based و مدل-free رو قاطی کرد (مثلاً اول مدل رو یاد گرفت بعد باهاش planning کرد)، ولی متدهای Monte Carlo تو دستهٔ **مدل-free RL** می‌افتن ـ یعنی کامل از اپیزودهای دیده-شده یاد می‌گیرن و هیچ مدل از قبل داده نمی‌شه.

*Illustration of مدل-based vs. مدل-free RL:* تو روش‌های مدل-based (planning) ایجنت با یه *مدل* داخلی از dynamics محیط outcomeها رو شبیه‌سازی می‌کنه و برای اکشن‌های بهینه plan می‌چینه (پیکان‌های نقطه‌چین یعنی look-ahead با مدل). تو روش‌های مدل-free (learning) اصلاً مدل استفاده نمی‌شه ـ ایجنت فقط از تعامل واقعی با محیط یاد می‌گیره. Planning یه مدل شناخته‌شده یا یادگرفته‌شده لازم داره و به ایجنت اجازه می‌ده «فکر» کنه، در حالی که مدل-free learning روی trial-and-error سوار می‌شه.

خلاصه این که **planning** (مدل-based RL) با داشتن مدل آینده‌نگری می‌کنه؛ اگه مدل مطمئن باشه، محاسبه‌ها سریع و ارزون درمیاد چون ایجنت می‌تونه کلی سناریوی فرضی رو تو یه چشم بر هم زدن بررسی کنه. **Learning** (مدل-free RL) بی‌خیال مدل می‌شه و از تجربه واقعی یاد می‌گیره؛ وقتی مدل در دسترس یا قابل‌حل نباشه همین راه رو داریم. متدهای Monte Carlo تو RL هم *مدل-free* هستن: ایجنت value-estimateها و policyشو فقط با میانگین گرفتن از experience episodic واقعی (یا شبیه‌سازی‌شده) بهتر می‌کنه، نه با حل معادلات تو یه مدل MDP معلوم.








## یه مقدمهٔ درباره‌ی Monte Carlo

روش‌های **Monte Carlo** در واقع یه دستهٔ بزرگ از الگوریتم‌ها هستن که حساب‌کتابشونو می‌سپرن به شانس. ایدهٔ اصلی اینه که با **random sampling** بری سراغ تخمین کمیت‌های ریاضی—اکثراً expectationها، انتگرال‌ها یا جواب مسائلی که نمی‌شه مستقیم و دقیق حلشون کرد. وقتی یه عالمه experiment تصادفی انجام بدی و آخرش از نتایج میانگین بگیری، Monte Carlo یه عدد تخمینی تحویلت می‌ده. پشتِ این کار، **Law of Large Numbers** خوابیده و خیالتو راحت می‌کنه که هرچی سمپل‌ها بیشتر بشن، این میانگین‌ها تهش می‌چسبن به عدد واقعی.

اگه بخوایم تعریفیشو بگیم، Monte Carlo معمولاً برای تخمین یه expectation یا یه انتگرال استفاده می‌شه. فرض کن می‌خوایم expectation
$I = \mathbb{E}[f(X)]$
رو نسبت به یه توزیع $p(x)$ دربیاریم. می‌شه اینو به شکل یه انتگرال برای متغیر تصادفی پیوسته $X$ نوشت:

$$
I \;=\; \int_{\Omega} f(x)\,p(x)\,dx,
$$

که $\Omega$ هم محدودهٔ $X$ه. خیلی وقت‌ها همچین expectation یا انتگرالی رو نمی‌شه با قلم و کاغذ درآورد. اینجاست که Monte Carlo میاد سراغ یه روش *simulation-based*. ما $N$ تا سمپل مستقل $x_1, x_2, \ldots, x_N$ از $p(x)$ می‌گیریم و بعد میانگین $f(x)$ روی این سمپل‌ها رو حساب می‌کنیم:

$$
\hat{I}_N \;=\; \frac{1}{N} \sum_{i=1}^{N} f(x_i).
$$

این $\hat{I}_N$ می‌شه **estimator** Monte Carlo برای $I$. چون سمپل‌ها رو درست از $p$ گرفته‌ایم، $\hat{I}_N$ بدون بایاسه، یعنی $\mathbb{E}[\hat{I}_N] = I$. دلیلش هم ساده‌س:
$\mathbb{E}[\hat{I}_N] = \tfrac{1}{N}\sum_{i=1}^N \mathbb{E}[f(x_i)] = \mathbb{E}[f(X)] = I$.
علاوه بر این، **LLN** می‌گه وقتی $N$ بره به بی‌نهایت، میانگین $\hat{I}_N$ (تقریباً حتماً) می‌چسبه به $I$. یعنی با یه عالمه random سمپل، خروجیِ میانگین هر قدر بخوای نزدیک می‌شه به مقدار واقعی. خوبیش اینه که شکل $f(x)$ یا $p(x)$ هر چی باشه (تا وقتی واریانس محدود باشه) این قانون کار می‌کنه؛ همین باعث می‌شه Monte Carlo هم کاربردی باشه، هم قوی.

**Convergence و واریانس:** دقت تخمین Monte Carlo با زیاد کردن $N$ بهتر می‌شه، ولی به‌صورت statistical جلو می‌ره، نه deterministic. طبق **CLT**، برای $N$ بزرگ، $\hat{I}_N$ دوروبر $I$ تقریباً نرمال پخش می‌شه با واریانس $\sigma^2/N$، جایی که $\sigma^2 = \mathrm{Var}(f(X))$ه:

$$
\hat{I}_N \;\approx\; \mathcal{N}\!\Big(I,\; \tfrac{\sigma^2}{N}\Big).
$$

پس خطا با مرتبهٔ $1/\sqrt{N}$ کم می‌شه. این نرخ $O(1/\sqrt{N})$ کند محسوب می‌شه: اگه بخوای خطا رو نصف کنی، باید $N$ رو چهار برابر کنی. بخوای یه رقم اعشار دقیق‌تر بشی، معمولاً باید صد برابر سمپل بیشتر جمع کنی—یه trade-off بین دقت و هزینهٔ محاسباتی. با این حال برگ برندهٔ Monte Carlo اینه که **dimension-independence** داره: همین $1/\sqrt{N}$ توی بعدهای بالا هم همینه و بدتر نمی‌شه. واسه انتگرال‌های پُربُعد یا سیستم‌های probabilistic پیچیده که روش‌های grid-base شده قربانی **curse of dimensionality** می‌شن، Monte Carlo حسابی می‌درخشه.

البته Monte Carlo فرض می‌کنه **سمپل‌ها independent** هستن. وقتی سمپل مستقل و i.i.d. از $p(x)$ داشته باشی، هم unbiasedness رو داری هم می‌تونی LLN و CLT رو بی‌دردسر اعمال کنی. اگه سمپل‌ها correlated باشن (مثل بعضی تکنیک‌های پیشرفته Monte Carlo)، تحلیل خطا سخت‌تر می‌شه—عملاً تعداد داده‌های مستقلت کمتر از $N$ می‌شه. این‌جا ولی برای راحتی، استقلال رو فرض می‌کنیم.

کاربردهای Monte Carlo کلی فراتر از RLه. از **Manhattan Project** برای تخمین neutron diffusion بگیر تا فیزیک، فایننس (مثل pricing مشتقات با شبیه‌سازی random price pathها)، Bayesian statistics (تخمین expectationهای posterior)، گرافیک کامپیووتری (رندر صحنه با random ray sampling) و خیلی حوزه‌های دیگه. برای جا افتادن قضیه، دو تا طModel ساده می‌زنیم: تخمین $\pi$ و حساب یه انتگرال مشخص با Monte Carlo. این مثالا نشون می‌دن چطور random sampling می‌تونه مسئله‌های هندسی و حسابیِ ظاهراً قطعی رو حل کنه.









#### مثال: تخمین $\pi$ با Random Sampling

یه آزمایش کلاسیک Monte Carlo برای تخمین $\pi$ اینه که یه مشت نقطهٔ رندوم توی یه صفحخ پرت کنیم. فرض کن یه دایره با شعاع $1$ توی یه مربع واحد جا شده. برای شفافیت، یه مربع واحد توی ربع اول (گوشه‌هاش $(0,0)$ و $(1,1)$) و یه ربع از یه دایره واحد با شعاع $1$ داخلش در نظر بگیر؛ همون قوسی که از $(1,0)$ تا $(0,1)$ می‌ره. مساحت ربع دایره می‌شه

$$
\frac{\pi r^{2}}{4} = \frac{\pi}{4}
$$

(چون $r=1$) و مساحت مربع واحد هم که $1$ه. پس اگه یه نقطه رو کاملاً تصادفی توی مربع $[0,1]\times[0,1]$ بندازیم، احتمال این‌که اون نقطه بیفته توی ربع دایره دقیقاً $\pi/4$ـه. یعنی

$$
P\{(x,y)\ \text{inside circle}\} = 
\frac{\text{Area of quarter-circle}}{\text{Area of square}}
= \frac{\pi/4}{1} = \frac{\pi}{4}.
$$

برای exploit کردن این قضیه، کافیه کلی نقطهٔ Random توی مربع واحد بسازیم و ببینیم چندتاشون داخل ربع دایره قرار می‌گیرن (همون شرط $x^{2}+y^{2}\le 1$ رو چک می‌کنیم). بعد اون کسر رو در $4$ ضرب کنیم تا یه تخمین از $\pi$ بگیریم. وقتی $N$ خیلی بزرگ می‌شه، طبق LLN این تخمین می‌چسبه به مقدار واقعی $\pi$.

1. **سمپل‌گیری:** $N$ تا نقطه $(x_{i},y_{i})$ با $x_{i},y_{i}\sim\mathrm{Uniform}(0,1)$ بساز. هر نقطه به یه اندازه شانس داره هر جای مربع واحد فرود بیاد.
2. **شمارش نقاط داخل دایره:** برای هر نقطه چک کن آیا داخل ربع دایره می‌افته، یعنی $x_{i}^{2}+y_{i}^{2}\le 1$؛ بعد یه indicator تعریف کن

   $$
   I_i = \begin{cases}
   1 & \text{if } x_i^{2}+y_i^{2}\le 1,\\[4pt]
   0 & \text{otherwise.}
   \end{cases}
   $$

   هر $I_i$ یه Bernoulli با احتمال $\pi/4$ می‌شه $1$ و با احتمال $1-\pi/4$ می‌شه $0$.
3. **تخمین $\pi$:** میانگین سمپل این indicatorها، $\tfrac{1}{N}\sum_{i=1}^{N} I_i$، احتمال افتادن داخل دایره رو می‌سنجه. بعد در $4$ ضربش کن تا $\pi$ دربیاد:

   $$
   \pi \approx 4 \times \frac{\#\{\text{points inside circle}\}}{N}.
   $$

چون $\mathbb{E}[I_i]=\pi/4$, این Estimator بی‌طرفه. هرچی سمپل بیش‌تر بندازی، نسبت $\tfrac{\text{inside}}{N}$ به $\pi/4$ نزدیک‌تر می‌شه و طبیعتاً $4\times(\text{inside}/N)$ هم به $\pi$ نزدیک می‌شه. انحراف معیار این تخمین حدود

$$
4\sqrt{\frac{(\pi/4)(1-\pi/4)}{N}}
$$

ه که تقریباً با $\tfrac{\text{const}}{\sqrt{N}}$ پایین می‌آد. یعنی افزایش $N$ دقت رو زیاد می‌کنه، ولی با بازده کاهنده.

*شبیه‌سازی Monte Carlo برا تخمین $\pi$.* توی این روش، نقاطی رو یکنواخت توی مربع واحد می‌ریزیم و اونایی رو که داخل ربع از دایره واحد افتادن می‌شماریم. این‌جا $N=300$ سمپل نشون داده شده. نسبت نقاط داخل ربع دایره به کل نقاط برآوردی از $\pi/4$ می‌ده. توی این اجرای نمونه، حدود سه‌چهارمِ نقاط داخل افتادن و $\pi \approx 4 \times 0.75 = 3.0$ دراومد؛ هرچی $N$ رو ببری بالا، این عدد به $\pi \approx 3.1416$ نزدیک‌تر می‌شه. ترفند اینه که احتمال افتادن داخل ربع دایره برابر $\pi/4$ـه، پس $4\times(\text{inside fraction})$ تقریب $\pi$ محسوب می‌شه.

این مثال $\pi$ نشون می‌ده چه‌طور Monte Carlo با تکیه بر تصادف یه پدیدهٔ پیچیده رو سمپل می‌کنه (اینجا رابطهٔ هندسی بین مساحت دایره و مربع) و با میانگین‌گیری عددی کمیت دلخواه رو تقریب می‌زنه. بدون نیاز به محاسبهٔ انتگرال دقیق یا فرمول‌بازی، فقط سمپل و میانگین. دقت این روش با $1/\sqrt{N}$ بالا می‌ره، ولی برای دقت خیلی زیاد گاهی باید میلیون‌ها نقطه سمپل کنی؛ این همون معاملهٔ زمان در برابر دقته که Monte Carlo بهش تکیه می‌کنه وقتی راه بهتر دیگه‌ای در دسترس نیست.


![1graph](Pictures/1.png)






#### مثال: Monte Carlo Integration

روش‌های Monte Carlo می‌تونن **انتگرال‌های حد دار** رو هم با رندوم سمپل‌گیری تقریب بزنن و یه جایگزین باحال برای روش‌های عددی انتگرال‌گیری deterministic بدن. فرض کن یه تابع حقیقی $f(x)$ داریم و می‌خوایم روی بازه $[a,b]$ انتگرالش رو حساب کنیم:

$$
I \;=\; \int_a^b f(x)\,dx.
$$

وقتی $f(x)$ پیچیده باشه یا حدود انتگرال high-dimensional باشه، روش‌های کلاسیک مثل Simpson’s rule یا Gaussian quadrature کلی دردسر درست می‌کنن. Monte Carlo integration یه رویکرد خیلی سرراسته:

* **روش Uniform سمپل:** اول $N$ تا نقطه $x_1,\dots,x_N$ رو از بازه $[a,b]$ به صورت یکنواخت سمپل می‌کنیم. بعد $f(x)$ رو روی هر کدوم حساب می‌کنیم و میانگین می‌گیریم، بعدش در طول بازه یعنی $(b-a)$ ضرب می‌کنیم:

$$
\hat{I}_N \;=\; \frac{b-a}{N}\sum_{i=1}^N f(x_i).
$$

این $\hat{I}_N$ می‌شه تخمین‌گر Monte Carlo برای انتگرال. ایده از این رابطه میاد که

$$
\int_a^b f(x)\,dx \;=\; (b-a)\,\mathbb{E}_{x\sim U(a,b)}[f(x)].
$$

عملاً انتگرال رو به عنوان مساحت زیر $f(x)$ می‌بینیم و با سمپل‌گیری رندوم از $[a,b]$ و گرفتن میانگین ارتفاع‌ها تقریبش می‌کنیم.

طبق Law of Large Numbers، وقتی $N$ خیلی بزرگ بشه، $\hat{I}_N$ می‌ره سمت مقدار واقعی $I$. این تخمینگر unbiasedـه، چون

$$
\mathbb{E}[\hat{I}_N] = I.
$$

به کمک CLT هم خطای تخمین برای $N$ بزرگ تقریباً نرمال با واریانس

$$
\mathrm{Var}(f(X))\,\frac{(b-a)^2}{N}
$$

می‌شه. خلاصه، دقّت با نرخ $1/\sqrt{N}$ بهتر می‌شه. نکته باحال اینه که انتگرال گیری با مونت کارلو **dimension-agnostic**ـه: توی هر بعدی فقط سمپل می‌گیریم، پس دردسر ابعاد براش نیست.

یه جور فهم شهودی دیگه، مدل **hit-or-miss**ه. فرض کن $f(x)$ تو $[a,b]$ غیرمنفی باشه. یه مستطیل بکش که رو محور $x$ از $a$ تا $b$ و رو $y$ از ۰ تا یه $H$ (یه کران بالا برای $f$) کشیده شده. مساحت مستطیل $(b-a)H$ـه. حالا $N$ تا نقطه رو تو این مستطیل رندوم سمپل کن. هر نقطه‌ای که مختصه $y$-ش زیر منحنی باشه یه “hit” حساب می‌شه. نسبت hitها تقریباً برابر می‌شه با

$$
\frac{ \int_a^b f(x)\,dx }{(b-a)H }.
$$

پس:

$$
\int_a^b f(x)\,dx \;\approx\; (b-a)H \times \frac{\#\text{hit}}{N}.
$$

اگه $H$ رو دقیقاً بذاریم ماکس $f$، سمپل‌های “بالای” منحنی عملاً هدر می‌رن. برای همین فرمول میانگین مستقیم معمولاً به‌صرفه‌تره، ولی دید hit-or-miss واسه تجسم خیلی کمک می‌کنه.

انتگرال گیری Monte Carlo با رندوم سمپل («hit-or-miss»).* تو این شکل، مساحت زیر منحنی $y=\sin x$ رو تو $[0,\pi]$ حدس می‌زنیم. نقاط رندوم تو باکس محدود‌کننده ($0\le x\le\pi$ و $0\le y\le1$) سمپل می‌شن؛ سبزها hit و قرمزها miss. نسبت سبزها × مساحت باکس یه تخمین از $\int_0^\pi \sin x \,dx = 2$ می‌ده. هرچی $N$ بره بالا، تخمین دقیق‌تر می‌شه.


![1graph](Pictures/2.png)

در عمل، روش‌هایی مثل importance sampling یا stratified sampling برای کاهش واریانس استفاده می‌شن، ولی اصل داستان همون سمپل و میانگین گرفتن ساده‌س.

حالا که Monte Carlo estimation رو دیدیم، برمی‌گردیم سراغ RL. تو RL، روش‌های Monte Carlo همین ایده‌ها رو برای تخمین **value function**ها و **optimize کردن policy**ها از روی داده اپیزودیک به کار می‌گیرن. بخش‌های بعدی می‌گن چطوری سمپل‌گیری Monte Carlo برای **prediction** (تخمین ارزش یه policy مشخص) و **control** (بهبود تدریجی همون policy) استفاده می‌شه، به‌علاوه نکاتی مثل first-visit vs. every-visit averaging و آپدیت‌های incremental.













### Monte Carlo Prediction

توی reinforcement learning، وقتی می‌گیم یه مسئله‌ی prediction، معمولاً منظورمون اینه که value function رو برای یه policy مشخصِ$\pi$ارزیابی کنیم. ایده‌ی Monte Carlo prediction اینه که$V^\pi(s)$(یا$Q^\pi(s,a)$) رو با استفاده از sample returnها از تعداد زیادی episode که توشون policy$\pi$رو دنبال می‌کنیم، تخمین بزنیم.

یادآوری: یه episode یه دنباله از stateها، اکشن‌ها و ریواردهاست که از یه initial state شروع می‌شه و آخرش به یه terminal state می‌رسه و تموم می‌شه. مثلاً می‌تونه یه بار کامل بازی کردن از شروع تا پایان باشه، یا یه اجرای منفرد از یه ایجنت تا وقتی که به goal یا failure state برسه. به صورت تعریفی، یه episode یه trajectory می‌سازه:

$$
s_0,a_0,r_1,s_1,a_1,r_2,\ldots,s_{T-1},a_{T-1},r_T,\;s_T=terminal,
$$

که توش$r_t$ریواردیه که بعد از گرفتن اکشن$a_{t-1}$توی state$s_{t-1}$می‌گیریم (پس$r_t$مربوط به transition از$s_{t-1}$به$s_t$ـه). اندیس زمانی$t$هم گام‌های زمانی داخل episode رو می‌شمره. اگر episode توی گام$T$terminate بشه،$s_T$یه absorbing terminal stateـه و بعد از$r_T$دیگه هیچ ریواردی نداریم. طول episodeها می‌تونه ثابت باشه یا تصادفی، ولی طبق تعریفِ یه episodic task، هر episode بالاخره به یه terminal state ختم می‌شه.

return از time step$t$به صورت جمعِ تجمعیِ discounted ریواردها از اون نقطه تا آخر episode تعریف می‌شه. اگر discount factor رو با$\gamma\in[0,1]$نشون بدیم، return یعنی$G_t$اینه:

$$
G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\cdots+\gamma^{T-t-1}r_T.
$$

یا به یه شکل جمع‌بندی‌شده‌تر:

$$
G_t=\sum_{k=0}^{T-t-1}\gamma^k\,r_{t+1+k}.
$$

اگر$\gamma=1$باشه، return همون جمع ریواردها تا زمان termination می‌شه (برای episodic taskهایی که$T$محدودِ خیلی معنی‌دار و طبیعی‌ه). اگر$\gamma<1$باشه، ریواردهای آینده به صورت هندسی کم‌وزن می‌شن، که کمک می‌کنه$G_t$حتی توی infinite-horizon (continuing) taskها هم finite بمونه؛ هرچند Monte Carlo methodها معمولاً فرضِ episodic scenario (یا حداقل یه راهی برای truncate کردن episodeها) رو دارن.

Monte Carlo prediction از همین returnها برای ارزیابی policy استفاده می‌کنه. طبق تعریف، مقدار$V^\pi(s)$برابر امیدریاضی returnـه وقتی از state$s$شروع می‌کنیم و بعدش policy$\pi$رو جلو می‌ریم:

$$
V^\pi(s)=\mathbb{E}_\pi[\,G_t\mid s_t=s\,],
$$

یعنی فرض می‌کنیم تو زمان$t$ایجنت تو state$s$قرار داره و از اون به بعد مطابق$\pi$رفتار می‌کنه. نکته‌ی مهم اینه که Monte Carlo methodها لازم ندارن state transition probabilityها یا reward function رو بدونن؛ به جاش این expectation رو به صورت تجربی و با میانگین گرفتن از returnهای واقعی‌ای که از visitهای متعددِ state$s$تحت policy$\pi$می‌بینیم، تقریب می‌زنن.

به طور مشخص، فرض کن ایجنت policy$\pi$رو برای$N$تا episode دنبال می‌کنه. هر بار که state$s$توی این episodeها دیده می‌شه، return مربوط به بعدش رو ثبت می‌کنیم. فرض کن تو کل این episodeها،$s$جمعاً$N(s)$بار visited می‌شه. این رخدادها رو با زمان‌های$t_1,t_2,\ldots,t_{N(s)}$نشون می‌دیم (هر$t_i$یه اندیس زمانی داخل یه episodeـه که ایجنت تو state$s$بوده؛ اگر first-visit MC انجام بدیم، منظور اولین باریه که$s$تو اون episode دیده شده). پس یه مجموعه return داریم:$\{G_{t_1},G_{t_2},\ldots,G_{t_{N(s)}}\}$که هر$G_{t_i}$هم return بعد از همون visit به$s$ـه. حالا تخمین Monte Carlo از$V^\pi(s)$می‌شه میانگین این returnها:

$$
V^\pi(s)\approx\frac{1}{N(s)}\sum_{i=1}^{N(s)}G_{t_i}.
$$

این در اصل یه sample mean ساده‌ست. طبیعتاً وقتی$N(s)\to\infty$(با این فرض که policy و environment ثابت و stationary بمونن)، این مقدار به$V^\pi(s)$واقعی converge می‌کنه، چون عملاً داریم expectationِ$G_t$رو با Monte Carlo sampling حساب می‌کنیم. از اون‌جایی که هر episode کامل یه sample از اینه که «از state$s$به بعد چه total ریواردی جمع می‌شه»، میانگین گرفتن از episodeهای مستقلِ زیاد، یه برآورد قابل‌اتکا از expected return می‌ده.

یه نکته‌ی خیلی کلیدی اینه که Monte Carlo evaluation تا وقتی episode تموم نشه، value estimateها رو update نمی‌کنه. یعنی فقط بعد از این‌که outcome واقعیِ episode رو دیدیم (همون total return)، این method میاد به stateها credit می‌ده. این با Temporal-Difference methodها فرق داره که بعد از هر گام و با bootstrapping valueها رو update می‌کنن. Monte Carlo از نتیجه‌ی کاملِ episode استفاده می‌کنه و عملاً bootstrapping bias نداره. ولی در عوض این یعنی Monte Carlo methodها به شکل مستقیم بیشتر برای episodic taskها جا می‌افتن—چون value هر state رو با یه دنباله ریوارد تعریف می‌کنیم که نهایتاً terminate می‌شه. توی یه continuing task (که episode طبیعی نداره)، یا باید خودمون episode cutoff بذاریم، یا کلاً به جای اون از TD methodها استفاده کنیم. تو عمل البته می‌شه Monte Carlo رو برای continuing taskها هم با truncating episodeها یا با$\gamma<1$(که باعث می‌شه returnها همگرا بمونن) به کار برد، ولی حالت episodic معمولاً شفاف‌تره.

**Updating Value Estimates:** بعد از این‌که تعداد زیادی episode تحت policy$\pi$اجرا کردیم، می‌تونیم برای همه‌ی stateهایی که visited شدن یه value function تجربی$V^\pi(s)$بسازیم. توی پیاده‌سازی، معمولاً$V(s)$رو یه جور دلخواه initialize می‌کنن و بعد returnها رو به صورت incremental داخل$V(s)$میانگین می‌گیرن. مثلاً هر بار که state$s$visited می‌شه و یه return به اسم$G$می‌بینیم، می‌شه این update رو انجام داد:

$$
V(s)\leftarrow V(s)+\alpha[\,G-V(s)\,],
$$

که توش$\alpha=\frac{1}{N(s)}$رو به عنوان stepsize (یعنی معکوس تعداد visitها) در نظر می‌گیریم. این دقیقاً معادل اینه که مجموع returnها رو جمع کنیم و بر تعدادشون تقسیم کنیم، با این فرق که اجازه می‌ده آنلاین و incremental جلو بریم، بدون این‌که لازم باشه همه‌ی returnهای قبلی رو نگه داریم. ما این فرمولِ “incremental Monte Carlo” رو تو بخش 3.6 بیشتر باز می‌کنیم، ولی شهودش اینه که یه running average انجام می‌ده: بعد از$n$بار visit،$V(s)$می‌شه میانگین همون$n$تا return.

Monte Carlo prediction یه روش model-freeـه: ایجنت لازم نیست transition probabilityها یا expected rewardها رو بدونه، فقط کافیه policy رو دنبال کنه و ببینه چی می‌شه. همین باعث می‌شه خیلی general باشه—حتی می‌شه صرفاً با مشاهده‌ی experienceهای یه ایجنت ازش استفاده کرد (حتی off-policy هم شدنیه، که اون موقع به importance sampling correction نیاز داریم، هرچند این فعلاً خارج از scope ماست). نقطه‌ضعفش اینه که ممکنه برای دقیق شدن estimateها به episodeهای زیادی نیاز داشته باشه، مخصوصاً برای stateهایی که variance returnشون بالاست. علاوه بر این، چون updateها فقط وقتی episode کامل تموم شد اتفاق می‌افتن، اگر episodeها طولانی باشن learning می‌تونه کند پیش بره. با این حال، Monte Carlo methodها از نظر مفهومی خیلی ساده‌ن و یه راه نسبتاً unbiased می‌دن برای یاد گرفتن valueها مستقیم از experience. تو موقعیت‌هایی مثل بازی‌ها یا simulationها که می‌تونیم episodeهای زیادی تولید کنیم، Monte Carlo eva
