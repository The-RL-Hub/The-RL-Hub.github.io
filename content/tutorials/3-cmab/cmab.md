# Contextual Bandits

## مقدمه

Contextual bandits یه دسته از مسائل تصمیم‌گیری sequentialهستن که تو هر مرحله، یه ایجنت یه سری اطلاعات جانبی (که بهش *context* می‌گن) رو می‌بینه و باید یه اکشن (که معمولا *arm* نامیده می‌شه) رو انتخاب کنه تا ریوارد بگیره. برعکس multi-armed bandit که context نداره، contextual bandits این امکان رو به ایجنت می‌ده که تصمیماتش رو بر اساس contextی که دیده تنظیم کنه. این ویژگی، contextual bandits رو توی کاربردهایی مثل پیشنهادات شخصی‌سازی‌شده و تبلیغات آنلاین خیلی قوی می‌کنه، چون تصمیم‌گیری‌ها می‌تونن بر اساس اطلاعات کاربر یا محیط انجام بشن.

اگه بخوایم تعریف کنیم، یه contextual bandit رو می‌شه یه مسأله یادگیری تقویتی تک‌مرحله‌ای در نظر گرفت: تو هر مرحله، ایجنت یه context رو می‌بینه، یه اکشن انجام می‌ده و یه ریوارد می‌گیره، بدون اینکه حالت در طول زمان تغییر کنه. چالش اصلی اینه که یه policy یاد بگیریم که context رو به اکشن‌ها بهینه وصل کنه، درحالی‌که باید بین exploration گزینه‌های جدید و نامطمئن و exploitation گزینه‌هایی که قبلا خوب جواب دادن، تعادل برقرار کنه.

اصطلاح "contextual bandit" رو Langford و Zhang (2007) رایج کردن، هرچند این مسأله قبلا با اسم‌های مختلفی مثل *bandit problems with covariates*، *associative reinforcement learning*، *associative bandits* و *bandits with side information* هم بررسی شده. همه این اصطلاحات به یه ایده اصلی اشاره دارن: ایجنت یه سری اطلاعات جانبی (covariates) داره که می‌تونه توی تصمیم‌گیری برای انتخاب بهترین arm کمکش کنه. چون context باعث می‌شه ایجنت تصمیماتش رو *شخصی‌سازی یا شرطی‌سازی* کنه، contextual bandits از multi-armed banditهای ساده پیچیده‌تر و قوی‌ترن. اگه context بی‌اهمیت باشه یا تو همه مراحل ثابت بمونه، این مسأله همون multi-armed bandit کلاسیک می‌شه. اما وقتی ویژگی‌های context روی انتخاب بهترین arm تأثیر دارن (که تو واقعیت معمولا همین‌طوره)، نادیده گرفتن context باعث عملکرد ضعیف می‌شه. پس الگوریتم باید یه نگاشت از context به اکشن‌ها بهینه یاد بگیره، که هم پیچیدگی مسأله رو زیاد می‌کنه و هم باعث می‌شه توی مسائل واقعی کاربرد داشته باشه.

## کاربردها در دنیای واقعی

خیلی از مسائل دنیای واقعی رو می‌شه به‌عنوان contextual bandits مدل کرد. مثلا، یه سایت خبری رو در نظر بگیرین که باید تصمیم بگیره کدوم مقاله رو به بازدیدکننده‌ها نشون بده. هر بازدیدکننده یه سری اطلاعات contextual خاص داره (مثلا ویژگی‌های جمعیت‌شناختی یا تاریخچه مرور)، و سایت باید یه مقاله (*arm*) رو برای نمایش انتخاب کنه. بعدش بازدیدکننده یا روی مقاله کلیک می‌کنه یا نه، که یه ریوارد (مثلا 1 برای کلیک و 0 برای کلیک نکردن) ایجاد می‌شه. هدف اینه که یه policy یاد بگیریم که مقاله‌ای رو انتخاب کنه که بیشترین احتمال کلیک شدن رو داشته باشه و نرخ کلیک (click-through rate) رو به حداکثر برسونه. Yahoo! News این مسأله رو به‌عنوان یه contextual bandit مدل کرد، طوری که هر بازدید کاربر یه context محسوب می‌شد و مقالات خبری arms بودن. هر کاربر یه بردار ویژگی (سن، جنسیت، مکان و غیره) داشت و هر مقاله یه احتمال ناشناخته برای کلیک شدن توسط اون کاربر داشت. تو حالت ایده‌آل، *optimal policy* برای هر context کاربر، مقاله‌ای رو انتخاب می‌کرد که بیشترین احتمال کلیک رو داشته باشه. یه الگوریتم contextual bandit باید این ارتباط بین ویژگی‌های کاربر و بهترین مقاله رو با آزمایش مقالات مختلف و مشاهده ریوارد باینری (کلیک یا عدم کلیک) یاد بگیره.

همین ایده توی **تبلیغات آنلاین** هم استفاده می‌شه، جایی که context می‌تونه اطلاعاتی درباره یه بازدید از صفحه یا کاربر باشه و arms هم نمایش تبلیغات مختلف رو نشون بدن. هر تبلیغ یه نتیجه‌ای داره (مثلا کلیک بشه یا نشه)، و سیستم باید بفهمه که کدوم تبلیغ برای کدوم context بهتره. تو **سیستم‌های توصیه‌گر**، contextual bandits واسه شخصی‌سازی محتوا استفاده می‌شن، مثلا پیشنهاد محصول یا ویدئو بر اساس پروفایل کاربر و رفتارهای قبلیش، و بعدش بروزرسانی پیشنهادات بر اساس بازخورد (مثلا خرید یا مشاهده). برخلاف روش‌های آزمایش A/B یا آزمایش‌های کاملا تصادفی، الگوریتم‌های bandit به‌طور مداوم خودشون رو بهینه می‌کنن و درحالی‌که یاد می‌گیرن، حداکثرسازی ریوارد هم انجام می‌دن. این قابلیت که *در لحظه یاد بگیری و تصمیمات رو سفارشی‌سازی کنی* باعث شده contextual bandits یه ابزار خیلی مهم توی سیستم‌های یادگیری ماشین تعاملی باشن.













## تعریف مسئله‌ی Contextual Bandits

اگه بخوایم تعریف کنیم، یه *contextual bandit problem* اینطوریه که یه دنباله از تعامل‌ها داریم به اسم دور یا round که با $t = 1, 2, 3, \dots, T$ شماره‌گذاری شدن. توی هر دور $t$ این اتفاقا می‌افته:

- یه *context* به اسم $c_t$ (که بعضی جاها با $x_t$ هم نشون می‌دن) به ایجنت نشون داده می‌شه. این context از یه فضای context به اسم $\mathcal{C}$ میاد که می‌تونه محدود یا نامحدود باشه (مثلاً یه بردار ویژگی توی $\mathbb{R}^d$).
- بعد از دیدن context، ایجنت باید یه *arm* (همون اکشن خودمون) $a_t$ از بین مجموعه‌ی arms یعنی $\mathcal{A} = \{1,2,\dots,K\}$ انتخاب کنه. برای راحتی فرض کن $K$ تا اکشن گسسته داریم. البته می‌تونه $K$ نامحدود هم باشه ولی تو بیشتر الگوریتم‌های کلاسیک، فرض می‌کنن $K$ محدوده.
- وقتی ایجنت یه arm انتخاب می‌کنه (یعنی $a_t$)، یه *reward* یا همون ریوارد $r_{t, a_t}$ می‌گیره. با $r_{t,a}$ هم ریواردی رو نشون می‌دیم که اگه arm دیگه‌ای انتخاب شده بود توی همون context، دریافت می‌شد. نکته مهم اینه که ایجنت فقط ریوارد اون armی که بازی کرده رو می‌بینه. به این بازخورد می‌گن *bandit feedback*. یعنی ریوارد بقیه‌ی arms رو نمی‌تونه ببینه چون انتخابشون نکرده.

هدف ایجنت اینه که توی این $T$ تا دور، تا جایی که می‌تونه ریوارد جمع کنه. یا به بیان دیگه، سعی کنه *regret* خودش رو کم کنه نسبت به یه معیار مقایسه. برای اینکه regret رو دقیق تعریف کنیم، باید مشخص کنیم داریم با چی مقایسه می‌کنیم.

معمولاً فرض می‌کنیم یه تابع ریوارد واقعی (یا یه توزیع واقعی) وجود داره که به هر جفت $(c,a)$ یعنی یه context و یه arm، یه ریوارد انتظاری نسبت می‌ده. بذار $\mu(c,a) = \mathbb{E}[r_{t,a} \mid c_t = c]$ رو تعریف کنیم که یعنی ریوارد مورد انتظار برای arm $a$ وقتی context برابر با $c$ باشه.  

ما اینجا داریم حالت *stochastic* contextual bandit رو بررسی می‌کنیم. یعنی اینکه $(c_t, r_{t,1}, \dots, r_{t,K})$ توی هر دور به‌صورت i.i.d. از یه توزیع ثابت و ناشناخته به اسم $D$ میان. به بیان ساده‌تر، هر دور یه context و یه مجموعه‌ای از ریواردهای ممکن داریم که از یه توزیع ثابت کشیده شدن (مثلاً حالتی مثل $r_{t,a} = \mu(c_t,a) + \eta_t$ که $\eta_t$ یه نویز تصادفیه). ایجنت از اول نمی‌دونه $\mu(c,a)$ چیه و باید خودش یاد بگیره از روی بازخوردها.

یه policy که با $\pi: \mathcal{C} \to \mathcal{A}$ نشون داده می‌شه، یه نگاشته از context به arm ـه (ممکنه تصادفی هم باشه). حالا بذار $\pi^*$ رو policy بهینه فرض کنیم که برای هر context اون armی رو انتخاب می‌کنه که ریواردش بیشترینه. یعنی:  

$$
\pi^*(c) \in \arg\max_{a \in \mathcal{A}} \mu(c,a)
$$  

برای همه‌ی contextها.

این $\pi^*$ همون چیزی‌یه که ایجنت دوست داره مثل اون عمل کنه. حالا regret یعنی تفاوت بین چیزی که $\pi^*$ می‌تونه جمع کنه و چیزی که ایجنت با الگوریتمش جمع کرده.

اگه $a_t$ armی باشه که الگوریتم توی زمان $t$ انتخاب کرده، اون موقع regret لحظه‌ای توی اون لحظه می‌شه:

$$
\mu(c_t, \pi^*(c_t)) - \mu(c_t, a_t)
$$  

یعنی اینکه چقدر ضرر کردیم چون arm بهینه رو تو اون context بازی نکردیم.

مجموع regret (به‌صورت انتظاری) توی $T$ تا دور اینطوری تعریف می‌شه:

$$
R(T) \;=\; \mathbb{E}\Bigg[\sum_{t=1}^T \big(\mu(c_t,\pi^*(c_t)) - \mu(c_t, a_t)\big)\Bigg]
$$

یا معادلش:

$$
R(T) = T \cdot \mathbb{E}[\mu(c,\pi^*(c))] - \mathbb{E}\Big[\sum_{t=1}^T r_{t,a_t}\Big]
$$

که امید ریاضی توی این فرمول‌ها روی randomness مربوط به contextها و انتخاب armهای الگوریتم گرفته می‌شه.  

این مقدار بهمون نشون می‌ده که اگه از همون اول می‌دونستیم بهترین کار چیه، چقدر بیشتر می‌تونستیم ریوارد بگیریم. اگه الگوریتممون خوب یاد بگیره، باید یه جایی regretش زیرخطی بشه. یعنی $R(T) = o(T)$ وقتی $T$ می‌ره بالا. این یعنی اینکه regret به ازای هر دور می‌ره به صفر.

در حالت ایده‌آل، دنبال الگوریتم‌هایی هستیم که regretشون یه کرانی مثل $\tilde{O}(\sqrt{T})$ داشته باشه. حالا اگه هیچ فرضی روی $\mu(c,a)$ نذاریم (که می‌شه حالت *adversarial*)، regret بهینه به‌صورت minimax برابر با $O(\sqrt{T})$ می‌شه، مشابه با banditهای بدون context. ولی توی حالت stochastic که یه ساختاری توی مسئله هست، می‌تونیم حتی بهتر هم عمل کنیم بسته به شرایط مسئله.

---

### فرقش با banditهای معمولی چیه؟

توی contextual bandits یه چیز جدید داریم به اسم context که هر دور بهمون داده می‌شه. ولی توی multi-armed bandit ساده، همچین چیزی نیست. اونجا ایجنت فقط باید بین armها یکی رو انتخاب کنه و هر arm هم یه توزیع ثابت داره. بهترین کار اینه که بفهمی کدوم arm بهتره و همونو بیشتر بازی کنی.

ولی توی contextual bandit، ممکنه بسته به context، arm بهینه فرق کنه. پس ایجنت فقط دنبال یه arm خوب نیست، دنبال یاد گرفتن یه policyه که بگه تو هر context، کدوم arm بهتره.

اگه context فقط یکی باشه (یعنی همه دورها context یکسانی دارن)، اون موقع $\mu(c,a)$ به context وابسته نیست و مسئله می‌شه همون bandit ساده. اون‌وقت نادیده گرفتن context مشکلی ایجاد نمی‌کنه.

اما اگه contextها اطلاعات مفید داشته باشن، یه contextual bandit می‌تونه خیلی بهتر از الگوریتمی باشه که context رو نادیده می‌گیره، چون می‌تونه تصمیم‌هاشو با context هماهنگ کنه.

به یه شکل دیگه هم می‌تونیم نگاه کنیم: توی non-contextual bandit، داریم $K$ تا عدد یاد می‌گیریم (میانگین ریوارد هر arm)، ولی توی contextual bandit، داریم یه تابع یاد می‌گیریم $\mu(c,a)$ که می‌تونه برای بی‌نهایت context فرق کنه. این کار خیلی سخت‌تره، مگر اینکه یه فرضی مثل مدل خطی یا یه ساختار خاص برای $\mu(c,a)$ داشته باشیم.

در کل contextual bandit یه جور تعمیم‌یافته از multi-armed banditهاست که به ایجنت اجازه می‌ده از اطلاعات جانبی (context) استفاده کنه. ولی خب این باعث می‌شه مسئله سخت‌تر بشه چون ایجنت باید *online function approximation* یا *online supervised learning* انجام بده، اونم فقط با bandit feedback.








![full scheme](Pictures/1.png)
*شکل1: یه نمای شماتیک از چرخه‌ی تعامل contextual bandit. توی هر راند:
(1) ایجنت context فعلی رو می‌بینه (مثلاً یه کاربر با یه‌سری ویژگی خاص)،
(2) بعدش بر اساس همین context، یه اکشن از بین اکشن‌های ممکن انتخاب می‌کنه،
(3) اون اکشن اجرا می‌شه (مثلاً یه محتوا به کاربر نشون داده می‌شه)،
و (4) ایجنت یه سیگنال ریوارد یا فیدبک از environment می‌گیره (مثلاً اینکه کاربر کلیک کرده یا نه).
ایجنت از فیدبک‌هایی که همراه با context جمع کرده، استفاده می‌کنه تا policy خودش رو کم‌کم بهتر کنه.*





## مفاهیم پایه: Exploration، Exploitation و Function Approximation

توی مسئله‌ی **contextual bandit** یه چالش خیلی اساسی وجود داره که بهش می‌گن **exploration-exploitation trade-off** و واقعاً تو همه‌ی مسائل تصمیم‌گیری ترتیبی مهمه. توی این مدل، چون **contextual information** (یه جور اطلاعات جانبی) قبل از هر تصمیم‌گیری در اختیار الگوریتم قرار می‌گیره، این قضیه یه ذره پیچیده‌تر هم می‌شه. یعنی قبل از اینکه یه تصمیم بگیره (مثلاً انتخاب یه arm)، یه context بهش نشون داده می‌شه که باید توی انتخابش ازش کمک بگیره. هدف اینه که armی انتخاب شه که نسبت به context، بیشترین **expected reward** رو بده. ولی خب، الگوریتم اول راه، چیزی از توزیع‌های ریوارد نمی‌دونه، پس باید با آزمون و خطا و تعامل با محیط، خودش یاد بگیره.

### Exploration و Exploitation تو محیط‌های Contextual

وقتی الگوریتم داره **exploitation** می‌کنه، سعی می‌کنه همون لحظه بیشترین ریواردو بگیره، بر اساس چیزایی که تا الان از محیط فهمیده. ولی توی **exploration** می‌ره سراغ اکشن‌هایی که هنوز مطمئن نیست خوبن یا نه، ولی ممکنه کلی اطلاعات مفید بهش بدن و به درد آینده بخورن. این دو تا هدف با هم درگیرن، مخصوصاً توی contextual bandit، چون تو هر مرحله فقط ریوارد اون armی که انتخاب شده رو می‌بینی و بقیه‌شو نمی‌فهمی (**partial feedback** داریم دیگه).

فرض کنیم توی مرحله‌ی $t$، contextی که الگوریتم می‌بینه اینه: $c_t \in \mathcal{C}$، که این $\mathcal{C}$ کل فضای contextهاست. حالا $a_t \in \mathcal{A}$ اون armیه که انتخاب شده از بین $\mathcal{A} = \{1, 2, \dots, K\}$.

ریواردی که می‌بینیم $r_t = r(c_t, a_t)$ ـه که یه خروجی تصادفیه و از یه توزیع ناشناخته میاد که به context و arm بستگی داره. نکته‌ی مهم اینه که الگوریتم نمی‌فهمه بقیه‌ی armها (اونایی که انتخاب نکرده) چه ریواردی داشتن. پس اگه یه arm بد انتخاب کنه، **regret** می‌خوره، که اینطوری تعریف می‌شه:

$$
\text{Regret}_t = \mathbb{E}[r(c_t, a^*_t) - r(c_t, a_t)]
$$

که توش $a^*_t = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(c_t, a)]$ بهترین arm برای اون context خاصه. ولی اگه exploration نکنه، ممکنه هیچ‌وقت متوجه نشه بعضی armهایی که اولش بد به نظر می‌رسیدن، تو بعضی contextها اتفاقاً خوبن، و اینجوری نمی‌تونه درست $a^*_t$ رو تخمین بزنه.

#### چطور الگوریتم‌ها مطمئن می‌شن exploration انجام می‌شه

برای اینکه الگوریتم واقعا بره سمت exploration، یه سری روش معروف هست:

- **Optimism in the face of uncertainty**: الگوریتم‌هایی مثل UCB میان یه بازه‌ی اطمینان برای ریوارد می‌سازن و اکشنی رو انتخاب می‌کنن که اون بالا بالای بازه‌شه. اینطوری الگوریتم ناخودآگاه می‌ره سراغ armهایی که کمتر دیده و هنوز مطمئن نیست ازشون.

- **Randomized strategies**: مثل Thompson Sampling که یه توزیع احتمالی روی پارامترها نگه می‌داره و ازش نمونه‌ می‌گیره تا تصمیم بگیره. این کار باعث می‌شه خود به خود تعادلی بین exploration و exploitation ایجاد بشه.

- **فازهای exploration مستقل**: مثلاً Epoch-Greedy یه سری مرحله داره که فقط exploration می‌کنه، بعدش exploitation می‌ره. این باعث می‌شه داده‌ها bias نداشته باشن و منصفانه جمع بشن.

همه‌ی این روش‌ها دارن تلاش می‌کنن که مجموع regret در طول زمان کم بشه و الگوریتم کم‌کم برسه به یه policy خوب.

#### تعمیم بین contextها

چیزی که contextual bandit رو خاص می‌کنه اینه که باید **generalize** کنه بین contextهای مختلف. توی multi-armed bandit معمولی، هر arm یه توزیع ثابتی از ریوارد داره. ولی توی contextual bandit ریوارد هر arm به context بستگی داره. پس یه استراتژی ساده مثل اینکه برای هر context یه الگوریتم جداگانه اجرا کنی، وقتی contextها زیاد یا پیوسته باشن، جواب نمی‌ده و خیلی زود از کنترل خارج می‌شه.

اگه بخوایم تعریفش کنیم، بگیم که ریوارد مورد انتظار برای یه context و arm خاص اینه:

$$
\mu(c, a) = \mathbb{E}[r \mid c, a]
$$

اگه فضای context کوچیک و محدود باشه، می‌تونی برای هر context یه bandit جدا داشته باشی. یعنی برای هر context جداگانه یه تخمین برای ریوارد armها بسازی و یه الگوریتم bandit مثل UCB یا $\epsilon$-greedy روش اجرا کنی. این برای contextهای کم خوبه، ولی اگه context زیاد یا پیوسته باشه، نمی‌شه این‌کارو کرد.

#### Linear Contextual Bandit

برای اینکه از پس contextهای زیاد یا پیوسته بربیایم، معمولاً فرض می‌کنیم یه **ساختار** وجود داره و از **function approximation** کمک می‌گیریم. یه فرض خیلی معروف و کاربردی، مدل ریوارد خطی‌ه که می‌گه می‌شه ریوارد رو با یه تابع خطی از ویژگی‌ها نشون داد:

$$
\mu(c, a) = \phi(c, a)^\top \theta^*
$$

توی این رابطه:

- $\phi(c, a) \in \mathbb{R}^d$ یه **feature vector** ـه که مشخصات context و action رو نشون می‌ده،  
- $\theta^* \in \mathbb{R}^d$ هم یه بردار پارامتره که نشون‌دهنده‌ی فرآیند واقعی تولید ریواردهاست.

کاری که الگوریتم باید بکنه اینه که با دیدن داده‌های $(\phi(c_t, a_t), r_t)$ کم‌کم $\theta^*$ رو تخمین بزنه. وقتی که یه تخمین خوب $\hat{\theta}$ داشته باشه، می‌تونه ریوارد context-actionهای جدید رو هم با $\phi(c, a)^\top \hat{\theta}$ حدس بزنه. به این حالت می‌گن **linear contextual bandit**.

#### یادگیری و تخمین در عمل

برای اینکه $\theta^*$ رو پیدا کنیم، معمولاً از **regularized least squares** استفاده می‌شه. فرض کن $X_t$ ماتریسی باشه که هر سطرش یه feature vector از قبل تا مرحله‌ی $t$ باشه و $\mathbf{r}_t$ هم بردار ریواردهای دیده‌شده‌ست. اون‌وقت تخمینی که می‌زنیم اینه:

$$
\hat{\theta}_t = (X_t^\top X_t + \lambda I)^{-1} X_t^\top \mathbf{r}_t
$$

که $\lambda > 0$ یه عدد مثبته که کار regularization رو انجام می‌ده. این تخمین باعث می‌شه توی مراحل اول یادگیری که داده کمه، الگوریتم overfit نکنه.

این مدل این امکان رو به الگوریتم می‌ده که حتی اگه یه context-action pair خاص رو ندیده باشه، بتونه با استفاده از داده‌های قبلی پیش‌بینی خوبی انجام بده. این توی فضاهای پیچیده و بزرگ خیلی مهمه.

#### Function Approximation توی Online Learning

کلاً contextual banditها رو می‌شه یه جور **online learning problem with function approximation** در نظر گرفت. یعنی یادگیرنده یه مدل پیش‌بینی $f_t(c, a)$ داره که هر لحظه با رسیدن داده‌ی جدید آپدیت می‌شه. هر بار، context جدید $c_t$ رو می‌بینه، یه اکشن $a_t$ انتخاب می‌کنه (مثلاً $a_t = \arg\max_a f_t(c_t, a)$)، بعد ریوارد $r_t$ رو می‌بینه و مدلش رو با اون آپدیت می‌کنه.

ولی برخلاف supervised learning که داده‌ها مرتب، تصادفی و کاملن، اینجا فرق داره:

- داده‌ها **biased** هستن، چون فقط از اکشن‌هایی که انتخاب شدن feedback داریم،  
- feedback **ناقصه** چون ریوارد اکشن‌های دیگه رو نمی‌بینیم،  
- توزیع داده‌ها **ثابت نیست** چون انتخاب‌های خودمون باعث تغییر داده‌های بعدی می‌شن.

واسه همین، الگوریتم‌های خاصی لازمه که این موارد رو پوشش بدن. مثلاً توی الگوریتم *Epoch-Greedy*، یه سری مرحله هست که توش فقط exploration انجام می‌شه (اکشن‌ها رندوم انتخاب می‌شن)، و یه سری مرحله هم هست که فقط از مدل استفاده می‌کنه برای انتخاب اکشن. این باعث می‌شه bias کم بشه و یادگیری بهتر انجام شه.

الگوریتم‌هایی مثل **LinUCB** و **Thompson Sampling** هم توی هر مرحله exploration و exploitation رو با هم قاطی می‌کنن. مثلاً LinUCB اکشنی رو انتخاب می‌کنه که مقدار **upper confidence bound**ش بیشتر باشه:

$$
a_t = \arg\max_a \left[ \phi(c_t, a)^\top \hat{\theta}_t + \alpha \sqrt{\phi(c_t, a)^\top (X_t^\top X_t + \lambda I)^{-1} \phi(c_t, a)} \right]
$$

که توش $\alpha$ مشخص می‌کنه چقدر خوش‌بین باشه. توی Thompson Sampling هم الگوریتم از توزیع posterior روی $\theta^*$ یه $\tilde{\theta}_t$ نمونه می‌گیره و اکشنی رو انتخاب می‌کنه که:

$$
a_t = \arg\max_a \phi(c_t, a)^\top \tilde{\theta}_t
$$








## LinUCB: الگوریتم Linear Upper Confidence Bound

یکی از ساده‌ترین و محبوب‌ترین الگوریتم‌ها برای contextual bandit با مدل ریوارد خطی همین LinUCBه.

LinUCB یه نسخه‌ی گسترده‌تر از UCBه که برای مدل‌های خطی تو فضای context طراحی شده.

فرض می‌کنه یه مدل خطی داریم که می‌گه:

$$
\mu(x, a) = x_a^\top \theta^*
$$

که توش $\theta^* \in \mathbb{R}^d$ یه بردار پارامتر ناشناخته‌ست، و $x_a \in \mathbb{R}^d$ بردار featureهای بازوی $a$ تو context $x$ هست.

مثلاً اگه $x$ خودش یه بردار feature باشه، ممکنه داشته باشیم $x_a = \psi(x,a)$ که یه نگاشت featureه (مثلاً $x$ همراه با نشونه‌هایی برای بازو).

تو هر راند $t$، ایجنت context $x_t$ رو می‌بینه و بردارهای ویژگی $x_{t,1}, \dots, x_{t,K}$ برای همه‌ی بازوها رو می‌گیره.

مدل خطی فرض می‌کنه که:

$$
r_{t,a} = x_{t,a}^\top \theta^* + \eta_{t,a}
$$

که توش $\eta_{t,a}$ نویزه با میانگین صفر (معمولاً فرض می‌شه 1-sub-Gaussian یا محدود باشه).  
این می‌شه همون سناریوی stochastic linear bandit.

حالا LinUCB یه تخمین $\hat{\theta}_t$ از بردار وزن‌ها نگه می‌داره، به‌علاوه‌ی یه برآورد uncertainty براش.  
بعد برای هر بازو تو context فعلی، یه upper confidence bound برای ریوارد می‌سازه، و بازویی رو انتخاب می‌کنه که بیشترین UCB رو داره.

ایده‌ش اینه که این حد بالا، exploration رو تشویق می‌کنه:  
اگه یه بازو ناشناخته‌تر باشه و اطلاعاتمون ازش کم باشه، الگوریتم یه مقدار ریوارد بالاتر براش در نظر می‌گیره، که ایجنت رو وسوسه می‌کنه امتحانش کنه تا داده بیشتری ازش بگیره.

با گذشت زمان و یاد گرفتن $\theta^*$، این بازه‌ی اعتمادا (confidence intervals) کوچیک می‌شن و الگوریتم کم‌کم بیشتر به مدل یادگرفته‌شده تکیه می‌کنه (یعنی greedyتر می‌شه).

این روش "خوش‌بینی در مواجهه با عدم‌قطعیت" یه تم رایج تو الگوریتم‌های bandit هست.














#### بدست آوردن الگوریتم LinUCB

قراره الگوریتم LinUCB رو قدم به قدم دربیاریم. توی راند $t$، فرض کن تا الان یعنی از راند ۱ تا $t-1$ یه‌سری داده جمع کردیم. بیایم $A_{t-1}$ رو بگیریم به عنوان *design matrix* و $b_{t-1}$ هم یه جورایی بردار ریوارد وزن‌دارمون باشه:

- $$A_{t-1} = I_d + \sum_{s=1}^{t-1} x_{s,a_s} x_{s,a_s}^\top$$  
  که توش $x_{s,a_s}$ همون feature vector ایجنت توی زمان $s$ـه. اون $I_d$ یه جور regularization حساب می‌شه، یعنی فرض کردیم یه prior گاوسی با کوواریانس $I$ روی $\theta^*$ داریم که باعث می‌شه $A_{t-1}$ همیشه وارون‌پذیر بمونه.
- $$b_{t-1} = \sum_{s=1}^{t-1} x_{s,a_s} \, r_{s,a_s}$$  
  که یعنی جمع ویژگی‌های context ضربدر ریواردهایی که دیدیم.

حالا با این تعریف‌ها، اگه بخوایم $\theta^*$ رو با ridge regression یا همون least-squares تخمین بزنیم، راه‌حل regularized least-squares بعد از راند $t-1$ این شکلی می‌شه:

$$
\hat{\theta}_{t-1} = A_{t-1}^{-1} b_{t-1}
$$

این $\hat{\theta}_{t-1}$ همون بردار ضرایب تخمینیه که تا الان بیشترین تطابق رو با داده‌هامون داشته. اگه بریم سراغ تحلیل رگرسیون خطی، می‌شه نشون داد که با احتمال بالا:

$$
\|\hat{\theta}_{t-1} - \theta^*\|_{A_{t-1}} \leq \beta
$$

که این $\beta$ تقریباً اندازه‌ی $\sqrt{d \ln(1 + \frac{t-1}{\lambda})}$ رشد می‌کنه. (اینجا $\|z\|_A^2 = z^\top A z$ تعریف شده.)

حالا این یعنی چی؟ یعنی یه حد بالا برای *predicted reward* داریم:  
برای هر arm $a$، داریم:

$$
|x_{t,a}^\top \hat{\theta}_{t-1} - x_{t,a}^\top \theta^*| \le \sqrt{x_{t,a}^\top A_{t-1}^{-1} x_{t,a}}\,\beta
$$

با احتمال بالا. پس اگه بخوایم یه حد بالا با احتمال زیاد برای ریوارد واقعی arm $a$ تو context $x_t$ داشته باشیم، اینجوری می‌شه:

$$
UCB_{t-1}(a) = x_{t,a}^\top \hat{\theta}_{t-1} + \beta\, \sqrt{x_{t,a}^\top A_{t-1}^{-1} x_{t,a}}
$$

الگوریتم LinUCB دقیقاً از همین فرمول استفاده می‌کنه تا تصمیم بگیره کدوم arm رو انتخاب کنه. به‌صورت خلاصه، توی هر راند $t$ این کارا رو می‌کنیم:

1. $\hat{\theta}_{t-1} = A_{t-1}^{-1} b_{t-1}$ رو حساب می‌کنیم با استفاده از تمام دیتاهای قبلی.
2. برای هر arm $a$ مقدار UCB‌ش رو حساب می‌کنیم:

   $$
   p_{t,a} = x_{t,a}^\top \hat{\theta}_{t-1} + \alpha \sqrt{x_{t,a}^\top A_{t-1}^{-1} x_{t,a}}
   $$

   اینجا $\alpha$ یه پارامتره که تعیین می‌کنه چقدر مطمئن باشیم. توی تحلیل تئوری معمولاً برابر $\beta$ در نظر گرفته می‌شه، ولی تو عمل یه عدد ثابته که خودمون تنظیمش می‌کنیم.

3. اون armی که بیشترین $p_{t,a}$ رو داره انتخاب می‌کنیم:

   $$
   a_t = \arg\max_{a \in \mathcal{A}} p_{t,a}
   $$

4. ایجنت اون arm رو بازی می‌کنه و ریوارد $r_{t,a_t}$ رو می‌گیره.
5. بعد هم $A_t$ و $b_t$ رو آپدیت می‌کنیم:

   $$
   A_t = A_{t-1} + x_{t,a_t} x_{t,a_t}^\top,\qquad 
   b_t = b_{t-1} + x_{t,a_t} r_{t,a_t}
   $$

اگه بخوایم به‌صورت pseudocode نگاه کنیم، الگوریتم LinUCB برای یه $\alpha$ مشخص این‌طوریه:

- مقداردهی اولیه:
  $$
  A_0 = I_d,\quad b_0 = \mathbf{0}_d
  $$

- برای هر $t = 1, 2, \dots, T$:
  1. context $x_t$ رو ببین و بردارهای ویژگی $\{x_{t,a}: a=1,\dots,K\}$ رو برای همه armها دربیار.
  2. $\hat{\theta}_{t-1} = A_{t-1}^{-1} b_{t-1}$ رو حساب کن.
  3. برای هر arm مقدار $p_{t,a}$ رو حساب کن.
  4. armی که $p_{t,a}$ بیشترینه رو انتخاب کن.
  5. ریوارد رو مشاهده کن.
  6. $A_t$ و $b_t$ رو با توجه به arm انتخاب‌شده آپدیت کن.

این دقیقاً همون چیزیه که تو مقاله اصلی LinUCB نوشته شده. این الگوریتم خیلی شبیه LinRelـه که تو سال ۲۰۰۲ توسط Auer معرفی شده بود، فقط فرقش اینه که LinUCB از ridge regression و ماتریس معکوس استفاده می‌کنه که هم ساده‌تره هم از نظر عددی پایدارتر، برعکس LinRel که eigen-decomposition لازم داشت. چون LinUCB از رگرسیون خطی استاندارد استفاده می‌کنه، خیلی راحت می‌شه با کتابخونه‌های جبرخطی پیاده‌سازیش کرد.

#### شهود

LinUCB رو می‌تونیم اینجوری تصور کنیم: یه بیضی (یه جور confidence region) تو فضای $\theta$ نگه می‌داره که می‌گه $\theta^*$ ممکنه کجا باشه با توجه به دیتایی که داریم. بعد برای هر arm و feature vectorش $x_{t,a}$، حداکثر ریواردی که می‌تونه بگیره رو حساب می‌کنه، فرض رو هم می‌ذاره بر بهترین حالتی که ممکنه $\theta^*$ باشه. این مقدار:

$$
x_{t,a}^\top \hat{\theta} + \alpha \sqrt{x_{t,a}^\top A^{-1} x_{t,a}}
$$

رو واسه همه armها حساب می‌کنه، بعد اون armی که این مقدار بیشتره رو انتخاب می‌کنه.

اینجاست که می‌گیم الگوریتم *optimistic* عمل می‌کنه: یعنی انگار می‌گه «بیا فرض کنیم اوضاع برای این arm خیلی خوبه»، بعد اون arm رو انتخاب می‌کنه.

اگه یه arm عدم قطعیت بالایی داشته باشه (اون ترم دوم بزرگ باشه)، boost بیشتری می‌گیره و ممکنه انتخاب بشه حتی اگه میانگین تخمینی‌ش از بقیه کمتر باشه. ولی اگه uncertainty کم باشه، انتخاب بیشتر به اون قسمت اول یعنی میانگین تخمینی وابسته می‌شه.

این باعث می‌شه الگوریتم کم‌کم از exploration بره سمت exploitation: اولش همه رو امتحان می‌کنه چون uncertainty بالاست، ولی وقتی مطمئن شد، می‌ره سراغ armی که هی داره بهتر جواب می‌ده.

هر بار که یه arm انتخاب می‌شه، ماتریس $A_t$ تو اون جهت feature space بزرگ‌تر می‌شه، که باعث می‌شه uncertainty تو اون جهت کمتر بشه. این یعنی الگوریتم تو جهت‌هایی که کمتر تجربه کرده بیشتر کاوش می‌کنه.

در نتیجه، یه جور *targeted exploration* انجام می‌ده: دقیقاً به اندازه‌ای که لازمه تو هر جهت می‌ره تا مطمئن بشه که $\theta^*$ رو درست فهمیده.




#### تضمین تئوریک برای LinUCB

اگه فرض کنیم مدل‌مون خطیه، LinUCB عملکرد خیلی خفنی از نظر تئوری داره. یعنی چی؟ یعنی الگوریتم‌هایی که شبیه LinUCB هستن، ریگرتی در حد $O(\sqrt{T})$ دارن (البته یه سری فاکتورهای لگاریتمی هم ممکنه دخیل باشن)، و این ریگرت نسبت به ابعاد مسئله تغییر می‌کنه.  
یه نتیجه‌ی خفن و اولیه توسط Auer تو سال 2002 برای الگوریتم LinRel نشون داد که می‌تونیم به یه کران ریگرت 

$$\tilde{O}(\sqrt{Td})$$ 

برسیم ([لینک](http://proceedings.mlr.press/v15/chu11a/chu11a.pdf#:~:text=bandit%20problem%20with%20linear%20payoffs,may%20be%20less%20prone%20to)).  
بعداً، Dani و رفقا (2008) و Abbasi-Yadkori و هم‌تیمی‌هاش (2011) بررسی کردن و فهمیدن که بدترین حالت ممکن برای رگرت توی linear banditای $d$-بعدی، چیزی در حد 

$$\Omega(d \sqrt{T})$$ 

هست، و یه سری الگوریتم هم ساختن (مثلاً نسخه‌های مختلف UCB) که به ریگرت 

$$O(d \sqrt{T \log T})$$ 

می‌رسن ([لینک](https://proceedings.mlr.press/v28/agrawal13.pdf#:~:text=A%20lower%20bound%20of%20%E2%84%A6,d)) ([لینک](https://proceedings.mlr.press/v28/agrawal13.pdf#:~:text=slightly%20restrictive%20in%20the%20sense,T%29%20%E2%88%9A%20T)).  

خود الگوریتم LinUCB هم طبق بررسی‌های Chu و بقیه تو سال 2011، نشون داده شده که ریگرت 

$$O(d \sqrt{T \log T})$$ 

داره (با احتمال بالا) که با همین کران‌های تئوری جور درمیاد (البته اگه لگاریتم رو نادیده بگیریم). این باعث شد اون حدسی که می‌گفت LinUCB می‌تونه مثل LinRel خوب باشه، تأیید بشه ([لینک](http://proceedings.mlr.press/v15/chu11a/chu11a.pdf#:~:text=numerical%20instability%20issues%20compared%20to,T)).  

در کل، LinUCB برای stochastic linear banditها تقریباً بهترین عملکرد ممکن رو داره، چون هیچ الگوریتمی تو بدترین حالت نمی‌تونه بهتر از 

$$O(d\sqrt{T})$$ 

نتیجه بگیره ([لینک](https://proceedings.mlr.press/v28/agrawal13.pdf#:~:text=A%20lower%20bound%20of%20%E2%84%A6,d)).  
نکته‌ی خفن ماجرا اینه که با تکیه بر یه ساختار ساده مثل مدل خطی، می‌تونیم ریگرتی داشته باشیم که نسبت به $T$ رشد خیلی کمی داره، حتی اگه فضای context بی‌نهایت باشه – یعنی به‌جای اینکه به تعداد context یا تعداد action وابسته باشه، فقط به $d$ (تعداد ویژگی‌ها یا همون ابعاد) وابسته‌ست.

البته یه چیزی رو باید در نظر داشت: این تضمین‌های تئوری فقط وقتی معتبرن که مدل واقعاً خطی باشه و ریواردها هم stochastic باشن. اگه تو دنیای واقعی، تابع ریوارد از حالت خطی منحرف بشه، خب دیگه شاید اون عملکرد خوبه رو نداشته باشیم.  
با این حال، linear UCB معمولاً به عنوان یه baseline قوی استفاده می‌شه و پایه‌ایه برای ساختن مدل‌های contextual bandit پیچیده‌تر، مثل generalized linear bandit یا kernelized bandit که اومدن UCB رو به توابع غیرخطی هم گسترش دادن.




## Thompson Sampling با مدل‌های خطی

Thompson Sampling (یا همون TS) که بهش *Bayesian posterior sampling* هم می‌گن، یه الگوریتم پایه‌ایه برای تصمیم‌گیری مرحله‌به‌مرحله توی محیط‌هایی که خیلی مطمئن نیستن. چیزی که TS رو خاص کرده، اینه که خیلی باحال و کارآمده توی کنترل کردن تعادل بین exploration و exploitation که یه چالش اساسی توی online learning ـه، مثلاً توی contextual banditها. TS بر پایه‌ی اصول بیزیه که می‌گه باید با عدم قطعیت کنار بیایم: به‌جای اینکه فقط یه مقدار برای پارامترهای مدل حدس بزنه، یه توزیع کامل از باورمون رو نگه می‌داره و با زیاد شدن داده‌ها این باور رو به‌روزرسانی می‌کنه. هر بار، یه پارامتر از این توزیع برمی‌داره و با توجه به اون یه تصمیم منطقی می‌گیره.

اولین بار این روش سال ۱۹۳۳ معرفی شد، وقتی که William R. Thompson ازش برای حل مسئله کلاسیک multi-armed bandit استفاده کرد. یه مدت زیاد کمتر بهش توجه می‌شد چون الگوریتم‌های قطعی‌تر معروف‌تر بودن، ولی حالا دوباره خیلی محبوب شده، چون هم توی عمل خوب جواب داده و هم از نظر تئوری تضمین‌هایی براش پیدا شده. توی کاربردهای امروزی، مخصوصاً توی contextual banditهایی که اطلاعات جانبی (contexts) داریم، TS نشون داده که یه استراتژی خوبه، هم انعطاف‌پذیره هم مقیاس‌پذیر. مخصوصاً توی حالت linear contextual bandit، که توش ریوارد به شکل تابع خطی از context و پارامترهای ناشناخته مدل‌سازی می‌شه، TS یه توزیع posterior روی بردار پارامتر $\theta^*$ نگه می‌داره و با نمونه گرفتن ازش تصمیم می‌گیره.

#### تعریف مسئله: Linear Contextual Bandits

بیاید ببینیم دقیقاً قضیه از چه قراره. فرض کن با یه فرآیند تصمیم‌گیری مرحله‌به‌مرحله طرفیم که قراره توی $T$ مرحله انجام بشه. توی هر مرحله $t \in \{1, 2, \dots, T\}$، یه سری بردار context (یا feature) به ایجنت نشون داده می‌شه به اسم $\{x_{t,1}, x_{t,2}, \dots, x_{t,K}\} \subset \mathbb{R}^d$، که هر کدوم مربوط به یه action (یا arm) از مجموعه $\mathcal{A} = \{1, 2, \dots, K\}$ هستن. فرض می‌کنیم ریواردی که از انتخاب یه arm می‌گیریم، طبق یه مدل خطیه:

$$
r_{t,a} = x_{t,a}^\top \theta^* + \eta_{t,a}
$$

که توش $\theta^* \in \mathbb{R}^d$ یه بردار ثابته (که ما نمی‌دونیم چیه) و توی همه مراحل ثابته، و $\eta_{t,a} \sim \mathcal{N}(0, \sigma^2)$ هم یه نویز تصادفیه که از یه توزیع گاوسی میاد. هدف ایجنت اینه که مجموع ریواردهایی که می‌گیره توی این $T$ مرحله رو زیاد کنه:

$$
\sum_{t=1}^T r_{t,a_t}
$$

که توش $a_t$ همون armیه که ایجنت توی مرحله $t$ انتخاب کرده. از یه دید دیگه، اگه بخوایم قضیه رو با دید یادگیری نگاه کنیم، هدف اینه که *regret* یا همون ضرری که بابت انتخاب نکردن بهترین arm ممکن می‌خوریم رو کم کنیم.

#### مدل بیزی و محاسبه‌ی توزیع Posterior

برای اینکه بتونیم از TS استفاده کنیم، یه مدل بیزی می‌سازیم تا اون پارامتر ناشناخته $\theta^*$ رو یاد بگیریم. اولش یه prior داریم که نشون می‌ده اول کار چقدر مطمئن نیستیم. با اضافه شدن داده، این باور رو به‌روزرسانی می‌کنیم. یه انتخاب رایج و راحت اینه که فرض کنیم prior از نوع گاوسی باشه:

$$
\theta^* \sim \mathcal{N}(\mu_0, \Sigma_0) = \mathcal{N}(0, \lambda^2 I_d)
$$

که توش $\lambda^2 > 0$ نشون می‌ده چقدر توی مؤلفه‌های مختلف $\theta^*$ عدم قطعیت داریم. این prior به‌خاطر اینکه با نویز گاوسی مدل هماهنگه (conjugate)، باعث می‌شه توزیع posterior توی هر مرحله همچنان گاوسی باقی بمونه.

فرض کن تا مرحله $t-1$ ایجنت یه سری context و ریوارد مربوط به اون‌ها رو دیده؛ اونا رو با $D_{t-1} = \{(x_{s,a_s}, r_{s,a_s})\}_{s=1}^{t-1}$ نشون می‌دیم. اون وقت توزیع posterior روی $\theta^*$ با توجه به این داده‌ها، یه توزیع گاوسیه با این پارامترها:

$$
A_{t-1} = \frac{1}{\sigma^2} \sum_{s=1}^{t-1} x_{s,a_s} x_{s,a_s}^\top + \Sigma_0^{-1}, \qquad b_{t-1} = \frac{1}{\sigma^2} \sum_{s=1}^{t-1} x_{s,a_s} r_{s,a_s} + \Sigma_0^{-1} \mu_0
$$

میانگین و کوواریانس این توزیع هم به‌صورت زیر حساب می‌شن:

$$
\mu_{t-1} = A_{t-1}^{-1} b_{t-1}, \qquad \Sigma_{t-1} = A_{t-1}^{-1}
$$

#### الگوریتم Thompson Sampling برای Bandit خطی

این الگوریتم مرحله‌به‌مرحله جلو می‌ره و هم از آمار استفاده می‌کنه، هم از تصمیم‌گیری. مراحلش این‌طوریه:

1. **Posterior Sampling**  
   اول مرحله $t$، یه نمونه $\tilde{\theta}_t$ از توزیع $\mathcal{N}(\mu_{t-1}, \Sigma_{t-1})$ می‌گیریم. این نمونه نشون می‌ده که ایجنت با توجه به داده‌های قبلی چه باوری درباره $\theta^*$ داره.

2. **انتخاب arm یا همون action**  
   حالا که contextهای مرحله $t$ رو داریم، برای هر arm یه ریوارد پیش‌بینی می‌کنیم با $\tilde{\theta}_t$ و اون armی که بیشترین ریوارد داره رو انتخاب می‌کنیم:

   $$
   a_t = \arg\max_{a \in \mathcal{A}} x_{t,a}^\top \tilde{\theta}_t
   $$

3. **دریافت ریوارد**  
   arm انتخاب‌شده رو اجرا می‌کنیم و ریوارد مربوطه رو می‌گیریم.

4. **به‌روزرسانی posterior**  
   داده جدید $(x_{t,a_t}, r_{t,a_t})$ رو اضافه می‌کنیم:

   $$
   A_t = A_{t-1} + \frac{1}{\sigma^2} x_{t,a_t} x_{t,a_t}^\top, \qquad b_t = b_{t-1} + \frac{1}{\sigma^2} x_{t,a_t} r_{t,a_t}
   $$

   بعد میانگین و کوواریانس جدید رو حساب می‌کنیم:

   $$
   \mu_t = A_t^{-1} b_t, \qquad \Sigma_t = A_t^{-1}
   $$

#### شهود

یه دید خیلی جالب به TS اینه که الگوریتم می‌خواد مثل احتمال عمل کنه (probability matching). به‌جای اینکه همیشه بره سراغ armی که الان بهترین به نظر می‌رسه، میاد یه دنیای ممکن رو شبیه‌سازی می‌کنه و تو اون دنیا بهترین تصمیم رو می‌گیره. یعنی اگه یه arm احتمال بیشتری داره که بهینه باشه، بیشتر انتخاب می‌شه. اینجوری بدون نیاز به confidence bound خاص، خود به خود exploration انجام می‌شه.

اول کار که هنوز داده نداریم، توزیع posterior خیلی پخش و پر از عدم قطعیت هست. با جمع شدن داده‌ها، این توزیع متمرکزتر می‌شه و تصمیم‌گیری‌ها هم دقیق‌تر. ولی اگه هنوز درباره یه arm خاص شک داشته باشه، اون arm رو هم با یه احتمال مشخصی امتحان می‌کنه تا مطمئن شه.

در مقایسه، روش‌هایی مثل UCB که بر اساس optimism طراحی شدن، می‌رن سراغ armی که بیشترین upper bound داره، که گاهی باعث می‌شه خیلی محافظه‌کارانه exploration کنن. ولی TS چون تصادفیه، هم کشف داره هم بهره‌برداری، اونم با یه تعادل قشنگ.

#### نکات محاسباتی

سخت‌ترین بخش محاسباتی TS اینه که باید از یه Gaussian چندمتغیره $\mathcal{N}(\mu, \Sigma)$ نمونه بگیریم. این کار معمولاً با تجزیه Cholesky انجام می‌شه: $\Sigma = LL^\top$، که توش $L$ یه ماتریس پایین‌مثلثیه. بعد نمونه‌برداری به‌شکل $\tilde{\theta} = \mu + Lz$ انجام می‌شه که $z \sim \mathcal{N}(0, I_d)$ هست.

محاسبه وارون یا تجزیه ماتریس هزینه‌ی حدود $O(d^3)$ داره، ولی می‌شه با روش‌هایی مثل به‌روزرسانی Cholesky با rank-1 این کارو سریع‌تر کرد. توی ابعاد بالا، حتی می‌شه از تخمین‌های ساده‌تر برای $\Sigma$ یا روش‌های تقریبی دیگه استفاده کرد.

اگه نویز ریوارد گاوسی نباشه یا conjugacy از بین بره، محاسبه دقیق posterior سخت می‌شه. اون‌وقت باید سراغ روش‌های تقریبی مثل Laplace، variational inference یا MCMC رفت. یا اینکه از نسخه‌های bootstrapped استفاده کرد که با ترفندهای frequentist یه جور شبیه‌سازی از posterior انجام می‌دن.





#### عملکرد تئوریک Thompson Sampling

Thompson Sampling (یا همون TS)، که بهش posterior sampling هم می‌گن، یه الگوریتم تصادفی برای تصمیم‌گیری تو شرایطیه که خیلی چیزا معلوم نیست. مخصوصاً تو دنیای **linear contextual bandits** حسابی سر زبونا افتاده. اولش چون ساده‌ست، برداشت بیزی شهودی داره و تو عملم جواب می‌داد، همه ازش استفاده می‌کردن، ولی خیلیا فکر می‌کردن فقط یه heuristic باحاله که پشتوانه علمی درست‌درمون نداره. ولی تو این ده سال اخیر، کلی مقاله و بررسی جدی روش انجام شده که نشون داده TS تقریباً یکی از بهترین الگوریتماست؛ هم از دید **frequentist** و هم **Bayesian**. واسه همینم دوباره کلی توجه جلب کرده.


#### کران regret به سبک frequentist برای TS

اولین کار مهم تو این زمینه رو **Agrawal و Goyal (2013)** انجام دادن. اونا تونستن یه جور تضمین frequentist برای TS تو مدل‌های linear bandit پیدا کنن. نشون دادن اگه یه سری فرض معمولی برقرار باشه (مثلاً featureها اندازه‌شون محدود باشه و نویزها هم sub-Gaussian باشن)، اون وقت regret الگوریتم TS می‌ره تو کران:

$$
\tilde{O}(d \sqrt{T}),
$$

که توش $d$ همون تعداد بعدهای contextهاست، و $\tilde{O}$ یعنی وابستگی لگاریتمی رو تو خودش قایم کرده. این کران تقریباً بهترین چیزیه که می‌تونیم بگیریم، و همون عملکردی رو داره که الگوریتم‌هایی مثل LinUCB دارن.

اگه یه‌کم دقیق‌تر بخوایم بگیم، اگه فرض کنیم $\|x_{t,a}\|_2 \leq 1$ و نویز $\eta_t$ هم $\sigma$-sub-Gaussian باشه، اون وقت:

$$
R(T) = O\left( d \sqrt{T \log T} \right).
$$

اصل داستان اینه که TS با posterior sampling کاری می‌کنه که خودش خودبه‌خود بین exploration و exploitation یه تعادل بزنه. یعنی چی؟ یعنی تو هر مرحله، میاد یه $\theta_t$ از تو یه توزیع Gaussian که از روی داده‌های قبلی درست کرده، نمونه‌گیری می‌کنه:

$$
\hat{\theta}_t = V_t^{-1} b_t, \quad \text{که توش } V_t = \lambda I + \sum_{s=1}^{t-1} x_{s,a_s} x_{s,a_s}^\top, \quad b_t = \sum_{s=1}^{t-1} x_{s,a_s} r_s.
$$

این تصادفی بودن باعث می‌شه گاهی وقتا اکشن‌هایی که مطمئن نیست suboptimal هستن رو هم امتحان کنه. هر چی مطمئن‌تر بشه که یه اکشن بده، کمتر سمتش می‌ره. اینجوری بدون اینکه بخواد confidence bound بسازه یا bonus بزنه، می‌تونه عدم قطعیت رو حس کنه و روش تصمیم بگیره.

#### کران Bayesian Regret

تو سمت دیگه‌ی ماجرا، **Russo و Van Roy (2014)** از دید **Bayesian** به TS نگاه کردن. اینجا فرض می‌کنیم که $\theta^* \sim \mathcal{P}_0$ از یه توزیع prior به اسم $\mathcal{P}_0$ اومده و از همون اول تو مدل هست. حالا regret رو نه فقط روی داده، بلکه روی randomness اون prior هم میانگین می‌گیریم:

$$
\mathbb{E}_{\theta^* \sim \mathcal{P}_0} [ R(T; \theta^*) ].
$$

نتیجه؟ اینکه TS می‌تونه بازم به یه کران خوب برسه:

$$
\mathbb{E}[R(T)] = \tilde{O}(d \sqrt{T}),
$$

بدون اینکه لازم باشه شرایط سختی برقرار باشه. این نشون می‌ده TS فقط یه heuristic نیست، بلکه یه **الگوریتم بیزی اصولی**ه که prior رو در نظر می‌گیره، با data به‌روزش می‌کنه و باهاش تصمیم‌گیری می‌کنه.

از این زاویه، TS مخصوصاً تو موقعیت‌هایی مثل آزمایش‌های پزشکی، سیستم‌های پیشنهاددهنده، یا جایی که prior قوی داری خیلی می‌درخشه. چون اون prior رو وارد بازی می‌کنه و بهتر از رقبا عمل می‌کنه.

#### شهود پشت عملکرد خوبش

قدرت TS تو اینه که خیلی شیک قضیه‌ی exploration و exploitation رو هندل می‌کنه. مثلاً UCB میاد یه confidence bound می‌سازه و سعی می‌کنه بالاترینش رو انتخاب کنه. ولی TS می‌گه بیا از روی چیزی که تا الان یاد گرفتی، یه نمونه از $\theta^*$ بگیر و بر اساس اون عمل کن.

این کارش تصادفی ولی با حسابه. اول کار که چیزی نمی‌دونه، بیشتر explore می‌کنه. هر چی بیشتر یاد بگیره، uncertainty کمتر می‌شه و بیشتر می‌ره سراغ exploitation. این باعث می‌شه TS **خودتنظیم** و **سازگار** باشه، یعنی لازم نیست با دست تنظیمش کنیم یا براش exploration schedule بنویسیم.

از اون مهم‌تر، TS باعث می‌شه بیشتر اون اکشن‌هایی که به نظر خوب میان، بیشتر امتحان بشن. یعنی بهتر از الگوریتم‌هایی مثل explore-then-exploit یا explore یکنواخته که به همه‌ی اکشن‌ها یه اندازه فرصت می‌دن.

#### وارد کردن دانش prior

یکی از برگ‌های برنده‌ی TS اینه که خیلی راحت می‌تونه دانش قبلی (یعنی همون prior) رو وارد کنه. مثلاً اگه تجربه یا داده‌ی قدیمی داری که نشون می‌ده $\theta^*$ احتمالاً یه جایی خاص تو فضای $\mathbb{R}^d$ هست، می‌تونی یه prior Gaussian با میانگین اونجا تعریف کنی.

این موضوع خیلی وقتا کمک می‌کنه، مخصوصاً جایی که گرفتن داده سخت یا گرونه. TS با این prior می‌تونه از همون اول یه start خوب بزنه و نیاز به exploration زیاد نداشته باشه. توی مقابل، الگوریتم‌هایی مثل LinUCB معمولاً بدون هیچ پیش‌فرضی شروع می‌کنن (میانگین صفر و ماتریس همانی)، و اگه بخوای prior بدی، کار رو سخت و پیچیده می‌کنه.

#### محدودیت‌ها و وقتی مدل اشتباهه

با همه‌ی این خوبی‌ها، TS بی‌نقص نیست. مثلاً اگه مدل بیزیت درست نباشه (یعنی **misspecified** باشه)، مثلاً ریواردها واقعاً خطی نباشن، ممکنه ایجنت به نتیجه‌ی اشتباه برسه و مطمئن شه یه اکشن خوبه، در حالی که نیست. این باعث می‌شه کارش خراب شه و regret زیادی بگیره.

یه چالش دیگه اینه که TS تو مدل linear جواب می‌ده، ولی اگه بخوای بری سراغ مدل‌های سخت‌تر مثل **generalized linear models**، یا مدل‌های **non-parametric** یا حتی با **deep neural network**ها، کلی چالش جدید داری. مثل چی؟ مثل سنگین شدن محاسبات، نیاز به تقریبی کردن inference با روش‌هایی مثل variational inference یا Monte Carlo، و سخت شدن اینکه تضمین تئوریک بدی.

آخرشم اینکه اگه مدل بیزیت Gaussian نباشه، sample گرفتن از posterior دیگه آسون نیست و کلی زمان‌بر می‌شه.


## الگوریتم Epoch-Greedy

الگوریتم Epoch-Greedy یه استراتژی جالبه واسه حل کردن مسائل contextual bandit. تو این مسائل، یادگیرنده باید با توجه به context، یه اکشن انتخاب کنه تا ریوارد بیشتری بگیره. چیزی که این الگوریتم رو خاص می‌کنه اینه که خیلی قشنگ exploration و exploitation رو از هم جدا می‌کنه. این دوتا مرحله تو بازه‌های زمانی مشخصی انجام می‌شن که بهشون می‌گن epoch. این الگوریتم اولین بار توسط Langford و Zhang تو سال ۲۰۰۷ معرفی شد و به خاطر سادگی، کارایی تو موقعیت‌های مختلف و ارتباطش با supervised learning، خیلی محبوب شده. اگه دوست داری بیشتر بخونی، مقاله‌ش اینه: [The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information](https://proceedings.neurips.cc/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf).

برخلاف الگوریتم‌هایی مثل LinUCB یا Thompson Sampling که تو هر لحظه exploration و exploitation رو با هم قاطی می‌کنن، تو Epoch-Greedy این دوتا حالت به صورت جداگونه و تو بازه‌های زمانی طولانی‌تر انجام می‌شن. تو epochهایی که مال exploration هستن، الگوریتم به صورت رندوم و یکنواخت اکشن انتخاب می‌کنه تا دیتا بدون بایاس جمع کنه. بعدش تو exploitation، با استفاده از اون دیتا یه policy یاد می‌گیره (معمولاً با یه الگوریتم supervised learning) و بعدش اون اکشنی رو انتخاب می‌کنه که پیش‌بینی می‌شه بیشترین ریوارد رو بده. اینجوری هم پیاده‌سازی ساده‌تر می‌شه و هم تحلیل عملکردش راحت‌تره.

یکی از نکات Epoch-Greedy اینه که خیلی منعطفه. نیاز نیست بدونی تابع ریوارد دقیقاً چه شکلیه یا مثلاً خطیه یا نه. کلاً اینجوری به قضیه نگاه می‌کنه که contextual bandit یه سری مسئله‌ی supervised learning پشت سر همه. تا وقتی یه روش supervised learning داشته باشی، می‌تونی از این الگوریتم استفاده کنی. این یعنی می‌تونه با مدل‌های مختلف کار کنه، از مدل‌های خطی ساده گرفته تا شبکه‌های عصبی سنگین، بسته به اینکه چقدر دیتا و منابع داری.

البته این انعطاف یه سری هزینه هم داره. چون این الگوریتم فرض نمی‌کنه که مثلاً ریوارد به صورت خطی به context وابستست، معمولاً کران‌های theoretical regretش از اون الگوریتمایی که این فرض‌ها رو دارن بیشتره. اینجا منظور از regret، تفاوت ریوارد کلی‌ایه که الگوریتم گرفته با چیزی که یه policy بهینه می‌تونست بگیره. با این وجود، تو خیلی از موقعیت‌ها، باز هم Epoch-Greedy انتخاب خوبیه، چون هم سادست، هم از لحاظ تئوری می‌شه با ابزارهای supervised learning بررسیش کرد، مخصوصاً از دیدگاه sample complexity.

$$
\text{Regret} = \sum_{t=1}^{T} \left( r^*_t - r_t \right)
$$

دیدگاه sample complexity یعنی اینکه عملکرد الگوریتم بستگی داره به اینکه اون مدل supervised learning چقدر می‌تونه از دیتای exploration خوب یاد بگیره و تعمیم بده. اگه مدل به اندازه‌ی کافی قوی باشه و دیتام درست و حسابی باشه، policyای که یاد گرفته می‌شه می‌تونه تو exploitation خوب عمل کنه. این ویژگی باعث می‌شه که آدمایی که رو supervised learning کار می‌کنن، بتونن راحت نتایجشون رو روی contextual banditها هم پیاده کنن.

خلاصه‌ش اینه که الگوریتم Epoch-Greedy یه پل بین reinforcement learning و supervised learning ایجاد می‌کنه، اونم با تبدیل کردن contextual banditها به یه سری وظیفه‌ی supervised خوب و مشخص. ساختار ساده، سازگاری با مدل‌های مختلف، و پایه‌های تئوریکش باعث می‌شن که هم برای تحقیق‌های تئوریک و هم برای کاربردهای واقعی یه ابزار به‌دردبخور باشه، مخصوصاً وقتی که سادگی و انعطاف واست مهم باشه.






#### توضیح الگوریتم

الگوریتم **Epoch-Greedy** یه روش معروفه واسه حل کردن مسائل contextual bandit که هدفش اینه بین exploration و exploitation یه تعادلی برقرار کنه. این الگوریتم تو یه سری از epochها اجرا می‌شه که با $j = 0, 1, 2, \ldots$ شماره‌گذاری شدن. معمولاً طول هر epoch به شکل هندسی بیشتر می‌شه، مثلاً هر بار دو برابر می‌شه (مثلاً epoch شماره $j$ طولش $2^j$ هست). ایده‌ش اینه که یه بخش کوچیک ولی مهم از هر epoch صرف exploration بشه، جایی که ایجنت یه جور رندوم بازوها (armها) رو می‌کشه تا داده‌ی unbiased جمع کنه، بعدش تا آخر اون epoch از چیزی که یاد گرفته استفاده می‌کنه و exploitation انجام می‌ده با دنبال کردن بهترین policy که تا اون لحظه داره.

هر epoch دوتا بخش اصلی داره:

- **Exploration Phase**: اول هر epoch (یا گاهی آخر قبلیه)، الگوریتم یه تعداد ثابتی از roundها (که با $m_j$ نشون داده می‌شه) رو به‌صورت یکنواخت و رندوم arm انتخاب می‌کنه. تو این فاز اصلاً به context نگاه نمی‌کنه — فقط یه arm رندوم انتخاب می‌کنه. این کار باعث می‌شه داده‌هایی که توی exploration جمع می‌شن، از نظر آماری unbiased باشن. اگه یه context چند بار تکرار بشه، الگوریتم ممکنه مطمئن شه که هر arm حداقل یه بار واسه اون context تست شده. این‌طوری یه دیتاست ساخته می‌شه که تقریب خوبی از نمونه‌های i.i.d. از توزیع مشترک context-arm-reward یعنی $(x, a, r_a)$ به‌دست می‌ده. این داده‌های exploration خیلی مهمن واسه اینکه تو فاز بعدی بشه یه policy درست‌درمون یاد گرفت.

- **Exploitation Phase**: وقتی exploration تموم شد، الگوریتم میاد سراغ کل داده‌هایی که تا الان جمع شده — چه تو همین epoch چه از قبلی‌ها — و باهاش یه مدل یا policy یاد می‌گیره که بتونه contextها رو به armها وصل کنه. هر نمونه تو این دیتاست یه context $x$، یه arm انتخاب‌شده $a$، و ریوارد مربوط به اون انتخاب $r$ داره. اینجا داستان مثل یه مسئله‌ی supervised learning می‌شه: می‌تونی از regression استفاده کنی (برای پیش‌بینی ریوارد مورد انتظار $\hat{\mu}(x, a)$) یا classification (برای اینکه بفهمی تو یه context خاص کدوم arm بهتره). اگه contextهات گسسته باشن، می‌تونی میانگین ریوارد تجربی برای هر جفت arm-context رو حساب کنی. اگه نه، باید یه مدل عمومی‌تر آموزش بدی که تابع ریوارد رو تخمین بزنه.

  بعد از اینکه policy یاد گرفته شد — که با $h_j: \mathcal{C} \to \mathcal{A}$ نشونش می‌دیم — الگوریتم میره تو فاز exploitation. از اینجا تا آخر epoch، هر وقت یه context $x$ دید، arm پیشنهادی policy یعنی $a = h_j(x)$ رو انتخاب می‌کنه. دیگه خبری از رندوم بازی نیست — ایجنت دقیقا همون انتخاب greedy رو انجام می‌ده که policy بهش گفته.

- **رفتن به epoch بعدی**: وقتی epoch فعلی تموم شد، الگوریتم می‌ره سراغ یه epoch جدید که یه کم طولانی‌تره. دوباره یه تیکه‌ی کوچیک برای exploration می‌ذاره، بعد با کل دیتای جمع‌شده دوباره مدلش رو آموزش می‌ده و بعدش می‌ره سراغ exploitation. هر چی جلوتر می‌ره و داده‌ها بیشتر می‌شن و مدل بهتر یاد می‌گیره، نیاز به exploration کمتر می‌شه.

این روش به‌شکل حساب‌شده‌ای exploration-exploitation رو مدیریت می‌کنه. اول کار باید کلی exploration کنی تا دیتای درست‌حسابی جمع شه واسه یادگیری. ولی وقتی داده زیاد شد و policy بهتر شد، می‌تونی exploration رو کمتر کنی. معمولاً یه schedule واسه exploration می‌ذارن که آروم‌آروم کمتر بشه؛ تو ورژن اصلی epoch-greedy، نرخ exploration تقریباً با $$O(T^{-1/3})$$ میاد پایین، که $T$ تعداد کل time stepهاست.

Langford و Zhang یه سری تضمین نظری واسه این روش دادن. به‌طور تعریفی، اگه اون الگوریتم supervised یادگیرنده بتونه با $n$ تا نمونه یه policy تقریباً خوب با خطای $\epsilon$ پیدا کنه، اون‌وقت regret کلی epoch-greedy تو $T$ تا round می‌شه حدود

$$
O(T^{2/3} S^{1/3}),
$$

که توش $S$ یه چیزی مثل VC-dimension کلاس فرضیه‌هاست. این bound نشون می‌ده که الگوریتم با گذشت زمان بهتر می‌شه، ولی سرعت همگراییش از بعضی روش‌های قوی‌تر که boundهایی مثل $\sqrt{T}$ دارن، کمتره.

در کل، هدف الگوریتم epoch-greedy اینه که اوایل کار به اندازه کافی داده‌ی exploration جمع کنه تا بشه یه policy خوب یاد گرفت، و بعد کم‌کم تمرکز بره روی exploitation اون policy. اون regretی که در نهایت پیش میاد از دوتا جا میاد: (1) تو roundهای exploration که آگاهانه armهای نه‌چندان خوب انتخاب می‌شن واسه جمع کردن داده، و (2) تو roundهای exploitation که هنوز policy کامل نیست و شاید اشتباه کنه. نرخ regret $$T^{2/3}$$ یه جور بالانس بین این دوتاست. هرچند این نرخ عالی نیست، ولی تو خیلی از شرایط واقعی با توجه به سادگی و کارایی الگوریتم، قابل قبوله.

یکی از خوبی‌های اصلی epoch-greedy اینه که لازم نیست بدونی زمان کل $T$ چقدره، واسه همین خیلی راحت و منعطفه. اینکه با supervised learning معمولی کار می‌کنه و مفهومی هم هست، باعث می‌شه انتخاب خوبی باشه برای جاهایی که آموزش مدل راحت باشه و context زیاد وجود داشته باشه. مخصوصاً تو سیستم‌های واقعی که قراره به‌مرور زمان وفق پیدا کنن و نمی‌خوای پیچیدگی زیادی تو مدل‌سازی یا تنظیم پارامتر داشته باشی، خیلی به درد می‌خوره.



#### بررسی Regret و دیگر ویژگیاش

الگوریتم epoch-greedy یه استراتژی contextual bandit هست که یه regret به اندازه‌ی $$O(T^{2/3})$$ می‌ده، اونم تو خیلی از شرایط معمول. طبق چیزی که Langford و Zhang تو سال ۲۰۰۷ گفتن، boundِ regret با احتمال بالا برای این الگوریتم می‌شه $$O(T^{2/3} S^{1/3})$$، که اینجا $S$ سختی یادگیری مدل supervised learning پشت قضیه‌ست که تصمیم‌گیری رو انجام می‌ده. هر چی تعداد راندها $T$ بیشتر بشه، میانگین regret تو هر راند (یعنی $R(T)/T$) کم می‌شه و به صفر نزدیک می‌شه؛ این یعنی الگوریتم داره کم‌کم یه policy درست‌حسابی یاد می‌گیره.

درسته که این عملکرد به پای الگوریتم‌های خفن‌تر که regret بهینه‌ی $$O(\sqrt{T})$$ دارن نمی‌رسه، ولی بازم باارزشه چون epoch-greedy زیاد درباره‌ی شکل مسئله سخت‌گیری نمی‌کنه. لازم نیست ریواردها طبق یه مدل خطی باشن یا شکل خاصی داشته باشن. این آزادی باعث می‌شه که تو شرایطی که نمی‌تونیم چیز زیادی درباره‌ی ساختار ریوارد بدونیم، باز هم بتونیم ازش استفاده کنیم.

یه خوبی دیگه‌ش هم اینه که لازم نیست بدونیم چندتا راند قراره اجرا بشه. کلی از الگوریتم‌هایی که بر پایه‌ی explore-then-commit هستن، باید از قبل بدونن چقدر زمان هست تا بدونن کی از exploration برن سمت exploitation. ولی epoch-greedy همین‌جوری ادامه می‌ده، فقط کم‌کم explorationش رو کمتر می‌کنه، که این باعث می‌شه برای کارای online یا طولانی‌مدت خیلی به درد بخوره.

برای پیاده‌سازی هم می‌تونیم تقریبش بزنیم با یه روش $\epsilon$-greedy که $\epsilon$ش کم‌کم کم بشه. یعنی تو هر راند، الگوریتم با احتمال $\epsilon_t$ یه arm رندوم انتخاب می‌کنه و با احتمال $1 - \epsilon_t$ همون armی که تا حالا بهترین بوده رو می‌زنه. اگه $\epsilon_t$ رو طوری تنظیم کنیم که با $t^{-1/3}$ کم بشه، می‌تونیم تقریباً رفتار epoch-greedy رو داشته باشیم، بدون اینکه لازم باشه دقیقاً epoch تعریف کنیم. این خیلی بدردبخوره تو شرایط واقعی که اینکه مرز epoch رو نگه داریم دردسر می‌شه.

یه چیزی که epoch-greedy رو خاص می‌کنه اینه که خیلی تمیز بین exploration و exploitation فرق می‌ذاره. وقتی نوبت explorationه، یه‌راست می‌ره سراغ زدن armهای تصادفی تا داده‌های unbiased جمع کنه. بعدش از این داده‌ها استفاده می‌کنه و با تکنیک‌های معمولی supervised learning یه policy آموزش می‌ده. تو راندهای بعدی هم همون policy رو exploit می‌کنه. این مدل ساختار تحلیل تئوریش رو آسون‌تر می‌کنه و خیلی خوب با ایده‌های empirical risk minimization و طراحی آزمایش (experimental design) جور درمیاد.

با همه‌ی این خوبی‌ها، ولی epoch-greedy تو بدترین حالت‌ها بهترین انتخاب نیست. الگوریتم‌های جدیدتر مثل UCB یا Thompson sampling یا اونایی که از optimization oracle استفاده می‌کنن، معمولاً regret بهتری می‌دن—حتی می‌رسن به $$O(\sqrt{T})$$—تو محیط‌های سخت یا با بُعد زیاد. این الگوریتم‌ها معمولاً exploration رو هم بهتر مدیریت می‌کنن و راحت‌تر با شرایط جدید وفق پیدا می‌کنن.

ولی بازم epoch-greedy یه نقش مهمی تو دنیای contextual banditها بازی می‌کنه. چون ساده‌ست، شفافه و خیلی خوب به درد آموزش یا آزمایش می‌خوره. وقتی context مسئله خیلی پیچیده نیست و نیاز نیست regret خیلی خیلی کم باشه، می‌تونه یه گزینه‌ی عملی و مطمئن باشه.


**خلاصه‌ش این‌طوری می‌شه که Epoch-Greedy:**
- فعالانه با انتخاب تصادفی armها داده جمع می‌کنه.
- هر چند وقت یه بار policyش رو با کل داده‌هایی که تا حالا داره آپدیت می‌کنه.
- بین راندهای exploration همون policy رو exploit می‌کنه.
- یه regret برابر $$O(T^{2/3})$$ می‌ده، که از $$O(\sqrt{T})$$ بدتره ولی تو شرایط گسترده‌تری قابل استفاده‌ست.
- لازم نداره از قبل بدونیم چندتا راند داریم.
- می‌شه با یه $\epsilon$-greedy ساده که $\epsilon_t \propto t^{-1/3}$ هست پیاده‌ش کرد.
- یه پل بین ایده‌های کلاسیک supervised learning و تصمیم‌گیری online می‌زنه.
- یه benchmark قابل فهم و ساده‌ست برای contextual banditها.






## استراتژی‌های Explore-Then-Commit (ETC)

Explore-Then-Commit یا همون ETC، یه استراتژی پایه‌ای ولی خیلی ساده‌س برای اینکه بتونیم با مسائل contextual bandit کنار بیایم. همون‌جوری که از اسمش معلومه، این استراتژی کل ماجرا رو به دو مرحله‌ی جدا تقسیم می‌کنه: اول یه فاز exploration داریم، بعدش یه policy رو انتخاب می‌کنیم و تا آخرش می‌چسبیم بهش. توی فاز exploration، الگوریتم میاد از اکشن‌های مختلف نمونه‌گیری می‌کنه، بدون اینکه بخواد حتماً ریوارد خوبی بگیره، فقط واسه اینکه بفهمه هر اکشن توی contextهای مختلف چطوری جواب می‌ده. بعدش با همین دیتا، سعی می‌کنه یه policy خوب یاد بگیره و از اون به بعد فقط از همون استفاده کنه. با اینکه این روش به regret بهینه‌ی الگوریتم‌های adaptive نمی‌رسه، ولی چون فهمیدنش آسونه و تحلیلش هم راحت‌تره، یه baseline مهمه هم برای مقاله‌ها و هم برای تست کردن روش‌ها.

#### روند کلی ETC

کل کار ETC توی دو تا مرحله‌ی پشت‌سرهم انجام می‌شه:

- **فاز exploration**: توی $n_0$ راند اول، الگوریتم به صورت تصادفی از بین اکشن‌ها یکی رو انتخاب می‌کنه. رایج‌ترین روش اینه که بین همه‌ی arms به‌صورت یکنواخت (uniform) رندوم انتخاب کنه، بدون اینکه اصلاً به context نگاه کنه. هدف اصلی اینه که یه دیتای متنوع و کافی جمع بشه از جفت‌های context-action. حالا اگه محیط ساخت‌دار باشه و contextها الگو داشته باشن یا تکراری باشن، می‌شه یه‌کم هوشمندانه‌تر exploration کرد. مثلاً با تکنیک‌هایی مثل stratified sampling که مطمئن می‌شن هر اکشن توی قسمت‌های مختلف context تست بشه، دقت policy نهایی بالا می‌ره.

- وقتی $n_0$ تا نمونه جمع شد که شامل یه context $c$، یه اکشن $a$ و ریوارد دیده‌شده $r$ هست، الگوریتم میاد با همین دیتا یه policy به اسم $\hat{\pi}$ یاد می‌گیره. یه راه معمول اینه که تابع ریوارد $\hat{\mu}(c, a)$ رو با regression مدل کنیم. بعدش policy نهایی این‌طوری تعریف می‌شه: 

$$
\hat{\pi}(c) = \arg\max_a \hat{\mu}(c,a)
$$

یعنی برای هر context اون اکشنی رو انتخاب می‌کنه که ریوارد پیش‌بینی‌شده‌ش بیشتره. توی مدل‌های پارامتریک مثل مدل‌های خطی، این کار معمولاً با تخمین یه بردار ضریب $\hat{\theta}$ با regression کمترین مربعات (OLS) انجام می‌شه.

- **فاز commit**: وقتی دیگه budget مربوط به exploration تموم شد، الگوریتم وارد فاز commit می‌شه. از این به بعد، توی همه‌ی راندهای $t > n_0$، از policy ثابت $\hat{\pi}$ برای انتخاب اکشن استفاده می‌کنه. یعنی اگه context جدید $c_t$ بیاد، الگوریتم اکشن $a_t = \hat{\pi}(c_t)$ رو می‌ده بیرون. دیگه توی این فاز خبری از یادگیری یا آپدیت مدل نیست.

#### چطوری $n_0$ رو انتخاب کنیم

کل عملکرد ETC بستگی داره به اینکه $n_0$ رو چقدر در نظر بگیریم. اگه خیلی کم باشه، policy یادگرفته‌شده ممکنه خیلی غلط باشه و توی فاز commit کلی اشتباه بزنه و regret بالا بده. ولی اگه خیلی زیاد باشه، اون‌وقت الگوریتم کلی وقتشو صرف اکشن‌های اشتباه توی exploration کرده، در حالی که می‌تونست زودتر بره سراغ exploitation یه policy بهتر.

واسه اینکه یه بالانسی بین اینا برقرار بشه، بیایم هر دو تا regret رو بررسی کنیم:

- توی فاز exploration، حدوداً $O(n_0)$ تا regret داریم، چون اکشن‌ها رندومی‌ن و شاید درست نباشن.
- توی فاز commit، regret به دقت policy یادگرفته‌شده بستگی داره، که معمولاً به صورت $O((T - n_0)/n_0)$ درمیاد، اگه خطای policy مثل $1/n_0$ کم بشه، شبیه چیزی که توی supervised learning داریم.

پس اگه بخوایم این دوتا با هم تراز بشن، بهتره $n_0 \sim T^{2/3}$ در نظر بگیریم، که باعث می‌شه هم exploration هم exploitation تقریباً $T^{2/3}$ تا regret بدن. این می‌شه یه regret کلی:

$$
O(T^{2/3})
$$

که گرچه بهینه نیست ولی توی خیلی از سناریوها جواب می‌ده، مخصوصاً اگه horizon کوتاه باشه یا مسئله ساده باشه.

#### استفاده‌ی عملی و محدودیت‌ها

وقتی بخوایم ETC رو توی دنیای واقعی استفاده کنیم، سادگیش هم می‌تونه مزیت باشه هم محدودیت. مثلاً با اینکه regret $O(T^{2/3})$ که ETC می‌ده نسبت به الگوریتم‌هایی مثل UCB یا Thompson Sampling خوب نیست، ولی با استراتژی‌های ساده‌تر مثل epoch-greedy یکیه. اتفاقاً epoch-greedy می‌تونه یه جور نسخه‌ی آپگرید شده‌ی ETC باشه که یادگیریش توی چند تا دوره (epoch) تقسیم شده: توی هر کدوم اول یه‌کم exploration می‌کنه بعد می‌ره سراغ exploitation. این‌جوری الگوریتم می‌تونه اگه اولش خوب exploration نکرد، بعداً جبران کنه، یا اگه محیط تغییر کنه (non-stationary باشه) خودش رو آپدیت کنه.

در مقابل، ETC فقط یه بار exploration می‌کنه و بعدش commit می‌کنه. اگه اون اولش خوب یاد نگرفته باشه یا دیتای کافی نداشته باشه، دیگه گیر می‌افته توی یه policy بد تا آخر. همینم باعث می‌شه توی محیط‌های پویا یا خیلی پیچیده که نیاز به آپدیت مداوم دارن، ETC انتخاب خوبی نباشه.

ETC توی مقاله‌ها بهش می‌گن استراتژی "$\epsilon$-first". یعنی می‌شه مثل یه نسخه‌ی خاص از $\epsilon$-greedy در نظرش گرفت که اولش $\epsilon = 1$ هست (یعنی کامل exploration) و بعدش $\epsilon = 0$ (یعنی فقط exploit). با اینکه توی کاربردهای واقعی خیلی به‌ندرت ازش استفاده می‌شه، ولی واسه آموزش و مقایسه‌ی الگوریتم‌ها خیلی مهمه چون به‌خوبی نشون می‌ده که trade-off بین exploration و exploitation چطوریه.

#### مثال: Linear Contextual Bandit

واسه اینکه ETC رو توی یه حالت واقعی‌تر ببینیم، فرض کن یه مسئله‌ی linear contextual bandit داریم که توش ریوارد مورد انتظار یه تابع خطی از feature vector هست. یعنی برای هر arm و هر راند یه feature vector داریم به اسم $x_{t,a}$ و تابع ریوارد واقعی با یه بردار پارامتر $\theta^*$ مشخص می‌شه. ریوارد هم به شکل 

$$
r_t = x_{t,a}^\top \theta^* + \eta_t
$$

هست که $\eta_t$ یه نویزه.

استفاده از ETC این‌جا این‌طوریه:

1. **Explore**: توی $n_0$ راند اول، arms رو تصادفی و یکنواخت انتخاب کن، context رو بی‌خیال شو.
2. **Learn**: بعد از اینکه $n_0$ تا نمونه گرفتی، بردار $\hat{\theta}$ رو با regression معمولی (OLS) تخمین بزن.
3. **Commit**: بعدش توی باقی مونده‌ی $T - n_0$ راند، از $\hat{\theta}$ استفاده کن و توی هر راند اون اکشنی رو انتخاب کن که 

$$
a_t = \arg\max_a x_{t,a}^\top \hat{\theta}
$$

باشه.

اگه فرض کنیم noise و featureها bounded باشن، می‌تونیم نشون بدیم که با 

$$
n_0 = O(d^2 \log T)
$$

تا نمونه، $\hat{\theta}$ خیلی به $\theta^*$ نزدیک می‌شه با احتمال زیاد. پس توی فاز commit بیشتر regretی که می‌خوریم به خاطر خطای تخمین هست که با داده‌ی بیشتر کمتر می‌شه.

یه سری استراتژی‌های پیشرفته‌تر هم هستن که به جای یه‌بار exploration، توی چند مرحله exploration و exploitation رو با هم قاطی می‌کنن، مثل phased exploration که توی مقاله‌ی Abbasi-Yadkori و همکاراش اومده. این روش‌ها معمولاً regret بهتری می‌دن، مثلاً 

$$
O(d \sqrt{T \log T})
$$

که توی مسائل با بعد بالا خیلی بهتر از ETC با $O(T^{2/3})$ هست. بعضی از الگوریتم‌های جدیدتر حتی exploration رو توی کل پروسه ادامه می‌دن یا اجبارش می‌کنن که ماتریس طراحی (design matrix) همیشه خوب condition بمونه، و اینطوری کارایی بهتر می‌گیرن.






## یه سری نکات تئوری و بررسی regret

بعد از اینکه الگوریتمای اصلی رو گفتیم، حالا می‌خوایم یه سری نکات تئوری درباره‌ی contextual bandits رو بچینیم کنار هم. یکی از سؤالای پایه‌ای اینه که: *یاد گرفتن این داستان از نظر regret چقدر سخته؟* جوابش بستگی داره به اینکه چه فرضایی روی context و ساختار reward داریم:

- توی بدترین حالت ممکن (یعنی وقتی adversary داریم و هیچ فرض احتمالاتی روی context یا reward نداریم)، ممکنه regret خطی بشه نسبت به $T$، مگه اینکه یه محدودیتی روی کلاس policy‌ها بذاریم. مثلاً اگه طرف مقابل بتونه هر بار یه context متفاوت بندازه جلو ایجنت، اون‌وقت ایجنت عملاً هر دفعه یه تصمیم کاملاً جدید باید بگیره و هیچ شانسی واسه generalize کردن نداره؛ هیچ الگوریتمی هم نمی‌تونه بهتر از یه حدس تصادفی عمل کنه، پس میشه $R(T) \propto T$. 

$$
R(T) \propto T
$$

حالا اگه فقط اجازه بدیم یه تعداد محدود از policy (یا همون expert) داشته باشیم، الگوریتم Exp4 هست که می‌تونه به regret حدود $O(\sqrt{T \log N})$ برسه، که $N$ هم تعداد اون expert/policyهاست. Exp4 دیگه فرض stochastic بودن reward رو نداره، ولی اگه $N$ خیلی گنده باشه (مثلاً نمایی نسبت به تعداد فیچرها)، دیگه از نظر محاسباتی خیلی سنگین و غیرقابل اجرا میشه. این نشون میده که برای اینکه الگوریتمامون عملیاتی باشن، لازمه یه مقدار ساختار یا فرض داشته باشیم.

- اگه بریم سراغ حالت stochastic با یه مقدار ساختار (همونی که ما فرض کردیم)، اون‌وقت میشه انتظار داشت که regret خیلی کمتر بشه. مثلاً اگه یه مدل خطی با بعد $d$ داشته باشیم، دیدیم الگوریتم‌ها می‌تونن برسن به 

$$
R(T) = \tilde{O}(d\sqrt{T})
$$

یه کران پایین هم هست تو همین حالت، که می‌گه 

$$
\Omega(d \sqrt{T})
$$

یعنی هیچ الگوریتمی نمی‌تونه بهتر از یه ضریب ثابت ضربدر $d\sqrt{T}$ عمل کنه. این شبیه همون کران پایین $\Omega(\sqrt{KT})$ توی bandit معمولیه (وقتی $d=K$ باشه و ساختار خاصی نداشته باشیم). خیلی ساده بخوایم بگیم، یاد گرفتن $d$ تا پارامتر (که policy رو مشخص می‌کنن) نیاز به یه حدی از exploration داره، و اون $\sqrt{T}$ هم نماد اینه که چجوری عدم قطعیت توی یادگیری کم میشه.

- فرق بین $O(\sqrt{T})$ و $O(T^{2/3})$ برای $T$های بزرگ حسابی به چشم میاد. مثلاً اگه $T=10^6$ باشه، اون‌وقت $\sqrt{T} \approx 10^3$ ولی $T^{2/3} \approx 10^4$. پس الگوریتمای قوی‌تر (مثل LinUCB، TS و غیره) می‌تونن توی همچین شرایطی تا یه مرتبه از بزرگی regret کمتر داشته باشن نسبت به الگوریتمای ساده‌تر مثل epoch-greedy یا ETC. هرچی $T$ بزرگ‌تر باشه، این اختلاف هم بیشتر میشه. واسه همین وقتی بازه زمانی بلنده یا اصلاً محدود نیست، بهتره بریم سراغ الگوریتمایی که regret نوع $\sqrt{T}$ دارن.

یه چیز مهم دیگه هم اینه که چجوری مدل‌سازی و generalization تأثیر می‌ذاره: دلیل اینکه LinUCB و TS عملکرد خوبی دارن اینه که از یه *مدل کلی* استفاده می‌کنن (همون تابع خطی $\theta^*$) که بین همه‌ی contextها و actionها یه جور ارتباط برقرار می‌کنه. یعنی اگه توی یه context خاص exploration کنیم، می‌تونیم چیزایی درباره‌ی $\theta^*$ یاد بگیریم که توی contextهای دیگه هم به درد بخوره – یه جور انتقال یادگیری بین contextهاست. ولی اگه خیلی ساده بریم جلو و هر context (یا هر جفت context-action) رو جدا ببینیم، هر بار باید از صفر شروع کنیم. خلاصه اینکه، فرض خطی بودن باعث میشه یه دنیای بی‌نهایت بازو (چون هر جفت context-action یه گزینه‌ی جدیده) تبدیل بشه به یه مسئله‌ی تخمینی با تعداد پارامتر محدود. استفاده از این ساختار کلید حل شدنی بودن مسئله‌ست. البته اگه این فرض اشتباه باشه، الگوریتم ممکنه به یه policy ضعیف همگرا کنه (یعنی یه تقریب خطی نه‌چندان خوب بده). از اون طرف، اگه فرض خیلی آزاد و بی‌در و پیکر باشه (مثلاً یه کلاس تابع خیلی پیچیده بدون کنترل)، یادگیری می‌تونه خیلی کند یا حتی نشدنی بشه. واسه همینه که یا فرض خطی می‌ذاریم یا با regularization و prior یه محدودیتی روی مدل اعمال می‌کنیم.

از نظر همگرایی هم، همه‌ی الگوریتمایی که گفتیم در نهایت (با احتمال بالا یا به‌طور میانگین) به policy بهینه می‌رسن. LinUCB و Thompson Sampling آخرش $\theta^*$ رو پیدا می‌کنن (خطاهاشون با تقریب $1/\sqrt{t}$ کم میشه بعد از $t$ تا مشاهده)، و دیگه تصمیم‌گیری‌های اشتباه به صفر نزدیک میشه. الگوریتمایی مثل epoch-greedy و ETC هم یه جایی دیگه exploration رو قطع می‌کنن، ولی تا اون موقع داده‌ی کافی جمع کردن که policyشون بهینه یا نزدیک به بهینه باشه، و اشتباهات بعدیشون خیلی کمه. به‌صورت دقیق، می‌تونیم بگیم:

$$
R(T) = o(T)
\quad \Rightarrow \quad \lim_{T\to\infty} \frac{R(T)}{T} = 0
$$

واسه همه‌ی این الگوریتما توی حالت stochastic برقرار میشه. یعنی آخرش الگوریتم همون reward ای رو می‌گیره که policy ایده‌آل $\pi^*$ می‌گرفت. البته توی دنیای واقعی اون regret توی بازه‌های زمانی کوتاه هم مهمه.

یه موضوع مهم دیگه هم پیچیدگی محاسباتیه. توی linear bandits، همه‌ی این الگوریتما توی هر راند توی زمان چندجمله‌ای نسبت به $d$ و $K$ اجرا می‌شن (مثلاً $O(K d^2)$ اگه مستقیم پیاده‌سازی کنیم، چون باید برای هر کدوم از $K$ تا بازو یه ماتریس $d \times d$ رو ضرب یا وارون کنیم). توی خیلی از کاربردا $K$ ممکنه خیلی زیاد باشه (مثلاً هزاران action مثل تبلیغات یا پیشنهادات) و $d$ هم بزرگ باشه (اگه ویژگی‌ها با بعد بالا باشن). واسه همین ترفندایی مثل فرمول Sherman-Morrison یا آپدیت rank-one استفاده می‌شن که بتونیم $A^{-1}$ رو با زمان $O(d^2)$ آپدیت کنیم (چون $A_t = A_{t-1} + x x^\top$ یه آپدیت rank-1 حساب میشه)، و حساب کردن امتیاز هر بازو هم $O(d)$ زمان می‌بره، پس در کل $O(Kd + d^2)$ در هر راند. که معمولاً شدنیه.

Thompson Sampling هم تو همین مایه‌هاست، چون واسه sample گرفتن از Gaussian باید Cholesky انجام بدیم که $O(d^2)$ طول می‌کشه. حالا اگه $K$ خیلی بزرگ یا حتی بی‌نهایت باشه (مثلاً action رو از یه فضای پیوسته بخوایم انتخاب کنیم)، اون‌وقت باید از روشای optimization یا sampling استفاده کنیم تا بهترین بازو رو پیدا کنیم. 

$$
\max_a x_{t,a}^\top \hat{\theta}
$$

توی مدل خطی، انتخاب بهترین بازو با داشتن $\hat{\theta}$ همین فرم بهینه‌ساز رو داره. اگه $x_{t,a}$ها از یه مجموعه ترکیبی بیان، ممکنه هر راند نیاز به حل یه مسئله بهینه‌سازی داشته باشیم. تحقیقای زیادی روی این موضوع شده (مثل combinatorial bandits و اینا)، ولی دیگه خارج از بحث ماست.

آخر سر، بد نیست یه اشاره‌ای کنیم به ارتباط contextual bandits با supervised learning ولی با feedback bandit: این مسئله رو میشه اینجوری دید که داریم سعی می‌کنیم یه predictor (policy) یاد بگیریم ولی فقط یه بخشی از feedback رو می‌بینیم. یه سری روش هست به اسم reduction که contextual bandit رو تبدیل می‌کنن به classification حساس به cost یا مسئله regression و بعد با importance weighting، از الگوریتمای supervised learning (مثلاً random forest، neural networks و غیره) استفاده می‌کنن که با feedback ناقص کنار بیان. البته این جور الگوریتما یه مقدار از کارایی آماری خودشون رو فدا می‌کنن. الگوریتما مثل LinUCB و TS اما رویکردای *مستقیم* هستن که ساختار خاص مدل رو کامل به کار می‌گیرن تا regret بهینه بدن. حالا توی عمل، اگه context خیلی پیچیده یا با بعد بالا باشه (مثلاً عکس خام یا متن)، ممکنه مجبور بشیم بریم سراغ همین reductionها با deep learning، و یه مقدار از اون بهینگی تئوریکی بزنیم، در عوض قدرت مدل‌سازی بیشتری داشته باشیم.






## کاربردها و مثال‌ها

الگوریتم‌های contextual bandit تو خیلی از سیستم‌های تعاملی آنلاین با موفقیت استفاده شدن. اینجا می‌خوایم چند تا از کاربردهای معروف رو نشون بدیم تا معلوم شه این مفاهیم چطوری تو دنیای واقعی پیاده می‌شن:

- **پیشنهاد مقاله‌ی خبری**: همون‌طور که قبلاً گفتیم، Yahoo! از contextual bandits استفاده کرد تا صفحه‌ی اول سایتش رو با توجه به کاربرها شخصی‌سازی کنه. تو این کاربرد، هر بار که یه کاربر میاد تو سایت، اون لحظه یه $context$ حساب می‌شه (مثلاً ویژگی‌هایی مثل سن و سال، رفتار قبلی کاربر، یا حتی ویژگی‌های مقاله‌هایی که قراره پیشنهاد داده بشن)، و $arms$ همون مقاله‌های مختلفی‌ان که ممکنه توی بخش $featured$ نمایش داده بشن. $reward$ هم اینه که کاربر کلیک کرده یا نه (شاید چقدر خونده هم مهم باشه ولی کلیک مهم‌ترین سیگناله). الگوریتم $contextual\ bandit$ که اونجا استفاده شد $LinUCB$ بود که کارش این بود که $adaptive$ بتونه مقاله‌هایی رو انتخاب کنه که بیشترین کلیک رو بگیرن:

$$
\text{LinUCB algorithm implementation for context-based recommendation}
$$

سیستم اول با استفاده از داده‌هایی که قبلاً $logged$ شده بودن آموزش دید و بعدش رفت تو حالت $online$ و همچنان یاد می‌گرفت. نتیجه‌ش این شد که نرخ کلیک حسابی بهتر شد نسبت به روش‌هایی که بدون $context$ بودن. یه ویژگی مهم این بود که الگوریتم می‌تونست خیلی زود $exploit$ انجام بده روی مقاله‌هایی که واسه بعضی از کاربرا خوب جواب می‌دادن، ولی در کنارش هنوز $explore$ هم می‌کرد تا شاید مقاله‌ی بهتری پیدا کنه. دیتاست $Yahoo!\ Today\ Module$ که الان عمومی هم هست ([این دیتا](https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=49)) الان یکی از $benchmark$‌های استاندارد واسه الگوریتم‌های $contextual\ bandit$ حساب می‌شه.

- **تبلیغات آنلاین**: تو تبلیغات بنری ($display$)، وقتی یه صفحه‌ی وب $load$ می‌شه، الگوریتم باید از بین یه سری $ad$ تصمیم بگیره کدومو نشون بده. $context$ می‌تونه شامل اطلاعاتی مثل محتوای صفحه، کاربر (اگه $logged\ in$ کرده باشه یا از طریق $cookie$ شناخته بشه)، زمان روز و این جور چیزا باشه. $arms$ همون $ads$ هستن یا شاید $campaigns$. $reward$ معمولاً کلیکه یا یه $conversion$ بعدش. الگوریتم‌هایی مثل $Thompson\ Sampling$ خیلی وقتا توسط پلتفرم‌های تبلیغاتی استفاده می‌شن تا نرخ کلیک رو زیاد کنن و به صورت خودکار $impressions$ بیشتری بدن به اون $ads$ی که تو یه $context$ خاص بهتر عمل می‌کنن:

$$
\text{Thompson Sampling based adaptive ad selection}
$$

برخلاف $A/B\ testing$ سنتی که یه $ad$ باید کلی تست شه تا بشه بهترین، توی $bandit$ها این فرایند دائمیه. یعنی اگه سلیقه‌ی کاربرها عوض بشه یا یه $ad$ جدید اضافه شه، الگوریتم بلافاصله خودش رو وفق می‌ده. شرکتایی مثل $Google$ و $Facebook$ بخش‌هایی از سیستم تبلیغاتیشون رو دارن که اساساً حل‌کننده‌های $contextual\ bandit$ هستن در مقیاس خیلی بالا.

- **سیستم‌های recommender**: غیر از اخبار، از $bandit$ها برای پیشنهاد محصول، آهنگ، ویدیو یا هر چیزی که $feedback$ کاربر سریع قابل دیدنه استفاده می‌شه. مثلاً یه سرویس پخش موسیقی ممکنه از $bandit$ استفاده کنه برای اینکه آهنگ بعدی یا $playlist$ رو پیشنهاد بده و ببینه که کاربر چی رو $skip$ یا $like$ می‌کنه. $context$ می‌تونه پروفایل کاربر و گوش‌دادن‌های اخیرش باشه و $arms$ هم آهنگ‌های پیشنهادی. $exploration$ اینجا خیلی مهمه چون نمی‌خوایم کاربر فقط یه سری چیز تکراری ببینه و توی $bubble$ بمونه. $Netflix$ و سرویس‌های مشابه از الگوریتم‌های شبیه $bandit$ برای پیشنهاد محتوا و حتی واسه بهینه‌سازی ظاهر سایت استفاده کردن (مثلاً اینکه کدوم $artwork$ رو نشون بدن برای یه فیلم – $context$ می‌تونه تاریخچه‌ی تماشای کاربر باشه و $arms$ همون $artwork$های مختلف).

- **شخصی‌سازی رابط کاربری و A/B تست**: سایت‌ها معمولاً باید تصمیم بگیرن که چه $layout$ی رو نشون بدن، چه $notification$ی بده، یا ترتیب محتوا چطور باشه. این چیزا رو می‌شه با مدل $bandit$ دید که هر بار بازدید یه $context$ جدید به حساب میاد (مثلاً دستگاه کاربر، $referrer$ و اینا) و $arms$ نسخه‌های مختلف رابط یا نوتیفیکشن‌ها هستن. $reward$ هم می‌تونه تعامل کاربر، $conversion$ یا بقیه‌ی شاخص‌های بیزینسی باشه. $contextual\ bandit$ها یه راه درست و حسابی می‌دن برای اینکه "$A/B\ testing$ در لحظه" انجام شه – یعنی خودکار ترافیک بیشتر بره سمت نسخه‌هایی که بهتر جواب دادن، در حالی که هنوز بعضی نسخه‌ها تست می‌شن تا مطمئن شن هنوز ضعیف‌ترن. بعضی وقتا به این کار می‌گن:

$$
\text{Multi-Armed Bandit Optimization for Web Design}
$$

یه سری پلتفرم‌های آنالیز و بهینه‌سازی هستن که این الگوریتم‌ها رو دارن و کمک می‌کنن آزمایش‌ها $real-time$ و $adaptive$ انجام بشن، نه اینکه صبر کنیم یه دوره‌ی تست تموم شه.

- **آزمایش‌های بالینی و پزشکی شخصی‌سازی‌شده**: تو دنیای پزشکی، می‌شه هر بیمار رو با ویژگی‌هاش یه $context$ دونست، درمان‌ها هم می‌شن $arms$، و نتیجه‌ی درمان $reward$. یه $contextual\ bandit$ می‌تونه انتخاب درمان رو با توجه به ویژگی‌های بیمار تغییر بده تا بهترین نتیجه رو بگیره. این در واقع یه مدل ساده از $personalized\ medicine$ یا $adaptive\ experimentation$ هست. البته تو دنیای واقعی آزمایش‌های بالینی یه عالمه محدودیت اخلاقی و اجرایی دارن، ولی از اصول $bandit$ برای طراحی این آزمایش‌ها استفاده شده (مثلاً اینکه کم‌کم بیمارهای بیشتری رو بفرستن سمت درمان‌هایی که داره بهتر جواب می‌ده ولی همچنان یاد بگیره). اینجا $regret$ یعنی اینکه بیمار یه درمان نه‌چندان خوب بگیره، پس کم کردن $regret$ مستقیماً به نفع سلامتی بیماره. اطلاعات $contextual$ مثل ژن یا سابقه‌ی پزشکی می‌تونه توی این تصمیم کمک کنه.

هر کاربرد هم قلق خاص خودش رو داره. مثلاً توی سیستم‌های پیشنهاد، تعداد $arms$ می‌تونه خیلی زیاد باشه یا هی تغییر کنه (مثلاً آیتم‌های جدید بیان). برای همین، باید راهکارهای مقیاس‌پذیر استفاده شه، مثلاً از $embedding$ها یا مدل‌های خطی پنهان برای کم کردن $dimension$. تو تبلیغات، تاخیر تو دیدن $reward$ (مثلاً وقتی $conversion$ رو ملاک می‌گیریم) یا اینکه کیفیت $ads$ با زمان تغییر کنه، قضیه رو سخت‌تر می‌کنه. ولی در کل، ایده‌ی اصلی $contextual\ bandit$ها همونه: از $context$ استفاده می‌کنی تا $reward$ رو پیش‌بینی کنی، بین گزینه‌هایی که مطمئن نیستی $exploration$ انجام بدی، و در نهایت $cumulative\ reward$ رو زیاد کنی.

یه چیز دیگه که باید حواسمون باشه ارزیابی الگوریتم‌های $contextual\ bandit$ هست. چون این الگوریتم‌ها معمولاً $online$ اجرا می‌شن و هی تغییر می‌کنن، برای ارزیابی $offline$ باید داده‌هایی داشته باشیم که توش $exploration$ شده باشه. یه سری روش‌ها هستن مثل $off\text{-}policy\ evaluation$ با استفاده از $inverse\ propensity\ scoring$ که اساس کارش اینه که داده‌ی $logged$ (که از یه $policy$ خاص گرفته شده) رو وزن‌گذاری می‌کنی تا ببینی اگه یه $policy$ جدید بود، چطور عمل می‌کرد:

$$
\text{Off-policy Evaluation with Inverse Propensity Scoring}
$$

این بحثش یه کم مفصله و الان واردش نمی‌شیم، ولی تو صنعت خیلی مهمه چون معمولاً نمی‌خوان یه الگوریتم رو همین‌جوری بندازن تو سیستم، اول با داده‌های قبلی تستش می‌کنن. واسه همین باید مطمئن باشن اون داده‌ی قبلی از یه $policy$ گرفته شده که $exploration$ کافی انجام داده بوده (یعنی همه‌ی $actions$ یه احتمالی داشتن). واسه همینه که الگوریتم‌های $bandit$ معمولاً یه مقدار تصادفی کار می‌کنن یا $explore$ می‌زنن، تا هم یاد بگیرن هم داده‌ی خوبی واسه ارزیابی جمع کنن.
