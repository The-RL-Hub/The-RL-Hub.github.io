
# چیزای پایه‌ای از تئوری احتمال برای RL

*Reinforcement learning (RL)* بر پایه‌ی مفاهیم پایه‌ای تئوری احتمال بنا شده. توی این فصل می‌خوایم یه مروری روی ایده‌های مهم تئوری احتمال داشته باشیم تا یه پایه‌ی محکم و درست‌حسابی برای RL بسازیم. فرض می‌کنیم که خواننده یه آشنایی اولیه با تئوری احتمال داره، و تمرکزمون روی تعریفای دقیق، قضیه‌ها و توضیحاتیه که بیشتر سمت ریاضی‌ان تا بیس رو بسازیم تا جایی که نیازه.

توی این بخش‌ها قراره راجع به متغیرهای تصادفی و توزیع‌ها، امید ریاضی و واریانس، *conditional probability* و قانون بیز، مستقل بودن، توزیع‌های مهم، و Limit Theoremها (*LLN* و *CLT*) حرف بزنیم. همه‌ی اینا رو هم هر جا که به RL مربوط باشه، توی اون *context* بررسی می‌کنیم.


## متغیرهای تصادفی و توزیع‌ها

یه **متغیر تصادفی (r.v.)** در واقع یه تابع عددیه که نتیجه‌ی یه آزمایش تصادفی رو می‌گیره و به یه عدد واقعی وصلش می‌کنه. تعریفی بخوایم بگیم، متغیر تصادفی هر نتیجه‌ی تو فضای نمونه رو میاره به یه عدد حقیقی نگاشت می‌کنه. معمولاً متغیرهای تصادفی رو با حروف بزرگ (مثلاً $X,Y$) نشون میدن. راحت بخوایم بگیم، $X$ بر اساس یه توزیع احتمالی که پشت اون آزمایش خوابیده، یه سری مقدار رو «برمی‌داره».

**گسسته در مقابل پیوسته:** اگه یه متغیر تصادفی بتونه فقط یه مجموعه‌ی قابل‌شماری از مقدارهای متمایز (یا متناهی یا بینهایت شمارا) بگیره، بهش میگیم **گسسته**. تو این حالت احتمال همه‌ی این مقدارها با هم میشه ۱. حالا برعکس، یه متغیر تصادفی **پیوسته** می‌تونه هر مقداری تو یه بازه بگیره (که دیگه مقدارهاش قابل‌شمارش نیست). برای یه r.v. پیوسته، احتمال اینکه دقیقاً یه مقدار خاص رو بگیره صفره – به جاش احتمال‌ها رو روی بازه‌ها تعریف می‌کنیم. در واقع برای هر $x$ داریم:

$$
P(X=x)=0
$$

توزیع یه r.v. گسسته با یه **Probability Mass Function (PMF)** تعریف میشه، ولی توزیع یه r.v. پیوسته با یه **Probability Density Function (PDF)** مشخص میشه. هر دوتاشون یه **Cumulative Distribution Function (CDF)** هم دارن.

**Probability Mass Function (PMF):** برای یه r.v. گسسته $X$، PMF میگه احتمال گرفتن هر مقدار چقدره. طبق تعریف:

$$
p_X(x)=P(X=x)
$$

PMF برای هر مقدار ممکن غیرمنفیه و رو همه‌ی مقدارهایی که $X$ می‌تونه بگیره جمعش میشه ۱. مثلاً اگه $X$ تعداد شیرهایی باشه که تو ۲ تا پرتاب سکه درمیاد، $X$ یه متغیر گسسته‌ست با support $\{0,1,2\}$ و PMF به صورت:

$$
P(X=0)=\frac{1}{4},\quad P(X=1)=\frac{1}{2},\quad P(X=2)=\frac{1}{4}
$$

همه‌ی جرم‌های احتمال روی این مقدارها پخش شده و جای دیگه $P(X=x)=0$.

**Probability Density Function (PDF):** برای یه r.v. پیوسته $X$، PDF در واقع مشتق CDF هست. تعریفی بخوایم بگیم، اگه $F(x)$ همون CDF باشه، اون وقت:

$$
f(x)=F'(x)
$$

PDF میگه چقدر احتمال داره $X$ تو یه بازه‌ی کوچولو یه مقدار خاص رو بگیره. برخلاف PMF، خود $f(x)$ احتمال نیست – یعنی حتی می‌تونه از ۱ هم بزرگتر باشه – ولی وقتی PDF رو روی یه بازه انتگرال می‌گیریم، اون موقع یه احتمال درمیاد. برای هر بازه‌ی $[a,b]$، داریم:

$$
P(a\leq X\leq b)=\int_a^b f(x)\,dx
$$

یه PDF درست حسابی باید همیشه $f(x)\geq 0$ باشه و:

$$
\int_{-\infty}^{\infty} f(x)\,dx=1
$$

support یه r.v. پیوسته هم جاییه که $f(x)>0$. یادمون نره که برای r.v.های پیوسته همیشه $P(X=x)=0$ و باید از انتگرال PDF استفاده کنیم تا یه احتمال درست بگیریم.

**Cumulative Distribution Function (CDF):** CDF یعنی $F_X(x)$ که نشون میده احتمال اینکه متغیر تصادفی کمتر یا مساوی $x$ باشه چقدره. طبق تعریف:

$$
F_X(x)=P(X\leq x)
$$

CDF برای *همه‌ی* متغیرهای تصادفی (چه گسسته، چه پیوسته، چه مخلوط) تعریف شده. برای r.v. گسسته، CDF مثل یه نردبونه که سر هر مقدار ممکن یه پله میره بالا؛ ولی برای r.v. پیوسته، CDF صاف و نرمه و بدون پرش میره بالا. CDF همیشه غیرکاهنده‌ست، راست‌پیوسته‌ست و شرط‌های زیر رو داره:

$$
\lim_{x\to -\infty}F(x)=0,\quad \lim_{x\to \infty}F(x)=1
$$

از روی یه PMF، CDF رو با جمع کردن احتمال‌ها تا $x$ به دست میاریم، و از روی یه PDF با انتگرال گرفتن تا $x$:

$$
F(x)=\int_{-\infty}^x f(t)\,dt
$$

*مثال:* **توزیع گسسته (Binomial)** – شکل، PMF و CDF برای $X\sim \mathrm{Bin}(4,0.5)$ رو نشون میده. PMF (سمت چپ) چندتا spike داره رو عددهای صحیح ۰ تا ۴ (اینجا به خاطر ۴ بار پرتاب سکه‌ی عادلانه متقارنه). CDF (سمت راست) یه تابع پلکانیه: بین عددهای صحیح صاف میمونه و رو عددهای خاص یهویی میره بالا. مثلاً:

$$
P(X=2)\approx 0.375
$$

یعنی تو $x=2$ یه پرش به اندازه‌ی همین مقدار داریم. CDF تو $x=4$ میرسه به ۱، که یعنی:

$$
P(X\leq 4)=1
$$

همون چیزی که انتظار داریم چون ۴ بیشترین مقداریه که $X$ می‌تونه بگیره.

*مثال:* **توزیع پیوسته (Normal)** – شکل، PDF (سمت چپ) و CDF (سمت راست) برای توزیع نرمال استاندارد $Z\sim N(0,1)$ رو نشون میده. PDF همون منحنی زنگوله‌ای معروفه (اینجا با میانگین ۰ و انحراف معیار ۱) و فرمولش اینه:

$$
f(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
$$

CDF هم یه منحنی S-شکل نرمه که از ۰ به ۱ افزایش پیدا میکنه. نکته‌ی جالب اینکه CDF اینجا هیچ پرشی نداره؛ یعنی:

$$
P(Z=z)=0
$$

برای هر مقدار دقیق $z$، ولی اگه بخوایم مثلاً $P(-1<Z<1)$ رو حساب کنیم باید PDF رو روی اون بازه انتگرال بگیریم. قله‌ی PDF تو ۰ باعث میشه CDF دور و بر ۰.۵ سریع‌تر از همه جا رشد کنه (چون به خاطر تقارن $P(Z\leq 0)=0.5$ میشه). این نمودارها خیلی خوب نشون میدن که توزیع‌های گسسته و پیوسته چطوری تو انباشت احتمال‌ها (جمع در مقابل انتگرال) با هم فرق دارن.

فهمیدن PMFها، PDFها و CDFها تو RL خیلی مهمه، چون اینا عدم قطعیت تو نتایج، ریواردها، و انتقال وضعیت‌ها رو توضیح میدن. مثلاً توزیع ریوارد برای یه policy تصادفی معمولاً با یه PMF یا PDF مشخص میشه و CDF کمک میکنه که احتمال ریواردهای تجمعی یا رخدادهای آستانه‌ای رو حساب کنیم. خلاصه‌ش اینکه توزیع یه متغیر تصادفی گسسته با یه سری جرم احتمال روی نقطه‌های خاص داده میشه ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=In%20probability%20%20and%20,78%20whose%20domain%20is%20discrete))، ولی توزیع یه متغیر تصادفی پیوسته با یه منحنی چگالی داده میشه که باید انتگرالش رو گرفت ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=A%20probability%20mass%20function%20differs,3)). هر دوتا CDF رو به عنوان یه توصیف یکپارچه دارن:

$$
F_X(x)=P(X\leq x)
$$

که همیشه برقرار میمونه.











## امید ریاضی، واریانس و کوواریانس

### امید ریاضی

**امید ریاضی** یه چیزی از اون مفاهیم پایه و خیلی پرکاربرد تو احتمال و آمار به حساب میاد. یه جورایی خلاصه‌ی رفتار یه متغیر تصادفیه که نشون میده تو طولانی مدت چطوری رفتار می‌کنه. اگه بخوایم راحت‌تر بگیم، امید ریاضی مثل *مرکز جرم* توزیع یه متغیره، جایی که هر خروجی ممکن، نسبت به احتمال اتفاق افتادنش وزن داده میشه. بعضی وقتا بهش میگن **میانگین**.

ترجمه تعریفی:  
اگه یه متغیر تصادفی **گسسته** $X$ با **تابع جرم احتمال (PMF)** $p(x) = \mathbb{P}(X = x)$ داشته باشیم، امید ریاضی $X$ اینطوری تعریف میشه:

$$
\mathbb{E}[X] = \sum_x x\, p(x),
$$

به شرطی که این جمع مطلقاً همگرا باشه، یعنی $\sum_x |x|\, p(x) < \infty$. اینطوری مطمئن میشیم که امید ریاضی تعریف شده و مقدارش بی‌نهایت نمیشه.

اگه متغیرمون **پیوسته** باشه و **تابع چگالی احتمال (PDF)** داشته باشه، تعریفش اینجوریه:

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x\, f(x)\, dx,
$$

که اینجا هم باید مطمئن باشیم که انتگرال مطلقاً همگراست.

#### برداشت شهودی
چه گسسته باشه چه پیوسته، داستان یکیه: باید هر مقدار ممکن رو در احتمال (یا چگالیش) ضرب کنیم و بعد همه رو با هم جمع بزنیم (یا انتگرال بگیریم). خلاصه اینکه یه میانگین وزندار درست می‌کنیم که چیزایی که احتمال بیشتری دارن تاثیر بیشتری رو نتیجه نهایی دارن.

#### نوتیشن
برای نشون دادن امید ریاضی معمولا اینطوری می‌نویسن:
- $\mathbb{E}[X]$ 
- یا با حرف یونانی $\mu$ (وقتی میخوایم تاکید کنیم که میانگین یه توزیعه).

یادت باشه که امید ریاضی یه *عدد قطعی*ـه — خودش دیگه تصادفی نیست، فقط خلاصه‌ای از یه چیز تصادفیه.


#### ویژگی‌های اصلی امید ریاضی

امید ریاضی چندتا ویژگی خفن داره که باعث میشه تو کارهای نظری و کاربردی خیلی دستمون رو باز کنه. دوتا از مهم‌تریناش **امید ریاضی یه مقدار ثابت** و **خطی بودن امید ریاضی** هستن.

#### ویژگی ۱: امید ریاضی یه مقدار ثابت

اگه $c$ یه عدد ثابت باشه (یعنی تصادفی نباشه)، اونوقت:

$$
\mathbb{E}[c] = c.
$$

**اثبات:**

چون $c$ تصادفی نیست، همیشه همون $c$ رو میگیره با احتمال ۱. پس چه تو جمع چه تو انتگرال، داریم فقط $c$ رو با وزن ۱ حساب می‌کنیم:

$$
\mathbb{E}[c] = \sum_x c \cdot p(x) = c \sum_x p(x) = c \cdot 1 = c,
$$

یا اگه پیوسته باشه:

$$
\mathbb{E}[c] = \int_{-\infty}^{\infty} c \cdot f(x)\, dx = c \int_{-\infty}^{\infty} f(x)\, dx = c \cdot 1 = c,
$$

چون مجموع کل احتمال یا چگالی باید ۱ باشه.


#### ویژگی ۲: خطی بودن امید ریاضی

**بیان:**  
اگه دوتا متغیر تصادفی $X$ و $Y$ داشته باشیم و یه عدد ثابت $a$ از دنیای $\mathbb{R}$، اونوقت:

$$
\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y],
$$

$$
\mathbb{E}[aX] = a\, \mathbb{E}[X].
$$

**نکته‌ی خیلی مهم:**  
این خاصیت بدون اینکه نیازی به استقلال $X$ و $Y$ داشته باشیم، همیشه برقرار میشه. یعنی متغیرها هرچقدر هم بهم چسبیده باشن، بازم خطیته سر جاشه.


#### اثبات خطی بودن امید ریاضی

بیایید تو حالت گسسته اثبات کنیم (تو حالت پیوسته همین روال با انتگرال پیش میره).

فرض کن $X$ و $Y$ گسسته باشن. اونوقت طبق تعریف:

$$
\mathbb{E}[X+Y] = \sum_{x,y} (x+y)\, \mathbb{P}(X=x, Y=y).
$$

اگه باز کنیمش:

$$
= \sum_{x,y} x\, \mathbb{P}(X=x, Y=y) + \sum_{x,y} y\, \mathbb{P}(X=x, Y=y).
$$

میشه اینطوری مرتبش کرد:

$$
= \sum_{x} x \left( \sum_y \mathbb{P}(X=x, Y=y) \right) + \sum_{y} y \left( \sum_x \mathbb{P}(X=x, Y=y) \right).
$$

اینجا باید بدونی که:

$$
\sum_y \mathbb{P}(X=x, Y=y) = \mathbb{P}(X=x)
$$

و

$$
\sum_x \mathbb{P}(X=x, Y=y) = \mathbb{P}(Y=y),
$$

چون داریم روی یکی حاشیه می‌گیریم.

در نتیجه:

$$
\mathbb{E}[X+Y] = \sum_x x\, \mathbb{P}(X=x) + \sum_y y\, \mathbb{P}(Y=y) = \mathbb{E}[X] + \mathbb{E}[Y].
$$

برای ضرب در یه ثابت هم که واضحه:

$$
\mathbb{E}[aX] = \sum_x (a x)\, \mathbb{P}(X=x) = a \sum_x x\, \mathbb{P}(X=x) = a\, \mathbb{E}[X].
$$


#### شهود خطی بودن

ممکنه یه کم عجیب به نظر برسه که وابستگی بین $X$ و $Y$ تاثیری روی امید ریاضی جمع‌شون نداره. ولی واقعاً اینطوریه چون امید ریاضی در اصل یه میانگینه، و میانگین‌ها با جمع جور در میان، فرقی هم نمی‌کنه پشت قضیه وابستگی باشه یا نه.

یه کم دقیق‌تر بخوایم بگیم:
- امید ریاضی یه جمع وزن‌داره (یا یه انتگراله).
- جمع داخل انتگرال به صورت خطی پخش میشه.
- فرآیند "میانگین گرفتن" (جمع وزن‌دار) وابستگی رو اصلاً حس نمی‌کنه.

پس خطی بودن، در واقع از ویژگی‌های پایه‌ی جمع و انتگرال میاد.

این باعث میشه امید ریاضی یه ابزار خیلی **قوی** و **مقاوم** باشه، مخصوصاً تو تحلیل‌های احتمالاتی، آمار و ماشین لرنینگ.


### امید ریاضی تو RL

اصل خطی بودن امید ریاضی یه کاربرد خیلی مستقیم و مهم توی **RL** داره، مخصوصاً وقتی که داریم **returns** و **value functions** رو بررسی می‌کنیم.

فرض کن یه ایجنت از گام زمانی $t$ شروع می‌کنه و یه دنباله‌ای از ریواردهای (احتمالاً تصادفی) زیر رو جمع می‌کنه:

$$
R_{t+1}, R_{t+2}, R_{t+3}, \ldots,
$$

و اینا رو می‌ریزه تو یه **discounted return**:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots,
$$

که توش $0 \leq \gamma < 1$ یه discount factor حساب میشه (که کمک می‌کنه جمع وا نره و جمع بشه).

امید ریاضی این بازگشت تصادفی، وقتی که حالت شروع $S_t = s$ باشه، **state-value function** رو تحت یه policy $\pi$ تعریف می‌کنه:

$$
v_\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right].
$$

حالا با استفاده از خطی بودن امید ریاضی:

$$
\mathbb{E}_\pi[G_t] = \mathbb{E}_\pi\left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \right] = \mathbb{E}_\pi[R_{t+1}] + \gamma \mathbb{E}_\pi[R_{t+2}] + \gamma^2 \mathbb{E}_\pi[R_{t+3}] + \cdots.
$$

یعنی **می‌تونیم دونه دونه** امید ریاضی هر ریوارد آینده رو حساب کنیم (با discount مناسبش) و بعد جمعشون کنیم.


#### درآوردن معادله Bellman Expectation

می‌تونیم امید ریاضی بازگشت رو یه جوری بازگشتی باز کنیم:

$$
v_\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right],
$$

که با توجه به خطی بودن، میشه:

$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} \mid S_t = s] + \gamma\,\mathbb{E}_\pi[G_{t+1} \mid S_t = s].
$$

نکته‌ی اصلی اینجاست که:
- جمله‌ی اول، یعنی $\mathbb{E}_\pi[R_{t+1} \mid S_t = s]$، فقط به ریوارد لحظه‌ای مربوطه.
- جمله‌ی دوم، یعنی $\mathbb{E}_\pi[G_{t+1} \mid S_t = s]$، مربوط به اتفاقات آینده است — از حالت بعدی $S_{t+1}$ شروع میشه.

میشه نوشت:

$$
\mathbb{E}_\pi[G_{t+1} \mid S_t = s] = \sum_{s'} \Pr(S_{t+1} = s' \mid S_t = s, \pi) v_\pi(s').
$$

حالا که اینو دونستیم:

$$
v_\pi(s) = \sum_{s'} \Pr(S_{t+1} = s' \mid S_t = s, \pi) \left( R(s, \pi, s') + \gamma v_\pi(s') \right).
$$

این معادله همون **Bellman expectation equation** معروفه که ستون فقرات خیلی از الگوریتم‌های RL مثل dynamic programming، temporal difference learning و policy iteration هست.


#### سمپلینگ و تخمین امید ریاضی

در آخر، یه نکته‌ی کاربردی از خطی بودن تو تخمین‌های تجربیه. فرض کن $n$ تا نمونه‌ی مستقل از یه متغیر تصادفی $X$ داریم، یعنی $x_1, x_2, \ldots, x_n$. اونوقت میانگین سمپلی:

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n x_i
$$

یه برآوردگر(واقعاً کلمه خفنیه!) بدون بایاس از $\mathbb{E}[X]$ حساب میشه.

چرا؟

چون:

$$
\mathbb{E}[\bar{X}_n] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] = \mathbb{E}[X],
$$

به خاطر خطی بودن امید ریاضی.


### واریانس

expectation فقط نشون میده وسط یا "میانگین" یه متغیر تصادفی کجاست، ولی هیچ حرفی درباره‌ی این نمی‌زنه که اون توزیع چقدر دور و بر اون میانگین پخش شده. دو تا توزیع میتونن expectation یکسان داشته باشن ولی کلی فرق توی "پراکندگی"شون باشه.

برای اینکه بتونیم **پراکنده بودن** یا **پخش بودن** یه متغیر تصادفی دور و بر میانگینش رو اندازه بگیریم، میایم و **واریانس** رو تعریف می‌کنیم.

#### تعریف واریانس

برای یه متغیر تصادفی $X$، **واریانس** رو اینطوری تعریف می‌کنیم:

$$
\operatorname{Var}(X) = \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
$$

خلاصه‌ش اینه:
- اختلاف $X$ با میانگینش $\mathbb{E}[X]$ رو حساب کن (یعنی ببین یه نتیجه‌ی خاص چقدر با مقدار مورد انتظارش فاصله داره).
- این اختلاف رو به توان دو برسون (که هم اختلاف‌های مثبت و هم منفی رو حساب کنیم).
- بعدش هم expectation این مقدار رو بگیر (یعنی average squared distance).

در کل، واریانس نشون میده یه متغیر چقدر به طور میانگین از وسط خودش فاصله داره.

---

#### فرمول‌های واریانس برای حالت گسسته و پیوسته

مثل expectation، واریانس هم هم برای متغیرهای گسسته تعریف میشه هم برای متغیرهای پیوسته:

- **حالت گسسته** (وقتی که PMF یعنی $p(x)$ داشته باشیم):

$$
\operatorname{Var}(X) = \sum_x (x - \mathbb{E}[X])^2 p(x)
$$

- **حالت پیوسته** (وقتی که PDF یعنی $f(x)$ داشته باشیم):

$$
\operatorname{Var}(X) = \int_{-\infty}^\infty (x - \mathbb{E}[X])^2 f(x) \, dx
$$

توی هر دو حالت، این کمیت موقعی خوب تعریف شده که جمع یا انتگرال $(x-\mathbb{E}[X])^2$ ضربدر احتمال یا چگالی به خوبی جمع بشه (یعنی همگرا باشه).

---

#### یه فرمول دیگه برای واریانس

یه فرمول دیگه هم داریم که از بسط دادن اون توان دومی که بالاتر دیدیم درمیاد:

$$
\operatorname{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

**اثبات:**

تعریف واریانس رو باز می‌کنیم:

$$
\operatorname{Var}(X) = \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right] = \mathbb{E}\left[ X^2 - 2X\mathbb{E}[X] + (\mathbb{E}[X])^2 \right]
$$

با خاصیت خطی بودن امید ریاضی:

$$
= \mathbb{E}[X^2] - 2\mathbb{E}[X]\mathbb{E}[X] + (\mathbb{E}[X])^2 = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

این فرمول خیلی وقتا محاسبه رو راحت‌تر می‌کنه، چون دیگه لازم نیست مستقیم بریم $(X-\mathbb{E}[X])^2$ رو حساب کنیم.

---

#### ویژگی‌های واریانس

1. **واریانس یه مقدار ثابت:**

$$
\operatorname{Var}(c) = 0
$$

برای هر عدد ثابت $c$، چون وقتی $X=c$ باشه، همیشه $(X-\mathbb{E}[X]) = 0$ میشه.

2. **خاصیت Scaling (دوباره فارسیشو نمیدونم ولی همون ضرب در یه مقدار ثابت):**

اگه یه عدد ثابت $a$ داشته باشیم:

$$
\operatorname{Var}(aX) = a^2 \operatorname{Var}(X)
$$

**اثبات:**

$$
\operatorname{Var}(aX) = \mathbb{E}\left[(aX - \mathbb{E}[aX])^2\right] = \mathbb{E}\left[(a(X-\mathbb{E}[X]))^2\right] = a^2 \mathbb{E}\left[(X-\mathbb{E}[X])^2\right] = a^2 \operatorname{Var}(X)
$$

یعنی واریانس با مربع اون عدد ثابت تغییر می‌کنه.

3. **واریانس جمع دو متغیر (حالت کلی):**

واسه هر دوتا متغیر تصادفی $X$ و $Y$:

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)
$$

که توش $\operatorname{Cov}(X,Y)$ همون **کواریانس** بین $X$ و $Y$ هست.

- اگه $X$ و $Y$ مستقل باشن، یعنی $\operatorname{Cov}(X,Y) = 0$، پس:

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)
$$


---

#### شهود تو واریانس

- چون انحراف‌ها رو به توان دو می‌بریم، واریانس انحراف‌های بزرگ‌تر رو سنگین‌تر حساب می‌کنه.
- واریانس کم یعنی $X$ خیلی نزدیک به میانگینشه.
- واریانس زیاد یعنی $X$ کلی پخش شده.

چون داریم مربع می‌کنیم، واحد واریانس هم میشه واحد $X$ به توان دو. واسه همین، خیلی وقتا از **انحراف معیار** (ریشه‌ی واریانس) استفاده می‌کنیم که دوباره واحدش مثل $X$ بشه.

---

### کواریانس

تا اینجا دیدیم واریانس اندازه می‌گیره که یه متغیر تصادفی از خودش چقدر دور میشه. ولی **کواریانس** کاری می‌کنه که بفهمیم **دو تا** متغیر چطوری با هم حرکت می‌کنن.

یعنی بفهمیم آیا با همدیگه میرن بالا و پایین یا مخالف هم حرکت می‌کنن یا اصلاً کاری به کار هم ندارن.

---

#### تعریف کواریانس

**کواریانس** بین دوتا متغیر تصادفی $X$ و $Y$ اینطوری تعریف میشه:

$$
\operatorname{Cov}(X,Y) = \mathbb{E}\left[ (X - \mathbb{E}[X])(Y - \mathbb{E}[Y]) \right]
$$

یعنی چی؟
- ببین $X$ چقدر از میانگین خودش فاصله گرفته و $Y$ هم چقدر.
- این دوتا فاصله رو در همدیگه ضرب کن.
- بعدش هم expectation این حاصل‌ضرب رو بگیر.

نتیجه‌ش:
- اگه معمولاً هر وقت $X$ میره بالا، $Y$ هم میره بالا، کواریانس مثبته.
- اگه یکی میره بالا و اون یکی میره پایین، کواریانس منفیه.
- اگه اصلاً ربطی به هم ندارن، کواریانس تقریباً صفره.

---

#### یه فرمول دیگه برای کواریانس 

اگه تعریف کواریانس رو باز کنیم، می‌رسیم به این:

$$
\operatorname{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

**اثباتش:**

بسط:

$$
(X-\mathbb{E}[X])(Y-\mathbb{E}[Y]) = XY - X\mathbb{E}[Y] - \mathbb{E}[X]Y + \mathbb{E}[X]\mathbb{E}[Y]
$$

بعدش expectation بگیریم:

$$
\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

---

#### ویژگی‌های کواریانس

1. **تقارن:**

$$
\operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X)
$$

2. **کواریانس یه متغیر با خودش:**

$$
\operatorname{Cov}(X,X) = \operatorname{Var}(X)
$$

پس یعنی واریانس یه حالت خاص از کواریانسه.

3. **کواریانس متغیرهای مستقل:**

اگه $X$ و $Y$ مستقل باشن:

$$
\operatorname{Cov}(X,Y) = 0
$$

(چون توی استقلال داریم $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$).

**یه نکته مهم:**  
کواریانس صفر همیشه به معنای استقلال نیست (مگر توی شرایط خاص مثلاً متغیرهای Gaussian).

4. **خطی بودن:**

واسه ضرایب $a,b,c,d$:

$$
\operatorname{Cov}(aX+b, cY+d) = ac\,\operatorname{Cov}(X,Y)
$$

پس کواریانس با ضرایب خطی تغییر می‌کنه.


#### شهود تو کواریانس

- **کواریانس مثبت**: هر وقت $X$ میره بالا، $Y$ هم معمولاً میره بالا.
- **کواریانس منفی**: هر وقت $X$ میره بالا، $Y$ میاد پایین.
- **کواریانس صفر**: انگار هیچ خطی ربطی بینشون نیست.



#### واریانس جمع (با کواریانس)

یادته چی گفتیم؟

$$
\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y)
$$

یعنی واریانس مجموع، یه تنظیمی داره بسته به اینکه $X$ و $Y$ چطوری با هم حرکت می‌کنن.

- اگه هم‌جهت باشن (مثبت همبسته باشن)، واریانس جمع بزرگ‌تر میشه.
- اگه خلاف جهت باشن (منفی همبسته باشن)، واریانس جمع کمتر میشه.




















## احتمال شرطی و قانون بیز

توی خیلی از سناریوهایی که پای عدم قطعیت وسطه، بیشتر از اینکه بخوایم بدونیم احتمال یه اتفاق چقدره، دلمون میخواد بدونیم احتمال اون اتفاق *با توجه به* یه اطلاعاتی که دیدیم چقدره. **احتمال شرطی** همون مفهوم ریاضیه که این ایده رو روال میکنه: اینکه وقتی یه مدرک جدید داریم، چجوری باورمون نسبت به یه اتفاق تغییر میکنه.

### احتمال شرطی

**احتمال شرطی** اینکه یه اتفاق $A$ بیفته وقتی بدونیم یه اتفاق دیگه $B$ افتاده، اینجوری تعریف میشه:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$

البته به شرطی که $P(B) > 0$ باشه.

اینجا:
- $P(A \mid B)$ یعنی "احتمال $A$ با توجه به $B$".
- $P(A \cap B)$ احتمال اینه که جفتشون با هم اتفاق بیفتن.
- $P(B)$ هم احتمال افتادن $B$ ـه.

حس پشت این تعریف خیلی واضحه: وقتی بدونیم $B$ اتفاق افتاده، دیگه کل فضای نمونه رو در نظر نمیگیریم، فقط حالتایی رو نگاه میکنیم که تو $B$ هستن. بین این حالت‌ها، دنبال اونایی میگردیم که تو $A \cap B$ هم باشن. این فرمول دقیقاً داره همین نسبت رو نشون میده:

$$
\text{احتمال شرطی} = \frac{\text{احتمال وقوع هر دو تا اتفاق}}{\text{احتمال اون مدرک معلوم}}
$$

**چندتا خاصیت مهم** احتمال شرطی:
- **نرمالیزیشن:** برای هر رویداد ثابت $B$ که $P(B) > 0$،
  
$$
P(B \mid B) = 1
$$

  چون وقتی میدونیم $B$ افتاده، دیگه قطعی‌ـه که $B$ تو context شرطی شده اتفاق افتاده.
- **مثل یه احتمال ساده حد داره بدیهتاً:** برای هر $A$ و $B$، همیشه داریم $0 \leq P(A \mid B) \leq 1$.

#### مثال (Policy تو RL)

فرض کن یه ایجنت طبق یه policy $\pi$ تو RL داره کار میکنه. وقتی state فعلی $S = s$ باشه، احتمال اینکه ایجنت اکشن $A = a$ رو انتخاب کنه اینطوریه:

$$
P(A = a \mid S = s) = \pi(a \mid s)
$$

اینجا $S = s$ همون شرطیه که داریم، و policy $\pi$ میگه روی این state، اکشن‌ها چه احتمالایی دارن.

پس احتمال شرطی یه پایه مهمه برای اینکه بفهمیم رفتار یه ایجنت تو محیط تصادفی چطوریه.

---

#### بدست آوردن قانون بیز

قانون بیز یکی از پایه‌ای‌ترین نتایج تو نظریه احتماله. این قانون یه روش بهمون میده که بتونیم احتمال‌های شرطی رو "برعکس" کنیم، مخصوصاً وقتی میخوایم دانسته هامون رو بعد از دیدن اطلاعات جدید آپدیت کنیم خیلی مهم میشه.

با شروع از تعریف احتمال شرطی:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B \mid A) = \frac{P(A \cap B)}{P(A)}
$$

میتونیم ببینیم که:

$$
P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)
$$

این دوتا عبارت برای $P(A \cap B)$ رو مساوی میذاریم:

$$
P(A \mid B)P(B) = P(B \mid A)P(A)
$$

حالا دو طرفو بر $P(B)$ تقسیم میکنیم (البته $P(B) > 0$ باشه):

$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$

#### توضیح نوتیشن

- $P(A)$ میشه **prior**: مقداری که قبل از دیدن $B$ داشتیم.
- $P(B \mid A)$ میشه **likelihood**: احتمال اینکه $B$ اتفاق بیفته اگر $A$ درست باشه.
- $P(B)$ همون **marginal** احتماله: احتمال کلی دیدن $B$.
- $P(A \mid B)$ میشه **posterior**: مقداری که بعد از دیدن $B$ داریم.

#### حساب کردن $P(B)$

برای حساب کردن $P(B)$، معمولاً از **قانون احتمال کل** استفاده میکنیم. مثلا اگه $\{A, A^c\}$ یه تقسیم فضای نمونه باشن (یعنی یا $A$ یا مکملش $A^c$ اتفاق میفته)، اونوقت داریم:

$$
P(B) = P(B \mid A)P(A) + P(B \mid A^c)P(A^c)
$$

یا تو حالت کلی‌تر که $\{A_1, A_2, \ldots, A_n\}$ یه تقسیم از فضای نمونه باشه:

$$
P(B) = \sum_{i=1}^{n} P(B \mid A_i)P(A_i)
$$



### امید ریاضی شرطی

همونطوری که میتونیم احتمال یه رویداد رو شرطی کنیم، میتونیم **امید ریاضی** یه متغیر تصادفی رو هم شرطی کنیم. اینجوری میرسیم به مفهوم **امید ریاضی شرطی**.

تعریفی، اگه $X$ و $Y$ دوتا متغیر تصادفی باشن، امید ریاضی شرطی $X$ وقتی بدونیم $Y=y$ اینطوری نوشته میشه:

$$
E[X \mid Y=y]
$$

که یعنی مقدار مورد انتظار $X$ وقتی که $Y$ رو برابر با $y$ بدونیم.

خود امید ریاضی شرطی هم میتونه یه **متغیر تصادفی** باشه:

$$
E[X \mid Y]
$$

که هر $y$ رو به $E[X \mid Y=y]$ وصل میکنه.

**خواصش:**
- **خطی بودن:**

$$
E[aX + bY \mid Z] = aE[X \mid Z] + bE[Y \mid Z]
$$

- **Law of total expectation (راستش فارسیشو نمیدونم :)):**

$$
E[X] = E\left[E[X \mid Y]\right]
$$

یعنی برای حساب امید کلی $X$، اول امید شرطی نسبت به $Y$ رو حساب میکنی، بعد روی توزیع $Y$ میانگین میگیری.

#### شهود

شرطی کردن روی $Y$، عدم قطعیت $X$ رو به دو بخش میشکونه: یکی عدم قطعیت داخل $Y$، یکی هم uncertainty خود $Y$.

#### مثال (State-value function تو RL)

توی RL، تابع مقدار برای یه state $s$ اینجوری نوشته میشه:

$$
v(s) = E[G_t \mid S_t = s]
$$

که $G_t$ مجموع ریواردهای آینده‌ست. یعنی امید ریاضی مجموع ریواردهایی که از state $s$ شروع میشه.

بعضی وقتا، $v(s)$ رو بیشتر میشکونیم و شرطی می‌کنیم روی چیزای دیگه مثل state بعدی $S_{t+1}$، و قانون امید کلی رو بازگشتی اعمال می‌کنیم.


یه جای دیگه هم هست: تو credit assignment تو محیط‌های partially observable، قانون بیز میتونه کمک کنه بفهمیم که یه نتیجه، احتمالاً به خاطر کدوم عامل مخفی بوده.  
در کل، احتمال‌های شرطی تو خود تعریف محیط RL هم وجود دارن: مثلاً محیط با $P(s', r \mid s, a)$ تعریف میشه — یعنی احتمال اینکه از state $s$ با اکشن $a$ به state $s'$ با ریوارد $r$ برسیم.  
این یه توزیع احتمال شرطی از دینامیک محیطه.  
ایجنتی که با $P(s'|s,a)$ یا $P(r|s,a)$ کار می‌کنه، داره عملاً با احتمال شرطی برنامه‌ریزی یا تصمیم‌گیری می‌کنه.











## استقلال و استقلال شرطی

### استقلال

توی تئوری احتمال، مفهوم **استقلال** یعنی اینکه دوتا رویداد یا متغیر تصادفی هیچ تاثیری روی هم ندارن از نظر احتمالاتی. فهمیدن استقلال خیلی مهمه چون معمولاً ساختار مسائل رو راحت‌تر می‌کنه، مخصوصاً وقتی داریم یه سیستم پیچیده رو مدل‌سازی می‌کنیم یا می‌خوایم حساب‌وکتاب کنیم.

#### تعریف

تعریفی بخوایم بگیم، دوتا رویداد $A$ و $B$ **مستقل** هستن اگه احتمال رخ دادن همزمانشون برابر باشه با ضرب احتمال هرکدوم به‌تنهایی:
$$
P(A \cap B) = P(A) P(B).
$$

این معادله ساده‌ترین شکل استقلال رو نشون می‌ده.

وقتی $P(A) > 0$ و $P(B) > 0$، می‌تونیم استقلال رو با استفاده از **احتمال شرطی** هم نشون بدیم:
$$
P(A \mid B) = P(A), \quad \text{و} \quad P(B \mid A) = P(B).
$$
احتمال شرطی $P(A \mid B)$ یعنی احتمال رخ دادن $A$ با دونستن اینکه $B$ افتاده. اگه $P(A \mid B) = P(A)$ باشه، یعنی دونستن $B$ هیچ اطلاعاتی درباره $A$ بهمون نمی‌ده.

خلاصه، اگه دوتا رویداد مستقل باشن، دیدن یکی‌شون هیچ چیز جدیدی درباره اون یکی بهمون نمی‌گه — از نظر احتمال کاری به کار هم ندارن.

#### ویژگی‌های مهم

- **دوطرفه بودن:** استقلال ذاتاً دوطرفه‌ست. یعنی اگه $A$ مستقل از $B$ باشه، پس $B$ هم مستقل از $A$ئه. این از این میاد که $P(A \cap B) = P(B \cap A)$.

- **فرق با جدا بودن:** باید حواسمون باشه که استقلال با **جدا بودن** فرق داره. اگه $A$ و $B$ جدا از هم باشن، یعنی $A \cap B = \emptyset$، پس $P(A \cap B) = 0$. ولی اگه $P(A)$ و $P(B)$ صفر نباشن، معمولاً $P(A)P(B) > 0$، پس رویدادهای جدا از هم معمولاً **مستقل نیستن**. استقلال یعنی *بی‌تأثیری*، نه *نبودن هم‌زمانی*.

- **چندتا رویداد:** می‌تونیم استقلال رو برای بیشتر از دوتا رویداد هم تعریف کنیم. رویدادهای $A_1, A_2, \ldots, A_n$ وقتی **به‌طور متقابل مستقل** هستن که برای هر زیرمجموعه‌ای از اونا مثل $
{ i_1, i_2, \dotsc, i_k \} \subseteq \{ 1, 2, \dotsc, n \}$ این شرط برقرار باشه:
$$
P(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_k}) = P(A_{i_1}) P(A_{i_2}) \cdots P(A_{i_k}).
$$
**استقلال دوتایی** یعنی فقط هر دوتا با هم مستقل باشن:
$$
P(A_i \cap A_j) = P(A_i) P(A_j) \quad \text{برای همه‌ی } i \neq j.
$$
استقلال متقابل قوی‌تر از استقلال دوتاییه.

#### مثال: موفقیت در تسک‌های مستقل

فرض کن یه ایجنت دوتا تسک جدا از هم رو امتحان می‌کنه و احتمال موفقیت توی هر کدوم $0.8$ باشه. اگه موفقیت توی یکی هیچ تاثیری روی اون یکی نداشته باشه، احتمال موفقیت توی هر دوتا می‌شه:
$$
P(\text{Success of both}) = 0.8 \times 0.8 = 0.64.
$$
این ضرب ساده فقط به خاطر فرض استقلال درسته.

#### استقلال متغیرهای تصادفی

مفهوم استقلال خیلی راحت از رویدادها به متغیرهای تصادفی گسترش پیدا می‌کنه. دوتا متغیر تصادفی $X$ و $Y$ مستقل هستن اگه به‌زبون ساده، دونستن مقدار یکی هیچ چیز جدیدی درباره اون یکی نگه.

تعریفی بخوایم بگیم، $X$ و $Y$ مستقل هستن اگه برای هر عدد حقیقی $x$ و $y$،
$$
P(X \leq x, Y \leq y) = P(X \leq x) P(Y \leq y).
$$
این تعریف تضمین می‌کنه که هر جمله‌ی احتمالاتی درباره $X$ و $Y$ رو می‌تونیم به دوتا جمله جدا تجزیه کنیم.

برای متغیرهای گسسته با تابع جرم احتمال (PMF)، $X$ و $Y$ مستقل هستن اگه:
$$
P(X = x, Y = y) = P(X = x) P(Y = y) \quad \text{برای همه‌ی } x, y.
$$

برای متغیرهای پیوسته با چگالی احتمال مشترک $f_{X,Y}(x,y)$ و چگالی‌های حاشیه‌ای $f_X(x)$، $f_Y(y)$، استقلال یعنی:
$$
f_{X,Y}(x,y) = f_X(x) f_Y(y) \quad \text{برای همه‌ی } x, y.
$$

خلاصه اینکه، فقط وقتی می‌تونیم چگالی مشترک رو به ضرب چگالی‌های حاشیه‌ای تجزیه کنیم که متغیرها مستقل باشن.

#### توابعی از متغیرهای مستقل

اگه $X$ و $Y$ مستقل باشن، اون‌وقت هر تابع قابل اندازه‌گیری از $X$ و $Y$ مثل $g(X)$ و $h(Y)$ هم مستقل از هم هستن. این ویژگی توی مدل‌سازی‌های پیشرفته خیلی به کار میاد.

### استقلال شرطی

استقلال شرطی، مفهوم استقلال رو با در نظر گرفتن یه **شرط** یا یه متغیر اضافی تعریف می‌کنه. استقلال شرطی یعنی ممکنه دوتا چیز وابسته باشن، ولی وقتی یه چیز سومی رو بدونیم، دیگه مستقل شن.

#### تعریف

دوتا رویداد $A$ و $B$ نسبت به یه رویداد سوم $E$ **شرطی مستقل** هستن اگه:
$$
P(A \cap B \mid E) = P(A \mid E) P(B \mid E).
$$
یعنی وقتی $E$ رو بدونیم، احتمال همزمان افتادن $A$ و $B$ می‌شه ضرب احتمال شرطی هرکدوم‌شون. این یعنی با دونستن $E$، دیگه دونستن $A$ چیزی به‌مون درباره $B$ نمی‌گه.

اگه $P(E) > 0$ و $P(A \cap E) > 0$، می‌تونیم اینو این‌جوری هم بگیم:
$$
P(A \mid B, E) = P(A \mid E),
\quad
P(B \mid A, E) = P(B \mid E).
$$

این مفهوم رو می‌تونیم برای متغیرهای تصادفی هم تعریف کنیم: متغیرهای $X$ و $Y$ شرطی مستقل نسبت به $Z$ هستن اگه برای همه مقادیر $x, y, z$ که $P(Z=z) > 0$ باشه:
$$
P(X = x, Y = y \mid Z = z) = P(X = x \mid Z = z) P(Y = y \mid Z = z).
$$


#### شهود و اشتباهات رایج

شهود قضیه اینه که شرطی کردن روی یه متغیر سوم می‌تونه وابستگی بین دوتا متغیر رو "فیلتر" کنه. یعنی ممکنه قبل از شرطی کردن $X$ و $Y$ وابسته باشن، ولی بعد از دونستن $Z$ مستقل شن.

**مثال:** فرض کن $A$ "امروز بارون میاد"، $B$ "هوا ابریه"، و $E$ "دیروز بارون اومده بود" باشه. معمولاً بارون و ابری بودن به هم ربط دارن، ولی اگه بارون و ابری بودن امروز فقط به آب‌وهوای دیروز بستگی داشته باشه، اون‌وقت $A$ و $B$ می‌تونن شرطی مستقل نسبت به $E$ باشن.

اشتباهای رایج:
- استقلال معمولاً به معنی استقلال شرطی نیست و برعکسش هم درسته.
- شرطی کردن می‌تونه وابستگی درست کنه: ممکنه دوتا متغیر که مستقل بودن، بعد از شرطی شدن روی یه متغیر سوم، وابسته بشن. این پدیده توی مدل‌های گرافی به اسم **"explaining away"** معروفه.

## استقلال و استقلال شرطی در محیط‌های MARL

#### استقلال در سیستم‌های چندایجنتی

توی یادگیری تقویتی چندایجنتی (MARL)، استقلال زمانی هست که ایجنت‌ها توی محیط‌های جدا از هم و بدون تعامل با هم کار کنن. مثلاً اگه ایجنت 1 و ایجنت 2 دوتا معمای کاملاً جدا رو بدون هیچ تعامل حل کنن، مسیرهای $
{S_t^1, A_t^1, R_t^1\}$ و $
{S_t^2, A_t^2, R_t^2\}$ مستقل هستن.

تعریفی بخوایم بگیم، برای دنباله‌های state-action-reward ایجنت $i$، استقلال یعنی:
$$
P\left( (S^1, A^1, R^1), (S^2, A^2, R^2) \right) = P(S^1, A^1, R^1) P(S^2, A^2, R^2).
$$

این باعث می‌شه بشه مسئله چندایجنتی رو به چندتا مسئله ساده‌تر تک‌ایجنتی تقسیم کرد.

ولی خب بیشتر محیط‌های واقعی شامل تعامل هستن — چه مستقیم چه غیرمستقیم — و استقلال کامل خیلی کم پیدا می‌شه.

#### partially observability

توی محیط‌هایی که مشاهده ناقصه (partially observable)، ممکنه ایجنت‌های مختلف مشاهداتی مثل $O^1$، $O^2$ بگیرن که از حالت واقعی $S$ مشتق شدن.

اگه:
$$
P(O^1, O^2 \mid S) = P(O^1 \mid S) P(O^2 \mid S),
$$
اون‌وقت $O^1$ و $O^2$ شرطی مستقل نسبت به $S$ هستن.

این فرضیه توی مدل‌های **decentralized POMDPs** (Dec-POMDPs) رایجه که توش سنسورهای مستقل دارن حالت کلی رو به‌صورت نویزی می‌بینن. این فرض کمک می‌کنه policyهایی طراحی کنیم که فقط به مشاهده‌های محلی وابسته باشن.


