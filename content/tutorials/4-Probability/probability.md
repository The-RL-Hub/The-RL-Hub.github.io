
# چیزای پایه‌ای از تئوری احتمال برای RL

*Reinforcement learning (RL)* بر پایه‌ی مفاهیم پایه‌ای تئوری احتمال بنا شده. توی این فصل می‌خوایم یه مروری روی ایده‌های مهم تئوری احتمال داشته باشیم تا یه پایه‌ی محکم و درست‌حسابی برای RL بسازیم. فرض می‌کنیم که خواننده یه آشنایی اولیه با تئوری احتمال داره، و تمرکزمون روی تعریفای دقیق، قضیه‌ها و توضیحاتیه که بیشتر سمت ریاضی‌ان تا بیس رو بسازیم تا جایی که نیازه.

توی این بخش‌ها قراره راجع به متغیرهای تصادفی و توزیع‌ها، امید ریاضی و واریانس، *conditional probability* و قانون بیز، مستقل بودن، توزیع‌های مهم، و Limit Theoremها (*LLN* و *CLT*) حرف بزنیم. همه‌ی اینا رو هم هر جا که به RL مربوط باشه، توی اون *context* بررسی می‌کنیم.


## متغیرهای تصادفی و توزیع‌ها

یه **متغیر تصادفی (r.v.)** در واقع یه تابع عددیه که نتیجه‌ی یه آزمایش تصادفی رو می‌گیره و به یه عدد واقعی وصلش می‌کنه. تعریفی بخوایم بگیم، متغیر تصادفی هر نتیجه‌ی تو فضای نمونه رو میاره به یه عدد حقیقی نگاشت می‌کنه. معمولاً متغیرهای تصادفی رو با حروف بزرگ (مثلاً $X,Y$) نشون میدن. راحت بخوایم بگیم، $X$ بر اساس یه توزیع احتمالی که پشت اون آزمایش خوابیده، یه سری مقدار رو «برمی‌داره».

**گسسته در مقابل پیوسته:** اگه یه متغیر تصادفی بتونه فقط یه مجموعه‌ی قابل‌شماری از مقدارهای متمایز (یا متناهی یا بینهایت شمارا) بگیره، بهش میگیم **گسسته**. تو این حالت احتمال همه‌ی این مقدارها با هم میشه ۱. حالا برعکس، یه متغیر تصادفی **پیوسته** می‌تونه هر مقداری تو یه بازه بگیره (که دیگه مقدارهاش قابل‌شمارش نیست). برای یه r.v. پیوسته، احتمال اینکه دقیقاً یه مقدار خاص رو بگیره صفره – به جاش احتمال‌ها رو روی بازه‌ها تعریف می‌کنیم. در واقع برای هر $x$ داریم:

$$
P(X=x)=0
$$

توزیع یه r.v. گسسته با یه **Probability Mass Function (PMF)** تعریف میشه، ولی توزیع یه r.v. پیوسته با یه **Probability Density Function (PDF)** مشخص میشه. هر دوتاشون یه **Cumulative Distribution Function (CDF)** هم دارن.

**Probability Mass Function (PMF):** برای یه r.v. گسسته $X$، PMF میگه احتمال گرفتن هر مقدار چقدره. طبق تعریف:

$$
p_X(x)=P(X=x)
$$

PMF برای هر مقدار ممکن غیرمنفیه و رو همه‌ی مقدارهایی که $X$ می‌تونه بگیره جمعش میشه ۱. مثلاً اگه $X$ تعداد شیرهایی باشه که تو ۲ تا پرتاب سکه درمیاد، $X$ یه متغیر گسسته‌ست با support $\{0,1,2\}$ و PMF به صورت:

$$
P(X=0)=\frac{1}{4},\quad P(X=1)=\frac{1}{2},\quad P(X=2)=\frac{1}{4}
$$

همه‌ی جرم‌های احتمال روی این مقدارها پخش شده و جای دیگه $P(X=x)=0$.

**Probability Density Function (PDF):** برای یه r.v. پیوسته $X$، PDF در واقع مشتق CDF هست. تعریفی بخوایم بگیم، اگه $F(x)$ همون CDF باشه، اون وقت:

$$
f(x)=F'(x)
$$

PDF میگه چقدر احتمال داره $X$ تو یه بازه‌ی کوچولو یه مقدار خاص رو بگیره. برخلاف PMF، خود $f(x)$ احتمال نیست – یعنی حتی می‌تونه از ۱ هم بزرگتر باشه – ولی وقتی PDF رو روی یه بازه انتگرال می‌گیریم، اون موقع یه احتمال درمیاد. برای هر بازه‌ی $[a,b]$، داریم:

$$
P(a\leq X\leq b)=\int_a^b f(x)\,dx
$$

یه PDF درست حسابی باید همیشه $f(x)\geq 0$ باشه و:

$$
\int_{-\infty}^{\infty} f(x)\,dx=1
$$

support یه r.v. پیوسته هم جاییه که $f(x)>0$. یادمون نره که برای r.v.های پیوسته همیشه $P(X=x)=0$ و باید از انتگرال PDF استفاده کنیم تا یه احتمال درست بگیریم.

**Cumulative Distribution Function (CDF):** CDF یعنی $F_X(x)$ که نشون میده احتمال اینکه متغیر تصادفی کمتر یا مساوی $x$ باشه چقدره. طبق تعریف:

$$
F_X(x)=P(X\leq x)
$$

CDF برای *همه‌ی* متغیرهای تصادفی (چه گسسته، چه پیوسته، چه مخلوط) تعریف شده. برای r.v. گسسته، CDF مثل یه نردبونه که سر هر مقدار ممکن یه پله میره بالا؛ ولی برای r.v. پیوسته، CDF صاف و نرمه و بدون پرش میره بالا. CDF همیشه غیرکاهنده‌ست، راست‌پیوسته‌ست و شرط‌های زیر رو داره:

$$
\lim_{x\to -\infty}F(x)=0,\quad \lim_{x\to \infty}F(x)=1
$$

از روی یه PMF، CDF رو با جمع کردن احتمال‌ها تا $x$ به دست میاریم، و از روی یه PDF با انتگرال گرفتن تا $x$:

$$
F(x)=\int_{-\infty}^x f(t)\,dt
$$

*مثال:* **توزیع گسسته (Binomial)** – شکل، PMF و CDF برای $X\sim \mathrm{Bin}(4,0.5)$ رو نشون میده. PMF (سمت چپ) چندتا spike داره رو عددهای صحیح ۰ تا ۴ (اینجا به خاطر ۴ بار پرتاب سکه‌ی عادلانه متقارنه). CDF (سمت راست) یه تابع پلکانیه: بین عددهای صحیح صاف میمونه و رو عددهای خاص یهویی میره بالا. مثلاً:

$$
P(X=2)\approx 0.375
$$

یعنی تو $x=2$ یه پرش به اندازه‌ی همین مقدار داریم. CDF تو $x=4$ میرسه به ۱، که یعنی:

$$
P(X\leq 4)=1
$$

همون چیزی که انتظار داریم چون ۴ بیشترین مقداریه که $X$ می‌تونه بگیره.

*مثال:* **توزیع پیوسته (Normal)** – شکل، PDF (سمت چپ) و CDF (سمت راست) برای توزیع نرمال استاندارد $Z\sim N(0,1)$ رو نشون میده. PDF همون منحنی زنگوله‌ای معروفه (اینجا با میانگین ۰ و انحراف معیار ۱) و فرمولش اینه:

$$
f(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
$$

CDF هم یه منحنی S-شکل نرمه که از ۰ به ۱ افزایش پیدا میکنه. نکته‌ی جالب اینکه CDF اینجا هیچ پرشی نداره؛ یعنی:

$$
P(Z=z)=0
$$

برای هر مقدار دقیق $z$، ولی اگه بخوایم مثلاً $P(-1<Z<1)$ رو حساب کنیم باید PDF رو روی اون بازه انتگرال بگیریم. قله‌ی PDF تو ۰ باعث میشه CDF دور و بر ۰.۵ سریع‌تر از همه جا رشد کنه (چون به خاطر تقارن $P(Z\leq 0)=0.5$ میشه). این نمودارها خیلی خوب نشون میدن که توزیع‌های گسسته و پیوسته چطوری تو انباشت احتمال‌ها (جمع در مقابل انتگرال) با هم فرق دارن.

فهمیدن PMFها، PDFها و CDFها تو RL خیلی مهمه، چون اینا عدم قطعیت تو نتایج، ریواردها، و انتقال وضعیت‌ها رو توضیح میدن. مثلاً توزیع ریوارد برای یه policy تصادفی معمولاً با یه PMF یا PDF مشخص میشه و CDF کمک میکنه که احتمال ریواردهای تجمعی یا رخدادهای آستانه‌ای رو حساب کنیم. خلاصه‌ش اینکه توزیع یه متغیر تصادفی گسسته با یه سری جرم احتمال روی نقطه‌های خاص داده میشه ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=In%20probability%20%20and%20,78%20whose%20domain%20is%20discrete))، ولی توزیع یه متغیر تصادفی پیوسته با یه منحنی چگالی داده میشه که باید انتگرالش رو گرفت ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=A%20probability%20mass%20function%20differs,3)). هر دوتا CDF رو به عنوان یه توصیف یکپارچه دارن:

$$
F_X(x)=P(X\leq x)
$$

که همیشه برقرار میمونه.











## امید ریاضی، واریانس و کوواریانس

### امید ریاضی

**امید ریاضی** یه چیزی از اون مفاهیم پایه و خیلی پرکاربرد تو احتمال و آمار به حساب میاد. یه جورایی خلاصه‌ی رفتار یه متغیر تصادفیه که نشون میده تو طولانی مدت چطوری رفتار می‌کنه. اگه بخوایم راحت‌تر بگیم، امید ریاضی مثل *مرکز جرم* توزیع یه متغیره، جایی که هر خروجی ممکن، نسبت به احتمال اتفاق افتادنش وزن داده میشه. بعضی وقتا بهش میگن **میانگین**.

ترجمه تعریفی:  
اگه یه متغیر تصادفی **گسسته** $X$ با **تابع جرم احتمال (PMF)** $p(x) = \mathbb{P}(X = x)$ داشته باشیم، امید ریاضی $X$ اینطوری تعریف میشه:

$$
\mathbb{E}[X] = \sum_x x\, p(x),
$$

به شرطی که این جمع مطلقاً همگرا باشه، یعنی $\sum_x |x|\, p(x) < \infty$. اینطوری مطمئن میشیم که امید ریاضی تعریف شده و مقدارش بی‌نهایت نمیشه.

اگه متغیرمون **پیوسته** باشه و **تابع چگالی احتمال (PDF)** داشته باشه، تعریفش اینجوریه:

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x\, f(x)\, dx,
$$

که اینجا هم باید مطمئن باشیم که انتگرال مطلقاً همگراست.

#### برداشت شهودی
چه گسسته باشه چه پیوسته، داستان یکیه: باید هر مقدار ممکن رو در احتمال (یا چگالیش) ضرب کنیم و بعد همه رو با هم جمع بزنیم (یا انتگرال بگیریم). خلاصه اینکه یه میانگین وزندار درست می‌کنیم که چیزایی که احتمال بیشتری دارن تاثیر بیشتری رو نتیجه نهایی دارن.

#### نوتیشن
برای نشون دادن امید ریاضی معمولا اینطوری می‌نویسن:
- $\mathbb{E}[X]$ 
- یا با حرف یونانی $\mu$ (وقتی میخوایم تاکید کنیم که میانگین یه توزیعه).

یادت باشه که امید ریاضی یه *عدد قطعی*ـه — خودش دیگه تصادفی نیست، فقط خلاصه‌ای از یه چیز تصادفیه.


#### ویژگی‌های اصلی امید ریاضی

امید ریاضی چندتا ویژگی خفن داره که باعث میشه تو کارهای نظری و کاربردی خیلی دستمون رو باز کنه. دوتا از مهم‌تریناش **امید ریاضی یه مقدار ثابت** و **خطی بودن امید ریاضی** هستن.

#### ویژگی ۱: امید ریاضی یه مقدار ثابت

اگه $c$ یه عدد ثابت باشه (یعنی تصادفی نباشه)، اونوقت:

$$
\mathbb{E}[c] = c.
$$

**اثبات:**

چون $c$ تصادفی نیست، همیشه همون $c$ رو میگیره با احتمال ۱. پس چه تو جمع چه تو انتگرال، داریم فقط $c$ رو با وزن ۱ حساب می‌کنیم:

$$
\mathbb{E}[c] = \sum_x c \cdot p(x) = c \sum_x p(x) = c \cdot 1 = c,
$$

یا اگه پیوسته باشه:

$$
\mathbb{E}[c] = \int_{-\infty}^{\infty} c \cdot f(x)\, dx = c \int_{-\infty}^{\infty} f(x)\, dx = c \cdot 1 = c,
$$

چون مجموع کل احتمال یا چگالی باید ۱ باشه.


#### ویژگی ۲: خطی بودن امید ریاضی

**بیان:**  
اگه دوتا متغیر تصادفی $X$ و $Y$ داشته باشیم و یه عدد ثابت $a$ از دنیای $\mathbb{R}$، اونوقت:

$$
\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y],
$$

$$
\mathbb{E}[aX] = a\, \mathbb{E}[X].
$$

**نکته‌ی خیلی مهم:**  
این خاصیت بدون اینکه نیازی به استقلال $X$ و $Y$ داشته باشیم، همیشه برقرار میشه. یعنی متغیرها هرچقدر هم بهم چسبیده باشن، بازم خطیته سر جاشه.


#### اثبات خطی بودن امید ریاضی

بیایید تو حالت گسسته اثبات کنیم (تو حالت پیوسته همین روال با انتگرال پیش میره).

فرض کن $X$ و $Y$ گسسته باشن. اونوقت طبق تعریف:

$$
\mathbb{E}[X+Y] = \sum_{x,y} (x+y)\, \mathbb{P}(X=x, Y=y).
$$

اگه باز کنیمش:

$$
= \sum_{x,y} x\, \mathbb{P}(X=x, Y=y) + \sum_{x,y} y\, \mathbb{P}(X=x, Y=y).
$$

میشه اینطوری مرتبش کرد:

$$
= \sum_{x} x \left( \sum_y \mathbb{P}(X=x, Y=y) \right) + \sum_{y} y \left( \sum_x \mathbb{P}(X=x, Y=y) \right).
$$

اینجا باید بدونی که:

$$
\sum_y \mathbb{P}(X=x, Y=y) = \mathbb{P}(X=x)
$$

و

$$
\sum_x \mathbb{P}(X=x, Y=y) = \mathbb{P}(Y=y),
$$

چون داریم روی یکی حاشیه می‌گیریم.

در نتیجه:

$$
\mathbb{E}[X+Y] = \sum_x x\, \mathbb{P}(X=x) + \sum_y y\, \mathbb{P}(Y=y) = \mathbb{E}[X] + \mathbb{E}[Y].
$$

برای ضرب در یه ثابت هم که واضحه:

$$
\mathbb{E}[aX] = \sum_x (a x)\, \mathbb{P}(X=x) = a \sum_x x\, \mathbb{P}(X=x) = a\, \mathbb{E}[X].
$$


#### برداشت شهودی

شاید اولش یه کم عجیب به نظر بیاد که بدون استقلال هم بشه $\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]$، ولی دلیلش واضحه: چون جمع و انتگرال ذاتاً عملیات‌های خطی هستن.

یعنی حتی اگه دوتا متغیر به هم وابسته باشن، باز هم میانگینشون جدا جدا حساب میشه. وابستگی روی چیزهای بالاتر مثل واریانس تاثیر میذاره، نه روی میانگین.

خلاصه اینکه، **امید ریاضی به وابستگی اهمیت نمیده**.


#### کاربرد تو Reinforcement Learning

تو **Reinforcement Learning (RL)** این خاصیت خطی بودن خیلی به کار میاد، مخصوصاً وقتی میخوایم **expected returns** و **value functions** رو حساب کنیم.

تو RL معمولاً دنبال **return** $G_t$ هستیم که اینجوری تعریف میشه:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots,
$$

که توش:
- $R_{t+k}$ ریواردیه که $k$ گام زمانی بعد از زمان $t$ گرفته میشه،
- $\gamma$ هم یه عدد بین صفر و یکه که ریواردهای آینده رو کوچیک‌تر می‌کنه تا ارزش کمترشون رو نشون بده.

هدف اینه که امید ریاضی $G_t$ یعنی:

$$
\mathbb{E}[G_t]
$$

رو حساب کنیم.

به لطف خطی بودن امید ریاضی:

$$
\mathbb{E}[G_t] = \mathbb{E}[R_{t+1}] + \gamma\, \mathbb{E}[R_{t+2}] + \gamma^2\, \mathbb{E}[R_{t+3}] + \gamma^3\, \mathbb{E}[R_{t+4}] + \cdots
$$


و این بدون اینکه نیاز باشه بین ریواردهای مختلف استقلال فرض کنیم.

این خاصیت تعریف **state-value function** تحت یه policy $\pi$ رو راحت می‌کنه:

$$
v_\pi(s) = \mathbb{E}_\pi[ G_t \mid S_t = s ],
$$
یعنی: *اگه ایجنت از state $s$ شروع کنه و از policy $\pi$ پیروی کنه، انتظار چه returnـی داره*.

با باز کردن $G_t$:

$$
v_\pi(s) = \mathbb{E}_\pi[ R_{t+1} + \gamma G_{t+1} \mid S_t = s ].
$$

و با استفاده از خطی بودن:


$$
= \mathbb{E}_\pi[ R_{t+1} \mid S_t=s ] + \gamma\, \mathbb{E}_\pi[ G_{t+1} \mid S_t=s ].
$$


بعدش هم میتونیم این $\mathbb{E}_\pi[ G_{t+1} \mid S_t=s ]$ رو روی همه stateهای بعدی $s'$ باز کنیم:

$$
\mathbb{E}_\pi[ G_{t+1} \mid S_t=s ] = \sum_{s'} \mathbb{P}(S_{t+1}=s' \mid S_t=s, \pi) \, v_\pi(s').
$$

در نهایت میرسیم به **معادله‌ی بلمن برای امید ریاضی**:

$$
v_\pi(s) = \sum_{s'} \mathbb{P}(s' \mid s, \pi)\, \left( \mathbb{E}[R_{t+1} \mid S_t=s, S_{t+1}=s'] + \gamma v_\pi(s') \right).
$$

اگه ریواردها قطعی باشن، ساده‌تر هم میشه:

$$
v_\pi(s) = \sum_{s'} P(s' \mid s, \pi) \left( R(s, \pi, s') + \gamma v_\pi(s') \right).
$$

این ساختار بازگشتی پایه‌ی روش‌های dynamic programming مثل **policy evaluation**، **value iteration** و **policy iteration** تو RL هست.











## احتمال شرطی و قانون بیز

توی خیلی از سناریوهایی که پای عدم قطعیت وسطه، بیشتر از اینکه بخوایم بدونیم احتمال یه اتفاق چقدره، دلمون میخواد بدونیم احتمال اون اتفاق *با توجه به* یه اطلاعاتی که دیدیم چقدره. **احتمال شرطی** همون مفهوم ریاضیه که این ایده رو روال میکنه: اینکه وقتی یه مدرک جدید داریم، چجوری باورمون نسبت به یه اتفاق تغییر میکنه.

### احتمال شرطی

**احتمال شرطی** اینکه یه اتفاق $A$ بیفته وقتی بدونیم یه اتفاق دیگه $B$ افتاده، اینجوری تعریف میشه:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$

البته به شرطی که $P(B) > 0$ باشه.

اینجا:
- $P(A \mid B)$ یعنی "احتمال $A$ با توجه به $B$".
- $P(A \cap B)$ احتمال اینه که جفتشون با هم اتفاق بیفتن.
- $P(B)$ هم احتمال افتادن $B$ ـه.

حس پشت این تعریف خیلی واضحه: وقتی بدونیم $B$ اتفاق افتاده، دیگه کل فضای نمونه رو در نظر نمیگیریم، فقط حالتایی رو نگاه میکنیم که تو $B$ هستن. بین این حالت‌ها، دنبال اونایی میگردیم که تو $A \cap B$ هم باشن. این فرمول دقیقاً داره همین نسبت رو نشون میده:

$$
\text{احتمال شرطی} = \frac{\text{احتمال وقوع هر دو تا اتفاق}}{\text{احتمال اون مدرک معلوم}}
$$

**چندتا خاصیت مهم** احتمال شرطی:
- **نرمالیزیشن:** برای هر رویداد ثابت $B$ که $P(B) > 0$،
  
$$
P(B \mid B) = 1
$$

  چون وقتی میدونیم $B$ افتاده، دیگه قطعی‌ـه که $B$ تو context شرطی شده اتفاق افتاده.
- **مثل یه احتمال ساده حد داره بدیهتاً:** برای هر $A$ و $B$، همیشه داریم $0 \leq P(A \mid B) \leq 1$.

#### مثال (Policy تو RL)

فرض کن یه ایجنت طبق یه policy $\pi$ تو RL داره کار میکنه. وقتی state فعلی $S = s$ باشه، احتمال اینکه ایجنت اکشن $A = a$ رو انتخاب کنه اینطوریه:

$$
P(A = a \mid S = s) = \pi(a \mid s)
$$

اینجا $S = s$ همون شرطیه که داریم، و policy $\pi$ میگه روی این state، اکشن‌ها چه احتمالایی دارن.

پس احتمال شرطی یه پایه مهمه برای اینکه بفهمیم رفتار یه ایجنت تو محیط تصادفی چطوریه.

---

#### بدست آوردن قانون بیز

قانون بیز یکی از پایه‌ای‌ترین نتایج تو نظریه احتماله. این قانون یه روش بهمون میده که بتونیم احتمال‌های شرطی رو "برعکس" کنیم، مخصوصاً وقتی میخوایم دانسته هامون رو بعد از دیدن اطلاعات جدید آپدیت کنیم خیلی مهم میشه.

با شروع از تعریف احتمال شرطی:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B \mid A) = \frac{P(A \cap B)}{P(A)}
$$

میتونیم ببینیم که:

$$
P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)
$$

این دوتا عبارت برای $P(A \cap B)$ رو مساوی میذاریم:

$$
P(A \mid B)P(B) = P(B \mid A)P(A)
$$

حالا دو طرفو بر $P(B)$ تقسیم میکنیم (البته $P(B) > 0$ باشه):

$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$

#### توضیح نوتیشن

- $P(A)$ میشه **prior**: مقداری که قبل از دیدن $B$ داشتیم.
- $P(B \mid A)$ میشه **likelihood**: احتمال اینکه $B$ اتفاق بیفته اگر $A$ درست باشه.
- $P(B)$ همون **marginal** احتماله: احتمال کلی دیدن $B$.
- $P(A \mid B)$ میشه **posterior**: مقداری که بعد از دیدن $B$ داریم.

#### حساب کردن $P(B)$

برای حساب کردن $P(B)$، معمولاً از **قانون احتمال کل** استفاده میکنیم. مثلا اگه $\{A, A^c\}$ یه تقسیم فضای نمونه باشن (یعنی یا $A$ یا مکملش $A^c$ اتفاق میفته)، اونوقت داریم:

$$
P(B) = P(B \mid A)P(A) + P(B \mid A^c)P(A^c)
$$

یا تو حالت کلی‌تر که $\{A_1, A_2, \ldots, A_n\}$ یه تقسیم از فضای نمونه باشه:

$$
P(B) = \sum_{i=1}^{n} P(B \mid A_i)P(A_i)
$$



### امید ریاضی شرطی

همونطوری که میتونیم احتمال یه رویداد رو شرطی کنیم، میتونیم **امید ریاضی** یه متغیر تصادفی رو هم شرطی کنیم. اینجوری میرسیم به مفهوم **امید ریاضی شرطی**.

تعریفی، اگه $X$ و $Y$ دوتا متغیر تصادفی باشن، امید ریاضی شرطی $X$ وقتی بدونیم $Y=y$ اینطوری نوشته میشه:

$$
E[X \mid Y=y]
$$

که یعنی مقدار مورد انتظار $X$ وقتی که $Y$ رو برابر با $y$ بدونیم.

خود امید ریاضی شرطی هم میتونه یه **متغیر تصادفی** باشه:

$$
E[X \mid Y]
$$

که هر $y$ رو به $E[X \mid Y=y]$ وصل میکنه.

**خواصش:**
- **خطی بودن:**

$$
E[aX + bY \mid Z] = aE[X \mid Z] + bE[Y \mid Z]
$$

- **Law of total expectation (راستش فارسیشو نمیدونم :)):**

$$
E[X] = E\left[E[X \mid Y]\right]
$$

یعنی برای حساب امید کلی $X$، اول امید شرطی نسبت به $Y$ رو حساب میکنی، بعد روی توزیع $Y$ میانگین میگیری.

#### شهود

شرطی کردن روی $Y$، عدم قطعیت $X$ رو به دو بخش میشکونه: یکی عدم قطعیت داخل $Y$، یکی هم uncertainty خود $Y$.

#### مثال (State-value function تو RL)

توی RL، تابع مقدار برای یه state $s$ اینجوری نوشته میشه:

$$
v(s) = E[G_t \mid S_t = s]
$$

که $G_t$ مجموع ریواردهای آینده‌ست. یعنی امید ریاضی مجموع ریواردهایی که از state $s$ شروع میشه.

بعضی وقتا، $v(s)$ رو بیشتر میشکونیم و شرطی می‌کنیم روی چیزای دیگه مثل state بعدی $S_{t+1}$، و قانون امید کلی رو بازگشتی اعمال می‌کنیم.


یه جای دیگه هم هست: تو credit assignment تو محیط‌های partially observable، قانون بیز میتونه کمک کنه بفهمیم که یه نتیجه، احتمالاً به خاطر کدوم عامل مخفی بوده.  
در کل، احتمال‌های شرطی تو خود تعریف محیط RL هم وجود دارن: مثلاً محیط با $P(s', r \mid s, a)$ تعریف میشه — یعنی احتمال اینکه از state $s$ با اکشن $a$ به state $s'$ با ریوارد $r$ برسیم.  
این یه توزیع احتمال شرطی از دینامیک محیطه.  
ایجنتی که با $P(s'|s,a)$ یا $P(r|s,a)$ کار می‌کنه، داره عملاً با احتمال شرطی برنامه‌ریزی یا تصمیم‌گیری می‌کنه.
