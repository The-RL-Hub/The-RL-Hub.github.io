
# چیزای پایه‌ای از تئوری احتمال برای RL

*Reinforcement learning (RL)* بر پایه‌ی مفاهیم پایه‌ای تئوری احتمال بنا شده. توی این فصل می‌خوایم یه مروری روی ایده‌های مهم تئوری احتمال داشته باشیم تا یه پایه‌ی محکم و درست‌حسابی برای RL بسازیم. فرض می‌کنیم که خواننده یه آشنایی اولیه با تئوری احتمال داره، و تمرکزمون روی تعریفای دقیق، قضیه‌ها و توضیحاتیه که بیشتر سمت ریاضی‌ان تا بیس رو بسازیم تا جایی که نیازه.

توی این بخش‌ها قراره راجع به متغیرهای تصادفی و توزیع‌ها، امید ریاضی و واریانس، *conditional probability* و قانون بیز، مستقل بودن، توزیع‌های مهم، و Limit Theoremها (*LLN* و *CLT*) حرف بزنیم. همه‌ی اینا رو هم هر جا که به RL مربوط باشه، توی اون *context* بررسی می‌کنیم.


## متغیرهای تصادفی و توزیع‌ها

یه **متغیر تصادفی (r.v.)** در واقع یه تابع عددیه که نتیجه‌ی یه آزمایش تصادفی رو می‌گیره و به یه عدد واقعی وصلش می‌کنه. تعریفی بخوایم بگیم، متغیر تصادفی هر نتیجه‌ی تو فضای نمونه رو میاره به یه عدد حقیقی نگاشت می‌کنه. معمولاً متغیرهای تصادفی رو با حروف بزرگ (مثلاً $X,Y$) نشون میدن. راحت بخوایم بگیم، $X$ بر اساس یه توزیع احتمالی که پشت اون آزمایش خوابیده، یه سری مقدار رو «برمی‌داره».

**گسسته در مقابل پیوسته:** اگه یه متغیر تصادفی بتونه فقط یه مجموعه‌ی قابل‌شماری از مقدارهای متمایز (یا متناهی یا بینهایت شمارا) بگیره، بهش میگیم **گسسته**. تو این حالت احتمال همه‌ی این مقدارها با هم میشه ۱. حالا برعکس، یه متغیر تصادفی **پیوسته** می‌تونه هر مقداری تو یه بازه بگیره (که دیگه مقدارهاش قابل‌شمارش نیست). برای یه r.v. پیوسته، احتمال اینکه دقیقاً یه مقدار خاص رو بگیره صفره – به جاش احتمال‌ها رو روی بازه‌ها تعریف می‌کنیم. در واقع برای هر $x$ داریم:

$$
P(X=x)=0
$$

توزیع یه r.v. گسسته با یه **Probability Mass Function (PMF)** تعریف میشه، ولی توزیع یه r.v. پیوسته با یه **Probability Density Function (PDF)** مشخص میشه. هر دوتاشون یه **Cumulative Distribution Function (CDF)** هم دارن.

**Probability Mass Function (PMF):** برای یه r.v. گسسته $X$، PMF میگه احتمال گرفتن هر مقدار چقدره. طبق تعریف:

$$
p_X(x)=P(X=x)
$$

PMF برای هر مقدار ممکن غیرمنفیه و رو همه‌ی مقدارهایی که $X$ می‌تونه بگیره جمعش میشه ۱. مثلاً اگه $X$ تعداد شیرهایی باشه که تو ۲ تا پرتاب سکه درمیاد، $X$ یه متغیر گسسته‌ست با support $\{0,1,2\}$ و PMF به صورت:

$$
P(X=0)=\frac{1}{4},\quad P(X=1)=\frac{1}{2},\quad P(X=2)=\frac{1}{4}
$$

همه‌ی جرم‌های احتمال روی این مقدارها پخش شده و جای دیگه $P(X=x)=0$.

**Probability Density Function (PDF):** برای یه r.v. پیوسته $X$، PDF در واقع مشتق CDF هست. تعریفی بخوایم بگیم، اگه $F(x)$ همون CDF باشه، اون وقت:

$$
f(x)=F'(x)
$$

PDF میگه چقدر احتمال داره $X$ تو یه بازه‌ی کوچولو یه مقدار خاص رو بگیره. برخلاف PMF، خود $f(x)$ احتمال نیست – یعنی حتی می‌تونه از ۱ هم بزرگتر باشه – ولی وقتی PDF رو روی یه بازه انتگرال می‌گیریم، اون موقع یه احتمال درمیاد. برای هر بازه‌ی $[a,b]$، داریم:

$$
P(a\leq X\leq b)=\int_a^b f(x)\,dx
$$

یه PDF درست حسابی باید همیشه $f(x)\geq 0$ باشه و:

$$
\int_{-\infty}^{\infty} f(x)\,dx=1
$$

support یه r.v. پیوسته هم جاییه که $f(x)>0$. یادمون نره که برای r.v.های پیوسته همیشه $P(X=x)=0$ و باید از انتگرال PDF استفاده کنیم تا یه احتمال درست بگیریم.

**Cumulative Distribution Function (CDF):** CDF یعنی $F_X(x)$ که نشون میده احتمال اینکه متغیر تصادفی کمتر یا مساوی $x$ باشه چقدره. طبق تعریف:

$$
F_X(x)=P(X\leq x)
$$

CDF برای *همه‌ی* متغیرهای تصادفی (چه گسسته، چه پیوسته، چه مخلوط) تعریف شده. برای r.v. گسسته، CDF مثل یه نردبونه که سر هر مقدار ممکن یه پله میره بالا؛ ولی برای r.v. پیوسته، CDF صاف و نرمه و بدون پرش میره بالا. CDF همیشه غیرکاهنده‌ست، راست‌پیوسته‌ست و شرط‌های زیر رو داره:

$$
\lim_{x\to -\infty}F(x)=0,\quad \lim_{x\to \infty}F(x)=1
$$

از روی یه PMF، CDF رو با جمع کردن احتمال‌ها تا $x$ به دست میاریم، و از روی یه PDF با انتگرال گرفتن تا $x$:

$$
F(x)=\int_{-\infty}^x f(t)\,dt
$$

*مثال:* **توزیع گسسته (Binomial)** – شکل، PMF و CDF برای $X\sim \mathrm{Bin}(4,0.5)$ رو نشون میده. PMF (سمت چپ) چندتا spike داره رو عددهای صحیح ۰ تا ۴ (اینجا به خاطر ۴ بار پرتاب سکه‌ی عادلانه متقارنه). CDF (سمت راست) یه تابع پلکانیه: بین عددهای صحیح صاف میمونه و رو عددهای خاص یهویی میره بالا. مثلاً:

$$
P(X=2)\approx 0.375
$$

یعنی تو $x=2$ یه پرش به اندازه‌ی همین مقدار داریم. CDF تو $x=4$ میرسه به ۱، که یعنی:

$$
P(X\leq 4)=1
$$

همون چیزی که انتظار داریم چون ۴ بیشترین مقداریه که $X$ می‌تونه بگیره.

*مثال:* **توزیع پیوسته (Normal)** – شکل، PDF (سمت چپ) و CDF (سمت راست) برای توزیع نرمال استاندارد $Z\sim N(0,1)$ رو نشون میده. PDF همون منحنی زنگوله‌ای معروفه (اینجا با میانگین ۰ و انحراف معیار ۱) و فرمولش اینه:

$$
f(z)=\frac{1}{\sqrt{2\pi}}e^{-z^2/2}
$$

CDF هم یه منحنی S-شکل نرمه که از ۰ به ۱ افزایش پیدا میکنه. نکته‌ی جالب اینکه CDF اینجا هیچ پرشی نداره؛ یعنی:

$$
P(Z=z)=0
$$

برای هر مقدار دقیق $z$، ولی اگه بخوایم مثلاً $P(-1<Z<1)$ رو حساب کنیم باید PDF رو روی اون بازه انتگرال بگیریم. قله‌ی PDF تو ۰ باعث میشه CDF دور و بر ۰.۵ سریع‌تر از همه جا رشد کنه (چون به خاطر تقارن $P(Z\leq 0)=0.5$ میشه). این نمودارها خیلی خوب نشون میدن که توزیع‌های گسسته و پیوسته چطوری تو انباشت احتمال‌ها (جمع در مقابل انتگرال) با هم فرق دارن.

فهمیدن PMFها، PDFها و CDFها تو RL خیلی مهمه، چون اینا عدم قطعیت تو نتایج، ریواردها، و انتقال وضعیت‌ها رو توضیح میدن. مثلاً توزیع ریوارد برای یه policy تصادفی معمولاً با یه PMF یا PDF مشخص میشه و CDF کمک میکنه که احتمال ریواردهای تجمعی یا رخدادهای آستانه‌ای رو حساب کنیم. خلاصه‌ش اینکه توزیع یه متغیر تصادفی گسسته با یه سری جرم احتمال روی نقطه‌های خاص داده میشه ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=In%20probability%20%20and%20,78%20whose%20domain%20is%20discrete))، ولی توزیع یه متغیر تصادفی پیوسته با یه منحنی چگالی داده میشه که باید انتگرالش رو گرفت ([Probability mass function - Wikipedia](https://en.wikipedia.org/wiki/Probability_mass_function#:~:text=A%20probability%20mass%20function%20differs,3)). هر دوتا CDF رو به عنوان یه توصیف یکپارچه دارن:

$$
F_X(x)=P(X\leq x)
$$

که همیشه برقرار میمونه.











## امید ریاضی (Expectation)، واریانس (Variance)، و کوواریانس (Covariance)

Expectation (میانگین): *مقدار مورد انتظار* یه متغیر تصادفی خلاصه‌ی خیلی پایه‌ای از توزیعشه و معمولاً مثل یه میانگین بلندمدت یا همون "مرکز جرم" برداشت میشه. واسه یه متغیر تصادفی گسسته $X$ با PMF برابر با $p(x)$، Expectation اینجوری تعریف میشه:  
$$
E[X] = \sum_x x\,p(x)
$$
(البته به شرطی که این جمع درست همگرا بشه). حالا اگه متغیرمون پیوسته باشه و PDFش $f(x)$ باشه، اون وقت داریم:  
$$
E[X] = \int_{-\infty}^{\infty} x\,f(x)\,dx
$$
اگه بخوایم ساده بگیم، هر مقدار ممکن رو در احتمالش ضرب می‌کنیم و بعد رو همه‌ی مقادیر جمع یا انتگرال می‌گیریم. Expectation رو گاهی میانگین یا ممان اول توزیع هم صدا می‌کنن و معمولاً با $\mu$ یا $\mathbb{E}[X]$ نشونش میدن.  
چندتا ویژگی مهم Expectation هم اینان:  
(1) $E[c] = c$ واسه هر عدد ثابت $c$،  
(2) خطی بودن Expectation – یعنی برای هر متغیر تصادفی $X$ و $Y$ و عدد ثابت $a$، داریم $E[X+Y] = E[X] + E[Y]$ و $E[aX] = a\,E[X]$.  
خوبیش اینه که این خطی بودن *هیچ نیازی به مستقل بودن متغیرها نداره* و همیشه کار می‌کنه، واسه همین خیلی چیز خفنیه.

تعریفی (اثبات خطی بودن Expectation): فرض کن $X$ و $Y$ متغیرهای تصادفی باشن. طبق تعریف:  
$$
E[X+Y] = \sum_{x,y}(x+y)P(X=x, Y=y)
$$
تو حالت گسسته (تو حالت پیوسته هم یه انتگرال دوگانه‌ی مشابه داریم).  
اینو میشه اینجوری شکست:  
$$
\sum_{x,y}xP(X=x,Y=y) + \sum_{x,y}yP(X=x,Y=y)
$$
این خودش برابر میشه با  
$$
\sum_x x P(X=x)\sum_y P(Y=y|X=x) + \sum_y y P(Y=y)\sum_x P(X=x|Y=y)
$$
هر کدوم از این دوتا جمع‌ها، با ساده‌سازی حاشیه‌ای، میشه $E[X] + E[Y]$. پس $E[X+Y]=E[X]+E[Y]$.  
همینطوری هم راحت میشه نشون داد که $E[cX] = c E[X]$.  
این نتیجه کاملاً کلیه و به شکل توزیع اصلاً بستگی نداره.  
خلاصه اینکه، *مقدار مورد انتظار مجموع، مساوی مجموع مقادیر مورد انتظاره*.  
هیچ جمله‌ی تعاملی اضافه‌ای ظاهر نمیشه – حتی اگه $X$ و $Y$ به هم وابسته باشن، باز هم Expectationهاشون جمع میشه. (شاید اولش برات عجیب باشه، ولی باید بدونی که Expectation یه جور خاصیت جمع و انتگرال داره.)

کاربردش تو RL: این خطی بودن Expectation باعث میشه راحت‌تر مقدار بازده مورد انتظار و مقدارهای دیگه رو حساب کنیم. مثلاً اگه داشته باشیم:  
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots
$$
که همون *return* تصادفیه (مجموع ریواردهای تخفیف‌خورده از زمان $t$)، اون وقت با خطی بودن داریم:  
$$
E[G_t] = E[R_{t+1}] + \gamma E[R_{t+2}] + \gamma^2 E[R_{t+3}] + \cdots
$$
یعنی میشه *value function* رو برحسب ریواردهای مورد انتظار نوشت.  
درواقع، تابع ارزش حالت تحت policy $\pi$ به این صورت تعریف میشه:  
$$
v_\pi(s) = \mathbb{E}_\pi[ G_t \mid S_t=s ]
$$
حالا اگه $G_t$ رو باز کنیم، داریم:  
$$
v_\pi(s) = E[R_{t+1} + \gamma G_{t+1} \mid S_t=s] = E[R_{t+1}\mid S_t=s] + \gamma E[G_{t+1}\mid S_t=s]
$$
بعدش $E[G_{t+1}\mid S_t=s]$ هم میشه:  
$$
\sum_{s'}P(s'|s, \pi) v_\pi(s')
$$
و اینجوری مستقیماً می‌رسیم به معادله Bellman expectation:  
$$
v_\pi(s) = \sum_{s'} P(s'|s,\pi)\big( R(s,\pi,s') + \gamma\,v_\pi(s')\big)
$$
این یکی از پایه‌های reinforcement learningه.  
حتی راحت‌تر اگه بخوایم نگاه کنیم: یه ایجنت برای تخمین expected reward یه اکشن تصادفی، کافیه چند بار اجرا کنه و میانگین بگیره – به لطف خطی بودن، میانگین نمونه‌ها کم‌کم میره سمت Expectation واقعی. (این رو بعدتر تو بخش 4.6 با قانون اعداد بزرگ دقیق‌تر توضیح میدیم.)

Variance: Expectation بهمون مرکز توزیع رو میگه، ولی واریانس بهمون میگه چقدر داده‌ها پخش و پلا شدن.  
تعریفی:  
$$
\mathrm{Var}(X) = E\!\big[(X - E[X])^2\big]
$$
یعنی انتظار اختلاف مربع شده از میانگین.  
وقتی اینو باز کنیم، یه فرمول به دردبخور درمیاد:  
$$
\mathrm{Var}(X) = E[X^2] - (E[X])^2
$$
که حساب کردنش معمولاً راحت‌تره. اول $E[X^2]$ (ممان دوم) رو حساب می‌کنی، بعد مربع میانگین رو کم می‌کنی.  
واریانس همیشه نامنفی درمیاد ($\mathrm{Var}(X)\ge 0$)، و صفر میشه فقط اگه $X$ تقریباً همیشه یه مقدار ثابت باشه.  
جذر واریانس هم میشه انحراف معیار یا همون $\sigma_X = \sqrt{\mathrm{Var}(X)}$ که همون واحد خود $X$ رو داره و یه جور دیگه گستردگی رو نشون میده.

چندتا ویژگی باحال واریانس:  
- $\mathrm{Var}(X + c) = \mathrm{Var}(X)$، یعنی جابجایی تاثیری نداره؛  
- $\mathrm{Var}(cX) = c^2\,\mathrm{Var}(X)$، یعنی تغییر مقیاس به توان دوم تو واریانس اثر میزاره؛  
- اگه $X$ و $Y$ مستقل باشن، $\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)$.  
(البته اگه وابسته باشن این یکی خراب میشه چون یه جمله covariance وارد میشه.)

Covariance: کوواریانس برای دوتا متغیر تصادفی $X$ و $Y$ نشون میده که این دوتا چطوری با هم بالا پایین میرن.  
تعریفی:  
$$
\mathrm{Cov}(X,Y) = E\!\big[(X - E[X]) (Y - E[Y])\big]
$$
اگه اینو باز کنیم، داریم:  
$$
\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y]
$$
اگه $X$ و $Y$ معمولاً با هم بالا برن (یا با هم پایین بیان)، کوواریانس مثبت میشه؛ اگه یکی بالا بره و اون یکی پایین، کوواریانس منفیه.  
اگه $X$ و $Y$ مستقل باشن، اون وقت $E[XY]=E[X]E[Y]$ و در نتیجه $\mathrm{Cov}(X,Y)=0$.  
ولی دقت کن برعکسش لزوماً درست نیست – کوواریانس صفر فقط یعنی *عدم همبستگی خطی*، ولی ممکنه وابستگی‌های دیگه غیرخطی بینشون باشه.  
واحد کوواریانس حاصلضرب واحدهای $X$ و $Y$ه، برای همین تفسیر اندازش یه خرده سخته.  
معمولاً از ضریب همبستگی  
$$
\rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$
استفاده می‌کنیم که بدون بعده و بین $[-1,1]$ حرکت می‌کنه.

چندتا نکته‌ی مهم:  
- $\mathrm{Cov}(X,X) = \mathrm{Var}(X)$  
- اگه $X$ و $Y$ مستقل باشن، $\mathrm{Cov}(X,Y) = 0$

تو کانتکست RL: تو reinforcement learning، فهم variance و covariance خیلی مهمه واسه اینکه بدونیم تخمین‌هامون چقدر قابل اعتمادن یا چقدر فرآیند یادگیری پایداره.  
مثلاً return $G_t$ از یه state واریانس داره چون هم policy تصادفیه هم محیط. الگوریتم‌هایی مثل REINFORCE به خاطر همین واریانس بالا گرادیانشون نوسان داره، و مجبوریم از تکنیک‌های کاهش واریانس (مثل baselineها) استفاده کنیم.  
یه مثال دیگه: فرض کن یه policy تصادفی داریم که اکشن‌ها رو شانسی انتخاب می‌کنه. ریواردهای فوری که میگیریم متغیر تصادفی هستن. واریانس ریوارد میتونه روی سرعت یادگیری ایجنت تاثیر بزاره: اگه ریوارد خیلی نوسان داشته باشه، ایجنت باید نمونه‌های بیشتری بگیره تا مقدار مورد انتظار رو درست تخمین بزنه.  
کوواریانس هم تو بازده‌های چندمرحله‌ای دیده میشه: ریواردهای $R_t, R_{t+1}, \dots$ ممکنه تو حالت‌های پشت سر هم به خاطر دینامیک محیط با هم ارتباط داشته باشن.  
اگه محیط مارکوفی باشه، ریوارد در زمان $t$ و $t+k$ ممکنه برای $k$ کوچیک مقداری کوواریانس داشته باشه، ولی هرچی $k$ بزرگ‌تر شه، این ارتباط کمتر میشه (خاصیت mixing).  
در نهایت، وقتی داریم میانگین بازده‌ها رو حساب می‌کنیم، اگه اپیزودها مستقل باشن، واریانس میانگین میشه:  
$$
\mathrm{Var}(\bar{G}_n) = \mathrm{Var}(G)/n
$$
برای همینه که با زیاد کردن تعداد اپیزودها، تخمین مقدار policy پایدارتر میشه.  
خلاصه: Expectation پیش‌بینی اصلیه (مثل expected reward)، واریانس نشون میده چقدر عدم قطعیت دوروبر اون داریم، و کوواریانس کمک می‌کنه روابط بین کمیت‌های تصادفی تو سیستم RL رو بهتر بفهمیم (مثلاً بین ریواردهای زمان‌های مختلف یا ایجنت‌های مختلف).











## احتمال شرطی و قانون بیز

خیلی وقتا دنبال اینیم که بدونیم یه احتمال، وقتی یه چیزی رو می‌دونیم، چقدر تغییر می‌کنه. احتمال شرطی دقیقاً همینو برامون مشخص می‌کنه: احتمال رویداد $A$ وقتی که می‌دونیم $B$ اتفاق افتاده اینجوری حساب میشه:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$

البته به شرطی که $P(B)>0$ باشه. این تعریف نشون میده که چطوری وقتی یه مدرک جدید (یعنی $B$) داریم، باید احتمال‌ها رو *به‌روزرسانی* کنیم. تو احتمال شرطی $P(A|B)$، اون $B$ همون *شرط* یا مدرکه و $A$ چیزیه که میخوایم احتمال‌شو بدونیم. به $P(A)$ میگیم احتمال *prior* و به $P(A|B)$ میگیم احتمال *posterior*، یعنی همون احتمال بعد از اینکه مدرک $B$ رو در نظر گرفتیم. خلاصه‌ش اینه که فضای نمونه رو محدود می‌کنیم به $B$ و دوباره احتمال‌ها رو تنظیم می‌کنیم: $P(A|B)$ اون قسمتی از احتمال $B$ هست که توش $A$ هم اتفاق افتاده.  
مثلاً اگه بدونیم ایجنت تو وضعیت $s$ هست، احتمال اینکه اکشن $a$ رو بزنه میشه $P(A=a \mid S=s) = \pi(a|s)$ طبق policy $\pi$. یعنی احتمال انجام اکشن $a$ با دونستن وضعیت، طبق اون policy حساب میشه.

با همین تعریف احتمال شرطی، میتونیم قانون بیز رو هم دربیاریم، که یه نتیجه‌ی خیلی پایه‌ایه برای اینکه احتمال شرطی رو *برعکس* کنیم.  
قانون بیز میگه: برای دو تا رویداد $A$ و $B$ (که احتمال‌شون صفر نیست)،

$$
P(A|B) = \frac{P(B|A)\,P(A)}{P(B)}
$$

اینو ساده میشه درآورد: چون طبق تعریف، $P(A \cap B) = P(B|A)P(A)$ و از اون طرف هم $P(A\cap B)=P(A|B)P(B)$. این دوتا رو که برابر بذاریم و ساده کنیم، قانون بیز درمیاد.  
با قانون بیز میتونیم احتمال *posterior* یعنی $P(A|B)$ رو از likelihood $P(B|A)$ و prior $P(A)$ و احتمال مدرک $P(B)$ حساب کنیم.  
اون مخرج $P(B)$ رو هم میشه با قانون احتمال کل پیدا کرد: اگه $\{A, A^c\}$ یه جور تقسیم‌بندی باشه، اون وقت داریم:

$$
P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)
$$

تو عمل، قانون بیز خیلی وقتا وقتی کاربرد داره که بخوایم *باور*مون رو نسبت به یه فرضیه $A$، بعد از اینکه یه چیزی مثل مدرک $B$ دیدیم، آپدیت کنیم.  
مثلاً تو context مربوط به RL، فرض کن $A$ اینه که "state فعلی یه goal stateه" و $B$ یه مشاهده‌ایه که ایجنت کرده.  
قانون بیز به ایجنت کمک می‌کنه که احتمال اینکه واقعاً تو goal state باشه رو، با توجه به اون مشاهده، به‌روزرسانی کنه.

**امید شرطی**: همونطور که احتمال شرطی تعریف کردیم، امید شرطی هم تعریف داره.  
امید شرطی $E[X \mid Y=y]$ یعنی میانگین یا مقدار انتظاری $X$ وقتی که $Y=y$ بدونیم.  
وقتی $Y$ رو مدرک فرض کنیم، $E[X|Y]$ خودش یه متغیر تصادفی حساب میشه (چون بستگی به $Y$ داره).  
یه خاصیت مهمش اینه که طبق قانون امید کل، داریم:

$$
E[X] = E[\,E[X|Y]\,]
$$

یعنی به زبان ساده، کل امید $X$ رو میشه اینجوری حساب کرد که اول شرطی کنیم روی $Y$، بعد از اون امید بگیریم.  
اینم خیلی شبیه قانون احتمال کل هست و تو RL حسابی به درد می‌خوره. مثلاً قبلاً حساب کردیم که $v(s) = E[ G_t | S_t=s ]$ که خودش به انتظار روی وضعیت‌های بعدی شکسته شده بود.

حالا بیایم ببینیم چطوری احتمال شرطی و قانون بیز تو partial observability به کار میاد تو reinforcement learning.  
تو خیلی از محیط‌های واقعی، ایجنت خبر نداره دقیقاً تو چه stateیه؛ فقط یه سری observation از محیط می‌گیره که اطلاعات ناقصی بهمون میدن.  
چنین محیطی رو با یه Partially Observable Markov Decision Process (POMDP) مدل می‌کنن.  
ایجنت یه belief state نگه میداره — یعنی یه توزیع احتمال روی stateهای ممکن.  
حالا وقتی یه observation جدید می‌گیره، چطوری beliefشو آپدیت کنه؟ اینجاست که قانون بیز میاد وسط.

فرض کن belief (prior) ایجنت روی stateها $P(S=s)$ باشه و یه observation $o$ بگیره. اونوقت belief جدیدش میشه متناسب با حاصل‌ضرب likelihood مشاهده $o$ در state $s$ و prior قبلی:

$$
P(S=s \mid o) \propto P(o \mid S=s)\,P(S=s)
$$

با استفاده از قانون بیز دقیق‌تر داریم:

$$
P(S=s \mid o) = \frac{P(o \mid S=s)\,P(S=s)}{P(o)}
$$

که $P(o)$ هم با جمع روی همه‌ی حالت‌های ممکن حساب میشه:

$$
P(o) = \sum_{s'} P(o\mid S=s')P(S=s')
$$

اینو میگن یه گام از Bayes filter برای state مخفی.  
هر دفعه که ایجنت یه observation جدید میگیره، باید belief خودش رو با استفاده از قانون بیز اصلاح کنه.  
تو محیط‌های دینامیکی، بعد از اینکه یه اکشن زد و observation جدید گرفت، به‌روزرسانی belief شامل یه پیش‌بینی (با توجه به احتمال‌های انتقال state) و بعد اصلاح با likelihood مشاهده هست.  
آخرش میشه belief posterior ایجنت، که برای چرخه‌ی بعدی (اکشن بعدی و observation بعدی) به عنوان prior جدید استفاده میشه.

**کاربردها تو RL**: قانون بیز تو قلب الگوریتم‌هایی مثل Bayesian reinforcement learning هست که توش uncertainty نسبت به state یا مدل وجود داره.  
مثلاً تو belief state planning برای POMDPها یا فیلترهایی مثل *HMM filter* و *Kalman filter* هم قانون بیز پایه‌ی کاره.  
تو multi-armed banditها هم الگوریتم‌هایی مثل Thompson sampling دارن بعد از هر ریوارد جدید، احتمال برد هر بازو رو با قانون بیز آپدیت می‌کنن.

یه جای دیگه هم هست: تو credit assignment تو محیط‌های partially observable، قانون بیز میتونه کمک کنه بفهمیم که یه نتیجه، احتمالاً به خاطر کدوم عامل مخفی بوده.  
در کل، احتمال‌های شرطی تو خود تعریف محیط RL هم وجود دارن: مثلاً محیط با $P(s', r \mid s, a)$ تعریف میشه — یعنی احتمال اینکه از state $s$ با اکشن $a$ به state $s'$ با ریوارد $r$ برسیم.  
این یه توزیع احتمال شرطی از دینامیک محیطه.  
ایجنتی که با $P(s'|s,a)$ یا $P(r|s,a)$ کار می‌کنه، داره عملاً با احتمال شرطی برنامه‌ریزی یا تصمیم‌گیری می‌کنه.